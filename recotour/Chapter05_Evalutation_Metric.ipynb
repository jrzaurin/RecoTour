{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5\n",
    "\n",
    "In this chapter I will introduce the Mean Average Precision (MAP). In addition, I will try to answer the question *\"What is the worst we could do?\"* by computing the MAP@10 for random recommendations. \n",
    "\n",
    "### 5. 1 Evaluation Metric\n",
    "\n",
    "In the kaggle competition they ask us to evaluate our recommendations in terms of [Mean Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval) at 10 recommendations (MAP@10). \n",
    "\n",
    "The Average Precision (AP) is measured as (from wikipedia):\n",
    "\n",
    "$$\\text{AP} = \\frac{\\sum_{k=1}^{n} (P(k) \\times rel(k))}{\\text{number of relevant documents}}$$\n",
    "\n",
    "where $k$ is the rank in the sequence of retrieved documents, $n$ is the number of retrieved documents, $P(k)$ is the precision at cut-off $k$ in the list and $rel(k)$ is an indicator function equaling 1 if the item at rank $k$ is a relevant document, zero otherwise. Let's illustrate this with one example. Let's say that a user has interacted with 5 items, that will denote with numbers:\n",
    "\n",
    "```\n",
    "actual_items = [3, 7, 4, 2, 5]\n",
    "```\n",
    "\n",
    "Now let's say that our algorithm, out of all of the items in stock recommends the following 10, ranked based on some score designed to represent the likes of that customer:\n",
    "\n",
    "```\n",
    "recommended_items = [12, 7, 53, 90, 3, 23, 14, 37, 18, 67]\n",
    "```\n",
    "\n",
    "Then the MAP@10 would read as follow: our first recommendation fails (since 12 is not among the actual items). The second recommendation is a \"hit\". In other words, by the time we make two recommendations we got 1 right, so we add $1/2$. Our third recommendation is again, a bad one, as it is the forth. The fifth one however is another hit, so we add $2/5$. From there in advance we don't get any more hits. Therefore, the AP@10 is:\n",
    "\n",
    "$$ \\text{AP@10} = \\frac{0.5 + 0.4}{5} = 0.18$$\n",
    "\n",
    "The MAP@10 is nothing more than the average of all recommendations we make for all the users. In python, AP and MAP are implemented as (**credit goes [here](https://github.com/benhamner/Metrics/tree/master/Python/ml_metrics)**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted\n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our example is alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_items = [3, 7, 4, 2, 5]\n",
    "actual_items = [str(i) for i in actual_items]\n",
    "recommended_items = [12, 7, 53, 90, 3, 23, 14, 37, 18, 67]\n",
    "recommended_items = [str(i) for i in recommended_items]\n",
    "\n",
    "apk(actual_items, recommended_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my working directory I have included a module called `recutils` where I place all custome submodules that I will use during for the different examples. There you can find a `average_precision.py` with the code in the previous cell.\n",
    "\n",
    "If you want to learn more about Information Retrieval Metrics, please, visit [this repo](https://gist.github.com/bwhite/3726239) and the references therein. In the future I aim to include a version on [Normalize Discount Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) (NDCG) where the relevance scale will be based on a meassure of interest of users in items. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Random recommendations\n",
    "\n",
    "One would hope that any algorithm, complex or not, performs better than just random recommendations. I would say that if the results of your algorithmic-based recommendations are similar to those of random recommendation, is almost certain that there is something wrong in your data. Maybe you made a mistake during feature engineering or the features you use don't mean a thing. \n",
    "\n",
    "In any case, let's see what is the MAP@10 for our dataset, when we use random recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from recutils.average_precision import mapk\n",
    "\n",
    "inp_dir = \"../datasets/Ponpare/data_processed/\"\n",
    "train_dir = \"train\"\n",
    "valid_dir = \"valid\"\n",
    "\n",
    "# load train users dataframe\n",
    "df_user_train_feat = pd.read_pickle(os.path.join(inp_dir, 'train', 'df_user_train_feat.p'))\n",
    "train_users = df_user_train_feat.user_id_hash.unique()\n",
    "\n",
    "# validation coupons\n",
    "df_coupons_valid_feat = pd.read_pickle(os.path.join(inp_dir, 'valid', 'df_coupons_valid_feat.p'))\n",
    "\n",
    "# validation activities\n",
    "df_purchases_valid = pd.read_pickle(os.path.join(inp_dir, 'valid', 'df_purchases_valid.p'))\n",
    "df_visits_valid = pd.read_pickle(os.path.join(inp_dir, 'valid', 'df_visits_valid.p'))\n",
    "df_visits_valid.rename(index=str, columns={'view_coupon_id_hash': 'coupon_id_hash'}, inplace=True)\n",
    "\n",
    "# subset users that were seeing in training\n",
    "df_vva = df_visits_valid[df_visits_valid.user_id_hash.isin(train_users)]\n",
    "df_pva = df_purchases_valid[df_purchases_valid.user_id_hash.isin(train_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we will consider \"hits\" all interactions, whether purchases or visits. In the real world a more thorough analysis on this decision would be required (if not mandatory). For example, it is possible that when computing the MAP we want to add a weight to purchase-related hits that is higher than that of visits. \n",
    "\n",
    "Also, if you are a data scientist/analyst/ML engineer/etc building a recommendation algorithm in your company one would hope that you are familiar with the product that you are building the algorithm for. This knowledge might give you information on what to do with different types of interactions. \n",
    "\n",
    "For example, let's say we are building a recommendation algorithm for an online retail company. A user visits an item page once and spend less than $X$ seconds on the page (for example less than 5 seconds). Very likely this is an indication that the user did not like that item. However, if the user visits the item more than once or spends more than $X$ seconds on the item page is probably an indicator of some interest. All this should be considered when implementing your algorithm. \n",
    "\n",
    "Throughout the notebooks here, purchases and visits will be considered equally when computing the MAP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_hash</th>\n",
       "      <th>coupon_id_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>000cc06982785a19e2a2fdb40b1c9d59</td>\n",
       "      <td>[68b8f4ff1151b51f864764cab41a30b5, 977a7c4963a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>002ae30377cd30f65652e52618e8b2d6</td>\n",
       "      <td>[1ae11153f2bfacec6ab5450d01453c4d, 404d7f06930...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>002b08971471e6083dd716f6c3bb6572</td>\n",
       "      <td>[fed386703b0295119cadda40b20efcba, bd336887b48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>003a7b4941222b7e507fdc9e95de2cc1</td>\n",
       "      <td>[4bb514d6a036c3caba31d4b62f873cd7, b2a128a9175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>00441c9b51cfe60b82bdf7a20ad79fc8</td>\n",
       "      <td>[d7fb5915505943b2c7a7f5c1c550bb25, 9beb2e8ddc1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id_hash  \\\n",
       "0  000cc06982785a19e2a2fdb40b1c9d59   \n",
       "1  002ae30377cd30f65652e52618e8b2d6   \n",
       "2  002b08971471e6083dd716f6c3bb6572   \n",
       "3  003a7b4941222b7e507fdc9e95de2cc1   \n",
       "4  00441c9b51cfe60b82bdf7a20ad79fc8   \n",
       "\n",
       "                                      coupon_id_hash  \n",
       "0  [68b8f4ff1151b51f864764cab41a30b5, 977a7c4963a...  \n",
       "1  [1ae11153f2bfacec6ab5450d01453c4d, 404d7f06930...  \n",
       "2  [fed386703b0295119cadda40b20efcba, bd336887b48...  \n",
       "3  [4bb514d6a036c3caba31d4b62f873cd7, b2a128a9175...  \n",
       "4  [d7fb5915505943b2c7a7f5c1c550bb25, 9beb2e8ddc1...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_cols = ['user_id_hash', 'coupon_id_hash']\n",
    "df_interactions_valid = pd.concat([df_pva[id_cols], df_vva[id_cols]], ignore_index=True)\n",
    "df_interactions_valid = (df_interactions_valid.groupby('user_id_hash')\n",
    "    .agg({'coupon_id_hash': 'unique'})\n",
    "    .reset_index())\n",
    "\n",
    "df_interactions_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss an additional consideration. For this particular section it does not matter much, but in moving forward is good to at least mention it. There are some users that during the validation period did not interact with any of the validation coupons. This means that it does not matter how I rank validation coupons for these users, the AP is always going to be 0. We could either remove or keep those users, it does not matter as long as ***all algorithm comparisons*** are made with the same set of users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fed386703b0295119cadda40b20efcba',\n",
       "       'bd336887b48211fdfffefa487a3f9825',\n",
       "       'b2a128a9175ce0b906f522136861c253',\n",
       "       '61b14e823046233811b066724ded6ec6'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_valid_dict = pd.Series(df_interactions_valid.coupon_id_hash.values,\n",
    "    index=df_interactions_valid.user_id_hash).to_dict()\n",
    "valid_coupon_ids = df_coupons_valid_feat.coupon_id_hash.values\n",
    "\n",
    "# let's keep users that during the validation period, interacted with at least one validation coupon\n",
    "keep_users = []\n",
    "for user, coupons in tmp_valid_dict.items():\n",
    "    if np.intersect1d(valid_coupon_ids, coupons).size !=0:\n",
    "        keep_users.append(user)\n",
    "\n",
    "# out of 6924 users seen during validation, 6071 interacted with at least one validation coupon.\n",
    "interactions_valid_dict = {k:v for k,v in tmp_valid_dict.items() if k in keep_users}\n",
    "\n",
    "# for each user this dictionary contains the coupon_hash_id for the validation coupons \n",
    "# that that user interacted with\n",
    "interactions_valid_dict['002b08971471e6083dd716f6c3bb6572']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then recommend at random and see which MAP we obtain. We run the loop a few times so we can average over a few random recommendation MAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007868248221401582\n"
     ]
    }
   ],
   "source": [
    "mapk_l = []\n",
    "\n",
    "for _ in range(10):\n",
    "    coupon_id_rn = valid_coupon_ids.copy()\n",
    "    recomendations_dict = {}\n",
    "    for user, _  in interactions_valid_dict.items():\n",
    "        np.random.shuffle(coupon_id_rn)\n",
    "        recomendations_dict[user] = coupon_id_rn\n",
    "\n",
    "    actual = []\n",
    "    pred = []\n",
    "    for k,_ in recomendations_dict.items():\n",
    "        actual.append(list(interactions_valid_dict[k]))\n",
    "        pred.append(list(recomendations_dict[k]))\n",
    "\n",
    "    mapk_l.append(mapk(actual,pred))\n",
    "\n",
    "print(np.mean(mapk_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep that number in mind, since we **ALWAYS** need to do better than that :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_recotour)",
   "language": "python",
   "name": "conda_recotour"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
