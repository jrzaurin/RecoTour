{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9\n",
    "\n",
    "### 9.1 KNN Collaborative Filtering\n",
    "\n",
    "The amount of blog posts and literature one can find regarding to this technique is *gigantic*. Therefore, I will not spend much time explaining how it works. For example, have a look [here](https://beckernick.github.io/music_recommender/). Let me also use this opportunity to recommend one good book about recommender systems: [Recommender Systems](https://www.amazon.co.uk/Recommender-Systems-Textbook-Charu-Aggarwal/dp/3319296574/ref=sr_1_1?ie=UTF8&qid=1531491611&sr=8-1&keywords=recommender+systems). \n",
    "\n",
    "Nonetheless, here is a quick explanation. In Chapter 8 we built an *\"Interaction Matrix\"* (hereafter $R$ as a generic reference to *rating matrix*) of dimensions $U\\times I$ where $U$ is the number of users and $I$ is the number of items. Each element of that matrix $R_{ij}$ is the interest of user $i$ in coupon $j$. If we transpose this matrix ($R^{T}$) we can use it to compute the similarity between coupons based on how user interacted with them. Then, if a user has shown interest in a given coupon, we can recommend similar coupons based on that similarity metric. In other words, we can recommend similar items using **item-based collaborative filtering**. \n",
    "\n",
    "However, as straightforward this approach might sound, there is an caveat here, and in any approach that is based purely on past interaction between users and items. This is, the coupons that need to be recommended in a given week, have never been seen before. Therefore, they are not in $R$. \n",
    "\n",
    "Here, we are going to \"overcomplicate\" things a bit simply because I thought any \"tour\" through recommendation algorithms without at least illustrating the use of CF is not complete. What we will do is the following:\n",
    "\n",
    "1. Use kNN CF as usual, recommending training coupons based on interactions.\n",
    "2. As in previous chapters, we will compute the distance between training and validation coupons using only coupon features. \n",
    "3. We will build a dictionary mapping training into validation coupons. \n",
    "4. We will map training coupon recommendations into validation coupon recommendation. \n",
    "\n",
    "Yes, this is overcomplicating if not even misusing the technique...Anyway, one advantage might be that we add some sense of interaction-based recommendation as we recommend the new coupons. Still, you might wonder: *\"why not simply recommend to a given user those new coupons in validation that resemble more to those he/she interacted with during training?!\"* And you will be right to ask that. I will leave this as an exercise if you want to do it. Simply: 1) take the $N$ coupons a user interacted with during training. 2) Find the corresponding most similar validation coupons and 3) rank them based on similarity and maybe consider adding a weight based on interest.\n",
    "\n",
    "With all that in mind, let's go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "from time import time\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.sparse import csr_matrix, load_npz\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from recutils.average_precision import mapk\n",
    "\n",
    "# Make sure you import this package the last (read below)\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "inp_dir = \"../datasets/Ponpare/data_processed/\"\n",
    "train_dir = \"train\"\n",
    "valid_dir = \"valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coupons_train_feat = pd.read_pickle(os.path.join(inp_dir, train_dir, 'df_coupons_train_feat.p'))\n",
    "df_coupons_valid_feat = pd.read_pickle(os.path.join(inp_dir, valid_dir, 'df_coupons_valid_feat.p'))\n",
    "coupons_train_ids = df_coupons_train_feat.coupon_id_hash.values\n",
    "coupons_valid_ids = df_coupons_valid_feat.coupon_id_hash.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As In Chapter 6, we compute the distance between coupons simply stacking one-hot encoded and numerical features and using the cosine distance.\n",
    "\n",
    "Let's first get the one-hot encoded features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add a flag for convenience\n",
    "df_coupons_train_feat['flag_cat'] = 0\n",
    "df_coupons_valid_feat['flag_cat'] = 1\n",
    "\n",
    "#Â There are better ways of doing this. See Chapter 14, for example. \n",
    "# In any case, computation-wise takes the same time\n",
    "flag_cols = ['flag_cat_0','flag_cat_1']\n",
    "\n",
    "cat_cols = [c for c in df_coupons_train_feat.columns if '_cat' in c]\n",
    "id_cols = ['coupon_id_hash']\n",
    "num_cols = [c for c in df_coupons_train_feat.columns if\n",
    "    (c not in cat_cols) and (c not in id_cols)]\n",
    "\n",
    "tmp_df = pd.concat([df_coupons_train_feat[cat_cols],\n",
    "    df_coupons_valid_feat[cat_cols]],\n",
    "    ignore_index=True)\n",
    "\n",
    "df_dummy_feats = pd.get_dummies(tmp_df.astype('category'))\n",
    "\n",
    "coupons_train_feat_oh = (df_dummy_feats[df_dummy_feats.flag_cat_0 != 0]\n",
    "    .drop(flag_cols, axis=1)\n",
    "    .values)\n",
    "coupons_valid_feat_oh = (df_dummy_feats[df_dummy_feats.flag_cat_1 != 0]\n",
    "    .drop(flag_cols, axis=1)\n",
    "    .values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/recotour/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "coupons_train_feat_num = df_coupons_train_feat[num_cols].values\n",
    "coupons_valid_feat_num = df_coupons_valid_feat[num_cols].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "coupons_train_feat_num_norm = scaler.fit_transform(coupons_train_feat_num)\n",
    "coupons_valid_feat_num_norm = scaler.transform(coupons_valid_feat_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack -> distance -> to dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupons_train_feat = np.hstack([coupons_train_feat_num_norm, coupons_train_feat_oh])\n",
    "coupons_valid_feat = np.hstack([coupons_valid_feat_num_norm, coupons_valid_feat_oh])\n",
    "\n",
    "dist_mtx = pairwise_distances(coupons_valid_feat, coupons_train_feat, metric='cosine')\n",
    "\n",
    "# now we have a matrix of distances, let's build the dictionaries\n",
    "valid_to_train_top_n_idx = np.apply_along_axis(np.argsort, 1, dist_mtx)\n",
    "train_to_valid_top_n_idx = np.apply_along_axis(np.argsort, 1, dist_mtx.T)\n",
    "train_to_valid_most_similar = dict(zip(coupons_train_ids,\n",
    "    coupons_valid_ids[train_to_valid_top_n_idx[:,0]]))\n",
    "# there is one coupon in validation: '0a8e967835e2c20ac4ed8e69ee3d7349' that\n",
    "# is never among the most similar to those previously seen.\n",
    "valid_to_train_most_similar = dict(zip(coupons_valid_ids,\n",
    "    coupons_train_ids[valid_to_train_top_n_idx[:,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly build a dictionary of interactions during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary or interactions during training\n",
    "df_interest = pd.read_pickle(os.path.join(inp_dir, train_dir, \"df_interest.p\"))\n",
    "df_interactions_train = (df_interest.groupby('user_id_hash')\n",
    "    .agg({'coupon_id_hash': 'unique'})\n",
    "    .reset_index())\n",
    "interactions_train_dict = pd.Series(df_interactions_train.coupon_id_hash.values,\n",
    "    index=df_interactions_train.user_id_hash).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the interactions matrix and user/item indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the activity matrix and dict of indexes\n",
    "interactions_mtx = load_npz(os.path.join(inp_dir, train_dir, \"interactions_mtx.npz\"))\n",
    "\n",
    "# We built the matrix as user x items, but for knn item based CF we need items x users\n",
    "interactions_mtx_knn = interactions_mtx.T\n",
    "\n",
    "# users and items indexes\n",
    "items_idx_dict = pickle.load(open(os.path.join(inp_dir, train_dir, \"items_idx_dict.p\"),'rb'))\n",
    "users_idx_dict = pickle.load(open(os.path.join(inp_dir, train_dir, \"users_idx_dict.p\"),'rb'))\n",
    "idx_item_dict = {v:k for k,v in items_idx_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run knn in two lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='brute', leaf_size=30, metric='cosine',\n",
       "         metric_params=None, n_jobs=1, n_neighbors=5, p=2, radius=1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build the KNN model...two lines :)\n",
    "model = NearestNeighbors(metric = 'cosine', algorithm = 'brute')\n",
    "model.fit(interactions_mtx_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dictionary of interactions during validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fe28f9f9055fde46855b1520a40e3c08'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the validation interactions dictionary\n",
    "interactions_valid_dict = pickle.load(open(os.path.join(inp_dir,valid_dir,\"interactions_valid_dict.p\"), \"rb\"))\n",
    "# remember that one user that visited one coupon and that coupon is not in the training set of coupons.\n",
    "# and in consequence not in the interactions matrix\n",
    "interactions_valid_dict.pop(\"25e2b645bfcd0980b2a5d0a4833f237a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for convenience\n",
    "user_items_tuple = [(k,v) for k,v in interactions_valid_dict.items()]\n",
    "\n",
    "def build_recommendations(user):\n",
    "    #given a user seen during training and validation, get her training interactions\n",
    "    coupons = interactions_train_dict[user]\n",
    "    \n",
    "    #Â get the training coupon indexes\n",
    "    idxs = [items_idx_dict[c] for c in coupons]\n",
    "    \n",
    "    #Â compute the k=11 NN\n",
    "    dist, nnidx = model.kneighbors(interactions_mtx_knn[idxs], n_neighbors = 11)\n",
    "\n",
    "    # Drop the 1st result as the closest to a coupon is always itself\n",
    "    dist, nnidx = dist[:, 1:], nnidx[:,1:]\n",
    "    dist, nnidx = dist.ravel(), nnidx.ravel()\n",
    "\n",
    "    # rank based on distances and keep top 50 (with 10 is enough really)\n",
    "    ranked_dist = np.argsort(dist)\n",
    "    ranked_cp_idxs = nnidx[ranked_dist][:50]\n",
    "\n",
    "    # recover the train coupon ids from their indexes and map then to validation coupons\n",
    "    ranked_cp_ids  = [idx_item_dict[i] for i in ranked_cp_idxs]\n",
    "    ranked_cp_idxs_valid = [train_to_valid_most_similar[c] for c in ranked_cp_ids]\n",
    "\n",
    "    return (user,ranked_cp_idxs_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially I tried to use the `joblib` package and run the function in the cell above in parallel. Note that I imported `joblib` the last one. This is **IMPORTANT** when using linux. Packages like `numpy, scipy or pandas` link against multithreaded OpenBLAS libraries. In other words, if you import them afterwards, `joblib` will not run, or will do it very slow and not using all cores. There are a few [ways around](https://stackoverflow.com/questions/15639779/why-does-multiprocessing-use-only-a-single-core-after-i-import-numpy), but the easiest one is simply to import joblib after having imported all other required packages (sometimes works, sometimes does not). However, **I have never managed to run it in a jupyter notebook in linux** (I am on an EC2 instance on AWS). Maybe is the environment, I don't know. \n",
    "\n",
    "Eventually I decided to use `multiprocessing.Pool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302.82322239875793\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "cores = multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(cores)\n",
    "all_users = list(interactions_valid_dict.keys())\n",
    "recommend_coupons = pool.map(build_recommendations, all_users)\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this technique performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019374116349816513\n"
     ]
    }
   ],
   "source": [
    "recommendations_dict = {k:v for k,v in recommend_coupons}\n",
    "actual = []\n",
    "pred = []\n",
    "for k,_ in recommendations_dict.items():\n",
    "    actual.append(list(interactions_valid_dict[k]))\n",
    "    pred.append(list(recommendations_dict[k]))\n",
    "\n",
    "result = mapk(actual, pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so this is just slightly better than \"most popular\" recommendations. As I mentioned in the beginning, kNN CF is not the best technique for this problem. \n",
    "\n",
    "However, I can tell you that for scenarios where one has to recommend products in stock to existing customers, Collaborative Filtering is almost *always* my go-to recommendation algorithm. It normally performs really well and there are kNN implementations in [python](https://github.com/spotify/annoy) that are really fast and ready for production (apart from, of course, the `sklearn` one). Therefore, if I faced a problem where I need to recommend existing and new products and I can afford it, I will probably build two algorithms: one based on CF for the existing products with recorded interactions/ratings, and another one based purely on features for new products (or a hybrid version)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
