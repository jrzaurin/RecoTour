{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6\n",
    "\n",
    "### 6.1 Recommending the most popular items\n",
    "\n",
    "In the previous chapter we presented our evaluation metric, MAP, and we computed its value for the case of random recommendation. Let's not move onto the next natural set of recommendations before exploring different algorithms, *\"most-popular\"* recommendations\n",
    "\n",
    "As always, let's start loading the required packages and defining some useful names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from recutils.average_precision import mapk\n",
    "\n",
    "inp_dir = \"/home/ubuntu/projects/RecoTour/datasets/Ponpare/data_processed/\"\n",
    "train_dir = \"train\"\n",
    "valid_dir = \"valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and as in previous chapter let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training interactions\n",
    "df_purchases_train = pd.read_pickle(os.path.join(inp_dir, 'train', 'df_purchases_train.p'))\n",
    "df_visits_train = pd.read_pickle(os.path.join(inp_dir, 'train', 'df_visits_train.p'))\n",
    "df_visits_train.rename(index=str, columns={'view_coupon_id_hash': 'coupon_id_hash'}, inplace=True)\n",
    "\n",
    "# train users\n",
    "df_user_train_feat = pd.read_pickle(os.path.join(inp_dir, 'train', 'df_users_train_feat.p'))\n",
    "train_users = df_user_train_feat.user_id_hash.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define *\"popularity\"*. A priori one could simply use the number of times an item was purchased. However here we also have information about visits, so we could compute a meassure of popularity that combines the two. In our case, we will simply compute popularity as:\n",
    "\n",
    "$$\\text{Coupon Popularity} = N_{purchases} + f \\times N_{visits}$$\n",
    "\n",
    "For this particular excercise, $f = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_purchases = (df_purchases_train\n",
    "    .coupon_id_hash\n",
    "    .value_counts()\n",
    "    .reset_index())\n",
    "df_n_purchases.columns = ['coupon_id_hash','purchase_counts']\n",
    "df_n_visits = (df_visits_train\n",
    "    .coupon_id_hash\n",
    "    .value_counts()\n",
    "    .reset_index())\n",
    "df_n_visits.columns = ['coupon_id_hash','visit_counts']\n",
    "\n",
    "df_popularity = df_n_purchases.merge(df_n_visits, on='coupon_id_hash', how='left')\n",
    "df_popularity.fillna(0, inplace=True)\n",
    "df_popularity['popularity'] = df_popularity['purchase_counts'] + 0.1*df_popularity['visit_counts']\n",
    "df_popularity.sort_values('popularity', ascending=False , inplace=True)\n",
    "df_popularity.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coupon_id_hash</th>\n",
       "      <th>purchase_counts</th>\n",
       "      <th>visit_counts</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>a262c7ff56a5cd3de3c5c40443f3018c</td>\n",
       "      <td>5760</td>\n",
       "      <td>14778.0</td>\n",
       "      <td>7237.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3d9029d3ec66802b11ee2645dc16e8cb</td>\n",
       "      <td>1511</td>\n",
       "      <td>3063.0</td>\n",
       "      <td>1817.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>09411858ae07c0be91aeeddacf4556b4</td>\n",
       "      <td>1016</td>\n",
       "      <td>2562.0</td>\n",
       "      <td>1272.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7fc6567f470af5356ae97097dbe18486</td>\n",
       "      <td>863</td>\n",
       "      <td>444.0</td>\n",
       "      <td>907.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bf69bd9e0e26fa1f62243d1fcada38f1</td>\n",
       "      <td>663</td>\n",
       "      <td>1810.0</td>\n",
       "      <td>844.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>047fb1f23d8cedea8cb86956cfd4b7cf</td>\n",
       "      <td>628</td>\n",
       "      <td>1785.0</td>\n",
       "      <td>806.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>229ff5cc21c8d26615493be7f3b42841</td>\n",
       "      <td>494</td>\n",
       "      <td>2626.0</td>\n",
       "      <td>756.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4a79cd05ecb2bf8672e1d955f5faa7fa</td>\n",
       "      <td>466</td>\n",
       "      <td>2623.0</td>\n",
       "      <td>728.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>d0e1b63cb7cc32edc3a6c619e4215368</td>\n",
       "      <td>355</td>\n",
       "      <td>3345.0</td>\n",
       "      <td>689.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3e6d617c55328b761d62510167c43c08</td>\n",
       "      <td>504</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>672.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     coupon_id_hash  purchase_counts  visit_counts  popularity\n",
       "0  a262c7ff56a5cd3de3c5c40443f3018c             5760       14778.0      7237.8\n",
       "1  3d9029d3ec66802b11ee2645dc16e8cb             1511        3063.0      1817.3\n",
       "2  09411858ae07c0be91aeeddacf4556b4             1016        2562.0      1272.2\n",
       "3  7fc6567f470af5356ae97097dbe18486              863         444.0       907.4\n",
       "4  bf69bd9e0e26fa1f62243d1fcada38f1              663        1810.0       844.0\n",
       "5  047fb1f23d8cedea8cb86956cfd4b7cf              628        1785.0       806.5\n",
       "6  229ff5cc21c8d26615493be7f3b42841              494        2626.0       756.6\n",
       "7  4a79cd05ecb2bf8672e1d955f5faa7fa              466        2623.0       728.3\n",
       "8  d0e1b63cb7cc32edc3a6c619e4215368              355        3345.0       689.5\n",
       "9  3e6d617c55328b761d62510167c43c08              504        1686.0       672.6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_popularity.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we need to consider the fact that the validation coupons were, of course, never seen during training. Therefore, we need to find a way to compute their \"popularity\". What I use here is one method of many, so please, feel free to try anything you might consider better.\n",
    "\n",
    "First I will compute a distance between validation and the top 10 most popular training coupons. Given that we have 358 validation coupons, this first step will result into a matrix of shape (358,10). I will then compute the mean per row which will give me an idea of how similar a validation coupon is to the top 10 most similar coupons during training. \n",
    "\n",
    "The next question is, of course, how we compute the distance between coupons? In this particular dataset we have a rich set of coupon features, which allows for substantiall experimentation. For example, you might want to consider an implementation where using the [jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) for one-hot encoded categorical features combines with the euclidean distance for numerical features. There is an implementation of this approach in `recutils`\n",
    "\n",
    "Here, I will simply stack the two feature-sets and use the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select top 10 most popular coupons from the training dataset\n",
    "top10 = df_popularity.coupon_id_hash.tolist()[:10]\n",
    "\n",
    "# and their feautures\n",
    "df_coupons_train_feat = pd.read_pickle(os.path.join(inp_dir, 'train', 'df_coupons_train_feat.p'))\n",
    "df_top_10_feat = (df_coupons_train_feat[df_coupons_train_feat.coupon_id_hash.isin(top10)]\n",
    "    .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['usable_date_mon_cat', 'usable_date_tue_cat', 'usable_date_wed_cat', 'usable_date_thu_cat', 'usable_date_fri_cat', 'usable_date_sat_cat', 'usable_date_sun_cat', 'usable_date_holiday_cat', 'usable_date_before_holiday_cat', 'validperiod_method1_cat', 'validperiod_method2_cat', 'validfrom_method1_cat', 'validfrom_method2_cat', 'validend_method1_cat', 'validend_method2_cat', 'dispfrom_cat', 'dispend_cat', 'dispperiod_cat', 'price_rate_cat', 'catalog_price_cat', 'discount_price_cat', 'capsule_text_cat', 'genre_name_cat', 'large_area_name_cat', 'ken_name_cat', 'small_area_name_cat']\n",
      "['price_rate', 'catalog_price', 'discount_price', 'dispperiod', 'validperiod']\n"
     ]
    }
   ],
   "source": [
    "# let's read the validation coupons and select categorical and numerical columns\n",
    "df_coupons_valid_feat = pd.read_pickle(os.path.join(inp_dir, 'valid', 'df_coupons_valid_feat.p'))\n",
    "coupons_valid_ids = df_coupons_valid_feat.coupon_id_hash.values\n",
    "cat_cols = [c for c in df_coupons_train_feat.columns if c.endswith('_cat')]\n",
    "id_cols = ['coupon_id_hash']\n",
    "num_cols = [c for c in df_coupons_train_feat.columns if\n",
    "    (c not in cat_cols) and (c not in id_cols)]\n",
    "print(cat_cols)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-hot encoding process needs to happen with all coupons in consideration, so we account for all possible feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flag</th>\n",
       "      <th>usable_date_mon_cat_0</th>\n",
       "      <th>usable_date_mon_cat_1</th>\n",
       "      <th>usable_date_mon_cat_2</th>\n",
       "      <th>usable_date_mon_cat_3</th>\n",
       "      <th>usable_date_tue_cat_0</th>\n",
       "      <th>usable_date_tue_cat_1</th>\n",
       "      <th>usable_date_tue_cat_2</th>\n",
       "      <th>usable_date_tue_cat_3</th>\n",
       "      <th>usable_date_wed_cat_0</th>\n",
       "      <th>...</th>\n",
       "      <th>small_area_name_cat_39</th>\n",
       "      <th>small_area_name_cat_40</th>\n",
       "      <th>small_area_name_cat_43</th>\n",
       "      <th>small_area_name_cat_44</th>\n",
       "      <th>small_area_name_cat_45</th>\n",
       "      <th>small_area_name_cat_46</th>\n",
       "      <th>small_area_name_cat_47</th>\n",
       "      <th>small_area_name_cat_48</th>\n",
       "      <th>small_area_name_cat_49</th>\n",
       "      <th>small_area_name_cat_51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flag  usable_date_mon_cat_0  usable_date_mon_cat_1  usable_date_mon_cat_2  \\\n",
       "0     0                      0                      0                      0   \n",
       "1     0                      0                      0                      0   \n",
       "2     0                      0                      0                      0   \n",
       "3     0                      0                      0                      0   \n",
       "4     0                      0                      0                      0   \n",
       "\n",
       "   usable_date_mon_cat_3  usable_date_tue_cat_0  usable_date_tue_cat_1  \\\n",
       "0                      1                      0                      0   \n",
       "1                      1                      0                      0   \n",
       "2                      1                      0                      0   \n",
       "3                      1                      0                      0   \n",
       "4                      1                      0                      0   \n",
       "\n",
       "   usable_date_tue_cat_2  usable_date_tue_cat_3  usable_date_wed_cat_0  ...  \\\n",
       "0                      0                      1                      0  ...   \n",
       "1                      0                      1                      0  ...   \n",
       "2                      0                      1                      0  ...   \n",
       "3                      0                      1                      0  ...   \n",
       "4                      0                      1                      0  ...   \n",
       "\n",
       "   small_area_name_cat_39  small_area_name_cat_40  small_area_name_cat_43  \\\n",
       "0                       0                       0                       0   \n",
       "1                       0                       0                       0   \n",
       "2                       0                       0                       0   \n",
       "3                       0                       0                       0   \n",
       "4                       0                       0                       0   \n",
       "\n",
       "   small_area_name_cat_44  small_area_name_cat_45  small_area_name_cat_46  \\\n",
       "0                       0                       0                       0   \n",
       "1                       0                       0                       0   \n",
       "2                       0                       0                       0   \n",
       "3                       0                       0                       0   \n",
       "4                       0                       0                       0   \n",
       "\n",
       "   small_area_name_cat_47  small_area_name_cat_48  small_area_name_cat_49  \\\n",
       "0                       0                       0                       0   \n",
       "1                       0                       0                       0   \n",
       "2                       0                       0                       0   \n",
       "3                       0                       0                       0   \n",
       "4                       0                       0                       0   \n",
       "\n",
       "   small_area_name_cat_51  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "\n",
       "[5 rows x 215 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_10_feat['flag'] = 0\n",
    "df_coupons_valid_feat['flag'] = 1\n",
    "tmp_df = pd.concat([\n",
    "    df_top_10_feat[cat_cols+['flag']],\n",
    "    df_coupons_valid_feat[cat_cols+['flag']]\n",
    "    ],\n",
    "    ignore_index=True)\n",
    "df_dummy_feats = pd.get_dummies(tmp_df, columns=cat_cols)\n",
    "df_dummy_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's split back to train and validation\n",
    "df_top_10_feat_oh = (df_dummy_feats[df_dummy_feats.flag == 0]\n",
    "    .drop('flag', axis=1)\n",
    "    .values)\n",
    "coupons_valid_feat_oh = (df_dummy_feats[df_dummy_feats.flag == 1]\n",
    "    .drop('flag', axis=1)\n",
    "    .values)\n",
    "del(tmp_df, df_dummy_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/recotour/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# scaling the numerical features in training and validation\n",
    "df_top_10_feat_num = df_top_10_feat[num_cols].values\n",
    "coupons_valid_feat_num = df_coupons_valid_feat[num_cols].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_top_10_feat_num_norm = scaler.fit_transform(df_top_10_feat_num)\n",
    "coupons_valid_feat_num_norm = scaler.transform(coupons_valid_feat_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now time to compute the distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons_train_feat = np.hstack([df_top_10_feat_num_norm, df_top_10_feat_oh])\n",
    "coupons_valid_feat = np.hstack([coupons_valid_feat_num_norm, coupons_valid_feat_oh])\n",
    "\n",
    "dist_mtx = pairwise_distances(coupons_valid_feat, coupons_train_feat, metric='cosine')\n",
    "\n",
    "# let's check \"all makes sense\"\n",
    "dist_mtx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the validation coupons \"popularity\", expressed as how similar are the validation coupons to the most popular coupons during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coupon_id_hash</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>282b5bda1758e147589ca517e02195c3</td>\n",
       "      <td>0.193095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0f43ef71c25d409c250f5a5042806342</td>\n",
       "      <td>0.175737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28ff0fb4b561a2fd6a360fe28f465e07</td>\n",
       "      <td>0.149547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>864f351e66cd3aeece5d06987fc2ed4b</td>\n",
       "      <td>0.136935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>279ba64539609d30114b68874cd0fb42</td>\n",
       "      <td>0.302550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     coupon_id_hash  popularity\n",
       "0  282b5bda1758e147589ca517e02195c3    0.193095\n",
       "1  0f43ef71c25d409c250f5a5042806342    0.175737\n",
       "2  28ff0fb4b561a2fd6a360fe28f465e07    0.149547\n",
       "3  864f351e66cd3aeece5d06987fc2ed4b    0.136935\n",
       "4  279ba64539609d30114b68874cd0fb42    0.302550"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_distances = np.apply_along_axis(np.mean, 1, dist_mtx)\n",
    "df_valid_popularity = pd.DataFrame({'coupon_id_hash': coupons_valid_ids,\n",
    "    'popularity': 1-mean_distances})\n",
    "\n",
    "df_valid_popularity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, so at this stage we have a measure of popularity for the coupons in the validation set. The code below is identical to the one in Chapter 5. Therefore, we will save the result of the cell so we do not have to write the whole snippet again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation activities\n",
    "df_purchases_valid = pd.read_pickle(os.path.join(inp_dir, 'valid', 'df_purchases_valid.p'))\n",
    "df_visits_valid = pd.read_pickle(os.path.join(inp_dir, 'valid', 'df_visits_valid.p'))\n",
    "df_visits_valid.rename(index=str, columns={'view_coupon_id_hash': 'coupon_id_hash'}, inplace=True)\n",
    "\n",
    "# subset users that were seeing in training. Code below is identical to that \n",
    "# in the previous chapter. I will save the corresponding dictionary to avoid\n",
    "# to much code repetition\n",
    "df_vva = df_visits_valid[df_visits_valid.user_id_hash.isin(train_users)]\n",
    "df_pva = df_purchases_valid[df_purchases_valid.user_id_hash.isin(train_users)]\n",
    "\n",
    "id_cols = ['user_id_hash', 'coupon_id_hash']\n",
    "df_interactions_valid = pd.concat([df_pva[id_cols], df_vva[id_cols]], ignore_index=True)\n",
    "df_interactions_valid = (df_interactions_valid.groupby('user_id_hash')\n",
    "    .agg({'coupon_id_hash': 'unique'})\n",
    "    .reset_index())\n",
    "tmp_valid_dict = pd.Series(df_interactions_valid.coupon_id_hash.values,\n",
    "    index=df_interactions_valid.user_id_hash).to_dict()\n",
    "\n",
    "valid_coupon_ids = df_coupons_valid_feat.coupon_id_hash.values\n",
    "\n",
    "keep_users = []\n",
    "for user, coupons in tmp_valid_dict.items():\n",
    "    if np.intersect1d(valid_coupon_ids, coupons).size !=0:\n",
    "        keep_users.append(user)\n",
    "\n",
    "interactions_valid_dict = {k:v for k,v in tmp_valid_dict.items() if k in keep_users}\n",
    "pickle.dump(interactions_valid_dict, open(\"../datasets/Ponpare/data_processed/valid/interactions_valid_dict.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally let's recommmend the most popular items to every user that interacted at least with one validation coupon during validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/recotour/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01856165521321787\n"
     ]
    }
   ],
   "source": [
    "# Cartesian Product between users and validation items\n",
    "left = pd.DataFrame({'user_id_hash':list(interactions_valid_dict.keys())})\n",
    "left['key'] = 0\n",
    "right = df_coupons_valid_feat[['coupon_id_hash']]\n",
    "right['key'] = 0\n",
    "df_valid = (pd.merge(left, right, on='key', how='outer')\n",
    "    .drop('key', axis=1))\n",
    "df_valid = pd.merge(df_valid, df_valid_popularity, on='coupon_id_hash')\n",
    "\n",
    "# rank based on popularity\n",
    "df_ranked = df_valid.sort_values(['user_id_hash', 'popularity'], ascending=[False, False])\n",
    "df_ranked = (df_ranked\n",
    "    .groupby('user_id_hash')['coupon_id_hash']\n",
    "    .apply(list)\n",
    "    .reset_index())\n",
    "recomendations_dict = pd.Series(df_ranked.coupon_id_hash.values,\n",
    "    index=df_ranked.user_id_hash).to_dict()\n",
    "\n",
    "# Compute mapk\n",
    "actual = []\n",
    "pred = []\n",
    "for k,_ in recomendations_dict.items():\n",
    "    actual.append(list(interactions_valid_dict[k]))\n",
    "    pred.append(list(recomendations_dict[k]))\n",
    "\n",
    "print(mapk(actual,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot better than random, as expected. In fact, let's pause for one second and reflect on that value and the solution in this notebook. \n",
    "\n",
    "In this particular dataset we have 18622 coupons and 22624 users in the training dataset. These are relatively small numbers. In addition, the most popular coupons are indeed very popular. The most popular coupon was purchased 5760 times by 5760 different customers. Therefore, it is straightforward to understand that in this scenario, a \"most-popular\" recommendation approach should work fairly well. In fact, I can anticipate that it will not be easy to improve.\n",
    "\n",
    "As the data size increases, of if there are not big popularity differences between the items in your dataset, this approach does not normally work that well. Nonetheless if you are in a company where your boss or \"the business\" asks you for an \"ML-based\" recommendation system, you are in a rush, and your scenario is similar to the one described here (relatively small number of items and some of them really popular), you might want to quickly implement and productionise a most-popular recommendation system and then move to a true algorithmic/ML solution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
