{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16\n",
    "\n",
    "## 16.1 Wrapping Up\n",
    "\n",
    "So, we made it up to here! we have gone through:\n",
    "\n",
    "1. Most popular recommendations\n",
    "2. User-Item similarity based recommendations\n",
    "3. kNN collaborative filtering (well, sort of...)\n",
    "4. GBM-based methods\n",
    "5. Non-Negative Matrix Factorization\n",
    "6. Factorization Machines\n",
    "7. Field Aware Factorization Machines \n",
    "8. Deep Learning based recommendations (1st contact)\n",
    "\n",
    "The best and most reliable technique has been **GBMs**, turning our recommendation algorithm into a regression problem and using `lightGBM`. Of course, this might be due to the fact that I might have not used the best packages for some of the other techniques (e.g. factorization machines) or the best set ups (e.g. wide and deep). Nonetheless, we know `lightGBM` produces a decent **MAP@10 (0.032)** and is a fully tested, production-ready package, so let's use it. \n",
    "\n",
    "Before going to the code in this notebook let me remind myself for a second the problem we are trying to solve here. We are given a training set (a year's worth of data) and I need to recommend new items (in this case coupons) to my customers. Let's say I receive new coupons during the week and I need to display them to my customers (or uses) during Sunday night, so next morning they all see fresh recommendations in the site or their mobile. From there in advance there are a series of possibilities one might consider that I do not have the information (or the time) to implement them here.  \n",
    "\n",
    "For example, the easiest scenario is one in which the recommended coupons stay there for the whole \"next week\". However, it is possible that we have to update the recommendations every time a user interacts with one of the recommended coupons. Let's assume that based on a thorough analysis of the existing data we know that we need at least 10 interactions for the user interaction-based features to have any predicting power. What do we do before those 10 interactions? Well, we could recommend based on how similar the new coupons are to the most popular existing coupons or we could just recommend based on coupon content (content-based recommendations). \n",
    "\n",
    "On the other hand it might be that our recommendations should focus more on new coupons, or maybe we should prioritize old/existing coupons that are about to expire. In this later case we could build an identical recommendation algorithm to the one I will show here simply adding \"about-to-expire\" coupons to the batch of new coupons. Then we would have to add a rule so that we do know show existing coupons to those customers that (somehow) expressed dislike or already purchased them. A further possibility is one where some of our sponsors pay us more to promote their coupons and we need to give their coupons an \"extra push\" in the ranking. There are many possibilities and they determine the final shape of your recommendation algorithm. With the information provided for this Kaggle competition and the dataset **I have, I will simply assume I am at that point on Sunday when I need to recommend new coupons to my customers**. \n",
    "\n",
    "Finally, let me mention that I have tested the algorithms with existing users, leaving outside the test those users that were not seen during training. However, my final recommendation algorithm should also contemplate the possibility that a new user might register on Monday evening and he/she needs to see some recommendations as well (I will assume that for this new user I have no information on his/her likes). \n",
    "\n",
    "Therefore, considering all of the above, my final solution will be a combination of: \n",
    "\n",
    "1. Most popular recommendations for unseen customers (cold start problem for users)\n",
    "2. `lightGBM`-based recommendations for existing customers\n",
    "\n",
    "To the code...\n",
    "\n",
    "## 16.2 Utility functions\n",
    "\n",
    "With the aim of making the code more readable I have wrap up most of the code from previous Chapters in functions. I will still include them here in this notebook, but they could be place somewhere (say a module called `recommender_utils` and simply do: \n",
    "\n",
    "    from recommeder_utils import *\n",
    "\n",
    "Also, we have \"validated\" already, so here I will be using all the data but the last week for training and the last week for testing. You could just append the train and validation data or you could run the code in Chapters 2, 3 and 4 simply changing the lines that look like this: \n",
    "\n",
    "    df_visits['days_to_present_flag'] = df_visits.days_to_present.apply(\n",
    "        lambda x: 0 if x<=tp-1 else 1 if ((x>tp-1) and (x<=(tp*2)-1)) else 2)\n",
    "\n",
    "into this:\n",
    "\n",
    "    df_visits['days_to_present_flag'] = df_visits.days_to_present.apply(\n",
    "    lambda x: 0 if x<=tp-1 else 1)\n",
    "\n",
    "This is a trivial change and I will leave you to do it. I did it and place the results in a directory called `ftrain`, such that:\n",
    "\n",
    "    ~/projects/RecoTour/datasets/Ponpare/data_processed$ tree\n",
    "    .\n",
    "    ├── ftrain\n",
    "    │   ├── df_coupons_train_feat.p\n",
    "    │   ├── df_coupons_train.p\n",
    "    │   ├── df_interest.p\n",
    "    │   ├── df_purchases_train.p\n",
    "    │   ├── df_users_train_feat.p\n",
    "    │   ├── df_users_train.p\n",
    "    │   └── df_visits_train.p\n",
    "    ├── test\n",
    "    │   ├── df_coupons_test_feat.p\n",
    "    │   ├── df_coupons_test.p\n",
    "    │   ├── df_purchases_test.p\n",
    "    │   ├── df_users_test.p\n",
    "    │   └── df_visits_test.p\n",
    "    \n",
    "Ok, let's go, one function per cell with comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import argparse\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import multiprocessing\n",
    "\n",
    "from recutils.utils import coupon_similarity_function\n",
    "from recutils.average_precision import mapk\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from hyperopt import hp, tpe, fmin, Trials\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Most Popular Coupons during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top10_train_coupon_populatiry(train_purchases_path, train_visits_path):\n",
    "\n",
    "    # train coupon popularity based on purchases and visits\n",
    "    df_purchases_train = pd.read_pickle(train_purchases_path)\n",
    "    df_visits_train = pd.read_pickle(train_visits_path)\n",
    "    df_visits_train.rename(index=str, columns={'view_coupon_id_hash': 'coupon_id_hash'}, inplace=True)\n",
    "\n",
    "    # popularity = n_purchases + 0.1*n_visits\n",
    "    df_n_purchases = (df_purchases_train\n",
    "        .coupon_id_hash\n",
    "        .value_counts()\n",
    "        .reset_index())\n",
    "    df_n_purchases.columns = ['coupon_id_hash','counts']\n",
    "    df_n_visits = (df_visits_train\n",
    "        .coupon_id_hash\n",
    "        .value_counts()\n",
    "        .reset_index())\n",
    "    df_n_visits.columns = ['coupon_id_hash','counts']\n",
    "\n",
    "    df_popularity = df_n_purchases.merge(df_n_visits, on='coupon_id_hash', how='left')\n",
    "    df_popularity.fillna(0, inplace=True)\n",
    "    df_popularity['popularity'] = df_popularity['counts_x'] + 0.1*df_popularity['counts_y']\n",
    "    df_popularity.sort_values('popularity', ascending=False , inplace=True)\n",
    "\n",
    "    # select top 10 most popular coupons from the training dataset\n",
    "    top10 = df_popularity.coupon_id_hash.tolist()[:10]\n",
    "\n",
    "    return top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Coupon Popularity \n",
    "\n",
    "Based on how similar they are to the top 10 most popular training coupons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_coupon_populatiry(train_coupons_path, test_coupons_path):\n",
    "\n",
    "    # Find top 10 most popular coupons during training\n",
    "    top10 = top10_train_coupon_populatiry(train_purchases_path, train_visits_path)\n",
    "\n",
    "    # Load train and test coupon features\n",
    "    df_coupons_train_feat = pd.read_pickle(train_coupons_path)\n",
    "    df_coupons_test_feat = pd.read_pickle(test_coupons_path)\n",
    "    coupons_test_ids = df_coupons_test_feat.coupon_id_hash.values\n",
    "\n",
    "    id_cols = ['coupon_id_hash']\n",
    "    cat_cols = [c for c in df_coupons_train_feat.columns if c.endswith('_cat')]\n",
    "    num_cols = [c for c in df_coupons_train_feat.columns if\n",
    "        (c not in cat_cols) and (c not in id_cols)]\n",
    "\n",
    "    # Compute test coupon popularity\n",
    "    # 1. Normalize numerical columns (remember this process needs to happen all at once)\n",
    "    df_coupons_train_feat['flag'] = 0\n",
    "    df_coupons_test_feat['flag'] = 1\n",
    "\n",
    "    tmp_df = pd.concat(\n",
    "        [df_coupons_train_feat,df_coupons_test_feat],\n",
    "        ignore_index=True)\n",
    "\n",
    "    tmp_df_num = tmp_df[num_cols]\n",
    "    tmp_df_norm = (tmp_df_num-tmp_df_num.min())/(tmp_df_num.max()-tmp_df_num.min())\n",
    "    tmp_df[num_cols] = tmp_df_norm\n",
    "\n",
    "    # 2. one-hot encoding for categorical features\n",
    "    tmp_df[cat_cols] = tmp_df[cat_cols].astype('category')\n",
    "    tmp_df_dummy = pd.get_dummies(tmp_df, columns=cat_cols)\n",
    "    coupons_train_feat = tmp_df_dummy[tmp_df_dummy.flag==0]\n",
    "    coupons_test_feat = tmp_df_dummy[tmp_df_dummy.flag==1]\n",
    "\n",
    "    # get the values for the pairwise_distances method\n",
    "    df_top_10_feat = (coupons_train_feat[coupons_train_feat.coupon_id_hash.isin(top10)]\n",
    "        .reset_index()\n",
    "        .drop(['flag','coupon_id_hash','index'], axis=1)\n",
    "        )\n",
    "    coupons_test_feat = (coupons_test_feat\n",
    "        .drop(['flag','coupon_id_hash'], axis=1)\n",
    "        .values)\n",
    "\n",
    "    # 3. cosine distance\n",
    "    dist_mtx = pairwise_distances(coupons_test_feat, df_top_10_feat, metric='cosine')\n",
    "\n",
    "    # 4. Test coupons average distance to top 10 most popular coupons during training\n",
    "    mean_distances = np.apply_along_axis(np.mean, 1, dist_mtx)\n",
    "    df_test_popularity = pd.DataFrame({'coupon_id_hash': coupons_test_ids,\n",
    "        'popularity': 1-mean_distances})\n",
    "\n",
    "    return df_test_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dictionary of interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_interactions_dictionary(interest_path, test_purchases_path, test_visits_path, is_hot=True):\n",
    "    \"\"\"\n",
    "    Function to build a dictionary of real interactions: {user_id: [coupon_id1, ..., coupon_idN]}\n",
    "    parameters are trivial with perhaps the exception of is_hot\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    is_hot: boolean. Whether we are dealing with existing or unseen customers\n",
    "    \"\"\"\n",
    "        \n",
    "    # interest dataframe\n",
    "    df_interest = pd.read_pickle(interest_path)\n",
    "    train_users = df_interest.user_id_hash.unique()\n",
    "    del(df_interest)\n",
    "\n",
    "    # test activities\n",
    "    df_purchases_test = pd.read_pickle(test_purchases_path)\n",
    "    df_visits_test = pd.read_pickle(test_visits_path)\n",
    "    df_visits_test.rename(index=str, columns={'view_coupon_id_hash': 'coupon_id_hash'}, inplace=True)\n",
    "\n",
    "    # whether they are existing users (hot) or not (cold)\n",
    "    if is_hot:\n",
    "        df_vte = df_visits_test[df_visits_test.user_id_hash.isin(train_users)]\n",
    "        df_pte = df_purchases_test[df_purchases_test.user_id_hash.isin(train_users)]\n",
    "    else:\n",
    "        df_vte = df_visits_test[~df_visits_test.user_id_hash.isin(train_users)]\n",
    "        df_pte = df_purchases_test[~df_purchases_test.user_id_hash.isin(train_users)]\n",
    "\n",
    "    # dictionary of real interactions\n",
    "    id_cols = ['user_id_hash', 'coupon_id_hash']\n",
    "\n",
    "    df_interactions_test = pd.concat([df_pte[id_cols], df_vte[id_cols]], ignore_index=True)\n",
    "    df_interactions_test = (df_interactions_test.groupby('user_id_hash')\n",
    "        .agg({'coupon_id_hash': 'unique'})\n",
    "        .reset_index())\n",
    "    interactions_test_dict = pd.Series(df_interactions_test.coupon_id_hash.values,\n",
    "        index=df_interactions_test.user_id_hash).to_dict()\n",
    "\n",
    "    return interactions_test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dictionary of recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_recomendations_dictionary(ranking_df, ranking_metric='interest'):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    ranking_df: pandas dataframe with 3 cols (user_id, coupon_id, ranking_metric)    \n",
    "\n",
    "    Returns:\n",
    "    recomendations_dict: dictionary with recommendations: {user_id: recommended_coupon_id1, ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    df_ranked = ranking_df.sort_values(['user_id_hash', ranking_metric], ascending=[False, False])\n",
    "    df_ranked = (df_ranked\n",
    "        .groupby('user_id_hash')['coupon_id_hash']\n",
    "        .apply(list)\n",
    "        .reset_index())\n",
    "    recomendations_dict = pd.Series(df_ranked.coupon_id_hash.values,\n",
    "        index=df_ranked.user_id_hash).to_dict()\n",
    "\n",
    "    return recomendations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular recommendations function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_popular_recommendations(train_coupons_path, test_coupons_path, interest_path):\n",
    "\n",
    "    # test coupons popularity\n",
    "    df_test_popularity = test_coupon_populatiry(train_coupons_path, test_coupons_path)\n",
    "\n",
    "    # list of purchases and visits for new users\n",
    "    interactions_test_dict = build_interactions_dictionary(interest_path,\n",
    "        test_purchases_path, test_visits_path, is_hot=False)\n",
    "\n",
    "    # ranking dataframe\n",
    "    left = pd.DataFrame({'user_id_hash':list(interactions_test_dict.keys())})\n",
    "    left['key'] = 0\n",
    "    right = pd.read_pickle(test_coupons_path)[['coupon_id_hash']]\n",
    "    right['key'] = 0\n",
    "    df_test = (pd.merge(left, right, on='key', how='outer')\n",
    "        .drop('key', axis=1))\n",
    "    df_test = pd.merge(df_test, df_test_popularity, on='coupon_id_hash')\n",
    "\n",
    "    recomendations_dict = build_recomendations_dictionary(df_test, ranking_metric='popularity')\n",
    "\n",
    "    return recomendations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving onto lightGBM...\n",
    "\n",
    "### Building lightGBM training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lightgbm_train_set(train_coupons_path, train_users_path, interest_path):\n",
    "\n",
    "    # train coupon features\n",
    "    df_coupons_train_feat = pd.read_pickle(train_coupons_path)\n",
    "    drop_cols = [c for c in df_coupons_train_feat.columns\n",
    "        if ((not c.endswith('_cat')) or ('method2' in c)) and (c!='coupon_id_hash')]\n",
    "    df_coupons_train_cat_feat = df_coupons_train_feat.drop(drop_cols, axis=1)\n",
    "\n",
    "    # train user features\n",
    "    df_users_train_feat = pd.read_pickle(train_users_path)\n",
    "\n",
    "    # interest dataframe\n",
    "    df_interest = pd.read_pickle(interest_path)\n",
    "    train_users = df_interest.user_id_hash.unique()\n",
    "\n",
    "    df_train = pd.merge(df_interest, df_users_train_feat, on='user_id_hash')\n",
    "    df_train = pd.merge(df_train, df_coupons_train_cat_feat, on = 'coupon_id_hash')\n",
    "\n",
    "    # for the time being we ignore recency\n",
    "    df_train.drop(['user_id_hash','coupon_id_hash','recency_factor'], axis=1, inplace=True)\n",
    "    train = df_train.drop('interest', axis=1)\n",
    "    y_train = df_train.interest\n",
    "    all_cols = train.columns.tolist()\n",
    "    cat_cols = [c for c in train.columns if c.endswith(\"_cat\")]\n",
    "\n",
    "    return train.values, y_train, all_cols, cat_cols, drop_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building lightGBM training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lightgbm_test_set(train_users_path, test_coupons_path, test_purchases_path, \n",
    "                            test_visits_path, interest_path, drop_cols):\n",
    "\n",
    "    interactions_test_dict = build_interactions_dictionary(\n",
    "        interest_path, test_purchases_path, test_visits_path, is_hot=True)\n",
    "\n",
    "    df_users_train_feat = pd.read_pickle(train_users_path)\n",
    "    df_coupons_test_feat = (pd.read_pickle(test_coupons_path)\n",
    "        .drop(drop_cols, axis=1))\n",
    "\n",
    "    left = pd.DataFrame({'user_id_hash':list(interactions_test_dict.keys())})\n",
    "    left['key'] = 0\n",
    "    right = pd.read_pickle(test_coupons_path)[['coupon_id_hash']]\n",
    "    right['key'] = 0\n",
    "    df_test = (pd.merge(left, right, on='key', how='outer')\n",
    "        .drop('key', axis=1))\n",
    "    df_test = pd.merge(df_test, df_users_train_feat, on='user_id_hash')\n",
    "    df_test = pd.merge(df_test, df_coupons_test_feat, on = 'coupon_id_hash')\n",
    "    X_test = (df_test\n",
    "        .drop(['user_id_hash','coupon_id_hash'], axis=1)\n",
    "        .values)\n",
    "    df_rank = df_test[['user_id_hash','coupon_id_hash']]\n",
    "\n",
    "    return X_test, df_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a wrap up around MAP@10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mapk(interactions_dict, recomendations_dict):\n",
    "    actual = []\n",
    "    pred = []\n",
    "    for k,_ in recomendations_dict_hot.items():\n",
    "        actual.append(list(interactions_dict[k]))\n",
    "        pred.append(list(recomendations_dict[k]))\n",
    "    return mapk(actual,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.3 FINAL SOLUTION: Most Popular + lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dir = \"../datasets/Ponpare/data_processed/\"\n",
    "train_dir = \"ftrain\"\n",
    "test_dir = \"test\"\n",
    "model_dir = \"models\"\n",
    "\n",
    "# Training datasets\n",
    "train_visits_path = os.path.join(inp_dir,train_dir, 'df_visits_train.p')\n",
    "train_purchases_path = os.path.join(inp_dir,train_dir, 'df_purchases_train.p')\n",
    "train_coupons_path = os.path.join(inp_dir,train_dir, 'df_coupons_train_feat.p')\n",
    "train_users_path = os.path.join(inp_dir,train_dir, 'df_users_train_feat.p')\n",
    "interest_path = os.path.join(inp_dir,train_dir, 'df_interest.p')\n",
    "\n",
    "# Testing datasets\n",
    "test_visits_path = os.path.join(inp_dir,test_dir, 'df_visits_test.p')\n",
    "test_purchases_path = os.path.join(inp_dir,test_dir, 'df_purchases_test.p')\n",
    "test_coupons_path = os.path.join(inp_dir,test_dir, 'df_coupons_test_feat.p')\n",
    "test_users_path = os.path.join(inp_dir,test_dir, 'df_users_test_feat.p')\n",
    "\n",
    "# Remember we have already optimize lightGBM, so we can upload the best \n",
    "# parameters or the model directly\n",
    "best_params_path = os.path.join(inp_dir, model_dir, 'gbm_optimal_parameters.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.3.1 Most Popular Recommendations for new customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomendations_dict_cold = most_popular_recommendations(train_coupons_path,\n",
    "    test_coupons_path, interest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.3.2 LightGBM Recommendations for existing customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,y_train,all_cols,cat_cols,drop_cols = build_lightgbm_train_set(\n",
    "    train_coupons_path,\n",
    "    train_users_path,\n",
    "    interest_path)\n",
    "X_test, df_rank = build_lightgbm_test_set(\n",
    "    train_users_path,\n",
    "    test_coupons_path,\n",
    "    test_purchases_path,\n",
    "    test_visits_path,\n",
    "    interest_path,\n",
    "    drop_cols)\n",
    "\n",
    "best = pickle.load(open(best_params_path, \"rb\"))\n",
    "model = lgb.LGBMRegressor(**best)\n",
    "model.fit(train,y_train,feature_name=all_cols,categorical_feature=cat_cols)\n",
    "preds = model.predict(X_test)\n",
    "df_rank['interest'] = preds\n",
    "recomendations_dict_hot = build_recomendations_dictionary(df_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.3.3 Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04239416493503196\n"
     ]
    }
   ],
   "source": [
    "recomendations_dict = recomendations_dict_cold.copy()\n",
    "recomendations_dict.update(recomendations_dict_hot)\n",
    "\n",
    "interactions_dict_cold = build_interactions_dictionary(\n",
    "    interest_path,\n",
    "    test_purchases_path,\n",
    "    test_visits_path,\n",
    "    is_hot=False)\n",
    "interactions_dict_hot = build_interactions_dictionary(\n",
    "    interest_path,\n",
    "    test_purchases_path,\n",
    "    test_visits_path,\n",
    "    is_hot=True)\n",
    "interactions_dict = interactions_dict_cold.copy()\n",
    "interactions_dict.update(interactions_dict_hot)\n",
    "\n",
    "final_mapk = compute_mapk(interactions_dict, recomendations_dict)\n",
    "print(final_mapk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP@10=0.042 pretty good! And with this we are done...building the algorithm and running al offline test. \n",
    "\n",
    "Now is when the real fun begins, where (data) scientists and engineers sit down and design a good online test and a decent production pipeline. But this is an entire different history. \n",
    "\n",
    "THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_jrz)",
   "language": "python",
   "name": "conda_jrz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
