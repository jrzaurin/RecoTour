{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2\n",
    "\n",
    "### 2.1 Split the dataset\n",
    "\n",
    "Note that for the exercise in this repo we will not be using the test dataset provided by Kaggle. This dataset has no information on whether the recommendations are good or bad (of course. This is tested at kaggle's side) and we are not taking part of any competition. Therefore, we will carry on as if the training dataset is all we have, and we will split the training dataset into train, validation and testing. \n",
    "\n",
    "As I mentioned in `Chapter 0` there is a temporal component to this problem. As written in the Kaggle's site: *\"You are provided with a year of transactional data for 22,873 users on the site ponpare.jp. The training set spans the dates 2011-07-01 to 2012-06-23. The test set spans the week after the end of the training set, 2012-06-24 to 2012-06-30. The goal of the competition is to recommend a ranked list of coupons for each user in the dataset (found in user_list.csv). Your predictions are scored against the actual coupon purchases, made during the test set week, of the 310 possible test set coupons.\"*\n",
    "\n",
    "Therefore, we will split the dataset according to this set up, this is: we will use the last week of data as test dataset, the previous week as validation, and the rest of the data will be our training data. However, the code below is written so that one has flexibility to choose the testing period (referred as `tp`).\n",
    "\n",
    "Before we move into the code, let's just clarify a few additional aspects and notation:\n",
    "\n",
    "1. **Interactions**: I refer to coupon visits (views) and purchases as \"interactions\".\n",
    "\n",
    "\n",
    "2. **Coupons**: based on the testing set provided by kaggle, testing coupons are selected based on the dispfrom column. \n",
    "\n",
    "    In addition, kaggle provides coupon features for the testing coupons. Therefore I will assume that next week's coupons are received with enough time in advance so that we can use *ALL* coupons (train, validation and test coupons) to engineer the features and split the dataset afterwards. Programmatically, this scenario is easier since no new features (as the result of, for example, one-hot enconding) will appear during testing. Given the amount of coupons in this example, this represents no limitation at all. \n",
    "    \n",
    "    A second possible scenario is one where we might need to recommend coupons as they come. In this case, we would only have time to do some fast processing (LabelEnconding perhaps) and, since a new coupon has never been seen before we would have to recommend based on some similarity metric between the new and old coupons (which would also need to be computed fast). \n",
    "\n",
    "\n",
    "3. **Users**: for the excercise here we will only concentrate in users that have been seen during training. In the real world you need to recommend to everyone, existing and new users (i.e. you have to face the so called cold start problem for users in this case). \n",
    "\n",
    "    For users that have not interacted at all in the past we know nothing, unless they fill a form or register in your site in which case we might have some demographic information. On top of that, in the example used here, remember that no one has interacted with the upcoming coupons. Altogether, it is straightforward to understand that is hard to recommend well to these users. \n",
    "    \n",
    "    An easy solution would be recommending new coupons that resemble the most popular coupons in the past. If, in addition, we have some demographic information (age, gender, location) we might want to constrain the definition of *\"similar coupons\"* to groups of similar demographics. In other words, recommend new coupons that are similar to the most popular coupons amongst users of similar age, gender, etc...\n",
    "\n",
    "Let's start defining some useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "inp_dir = \"../datasets/Ponpare/data_translated\"\n",
    "out_dir = \"../datasets/Ponpare/data_processed\"\n",
    "# Testing period\n",
    "tp = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reg_date</th>\n",
       "      <th>sex_id</th>\n",
       "      <th>age</th>\n",
       "      <th>withdraw_date</th>\n",
       "      <th>pref_name</th>\n",
       "      <th>user_id_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-03-28 14:14:18</td>\n",
       "      <td>f</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d9dca3cb44bab12ba313eaa681f663eb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-05-18 00:41:48</td>\n",
       "      <td>f</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tokyo</td>\n",
       "      <td>560574a339f1b25e57b0221e486907ed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-06-13 16:36:58</td>\n",
       "      <td>m</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aichi</td>\n",
       "      <td>e66ae91b978b3229f8fd858c80615b73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-02-08 12:56:15</td>\n",
       "      <td>m</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43fc18f32eafb05713ec02935e2c2825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-05-22 23:43:56</td>\n",
       "      <td>m</td>\n",
       "      <td>62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kanagawa</td>\n",
       "      <td>dc6df8aa860f8db0d710ce9d4839840f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             reg_date sex_id  age withdraw_date pref_name  \\\n",
       "0 2012-03-28 14:14:18      f   25           NaN       NaN   \n",
       "1 2011-05-18 00:41:48      f   34           NaN     tokyo   \n",
       "2 2011-06-13 16:36:58      m   41           NaN     aichi   \n",
       "3 2012-02-08 12:56:15      m   25           NaN       NaN   \n",
       "4 2011-05-22 23:43:56      m   62           NaN  kanagawa   \n",
       "\n",
       "                       user_id_hash  \n",
       "0  d9dca3cb44bab12ba313eaa681f663eb  \n",
       "1  560574a339f1b25e57b0221e486907ed  \n",
       "2  e66ae91b978b3229f8fd858c80615b73  \n",
       "3  43fc18f32eafb05713ec02935e2c2825  \n",
       "4  dc6df8aa860f8db0d710ce9d4839840f  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the master list of users in the dataset\n",
    "df_users = pd.read_csv(os.path.join(inp_dir,\"user_list.csv\"))\n",
    "df_users['reg_date'] =  pd.to_datetime(df_users.reg_date, infer_datetime_format=True)\n",
    "\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capsule_text</th>\n",
       "      <th>genre_name</th>\n",
       "      <th>price_rate</th>\n",
       "      <th>catalog_price</th>\n",
       "      <th>discount_price</th>\n",
       "      <th>dispfrom</th>\n",
       "      <th>dispend</th>\n",
       "      <th>dispperiod</th>\n",
       "      <th>validfrom</th>\n",
       "      <th>validend</th>\n",
       "      <th>validperiod</th>\n",
       "      <th>usable_date_mon</th>\n",
       "      <th>usable_date_tue</th>\n",
       "      <th>usable_date_wed</th>\n",
       "      <th>usable_date_thu</th>\n",
       "      <th>usable_date_fri</th>\n",
       "      <th>usable_date_sat</th>\n",
       "      <th>usable_date_sun</th>\n",
       "      <th>usable_date_holiday</th>\n",
       "      <th>usable_date_before_holiday</th>\n",
       "      <th>large_area_name</th>\n",
       "      <th>ken_name</th>\n",
       "      <th>small_area_name</th>\n",
       "      <th>coupon_id_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "      <td>50</td>\n",
       "      <td>3000</td>\n",
       "      <td>1500</td>\n",
       "      <td>2011-07-08 12:00:00</td>\n",
       "      <td>2011-07-09 12:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-10</td>\n",
       "      <td>2011-12-08</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>kanto</td>\n",
       "      <td>saitama</td>\n",
       "      <td>saitama</td>\n",
       "      <td>6b263844241eea98c5a97f1335ea82af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "      <td>51</td>\n",
       "      <td>2080</td>\n",
       "      <td>1000</td>\n",
       "      <td>2011-07-01 12:00:00</td>\n",
       "      <td>2011-07-02 12:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-03</td>\n",
       "      <td>2011-12-04</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>kanto</td>\n",
       "      <td>chiba</td>\n",
       "      <td>chiba</td>\n",
       "      <td>cc031f250e8bad1e24060263b9fc0ddd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "      <td>50</td>\n",
       "      <td>7000</td>\n",
       "      <td>3500</td>\n",
       "      <td>2011-07-12 12:00:00</td>\n",
       "      <td>2011-07-15 12:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-07-16</td>\n",
       "      <td>2012-01-11</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>kanto</td>\n",
       "      <td>chiba</td>\n",
       "      <td>chiba</td>\n",
       "      <td>ba5e9b7453ca52ff711635a5d2e8102d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "      <td>50</td>\n",
       "      <td>3000</td>\n",
       "      <td>1500</td>\n",
       "      <td>2011-07-09 12:00:00</td>\n",
       "      <td>2011-07-11 12:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-07-12</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>kanto</td>\n",
       "      <td>chiba</td>\n",
       "      <td>chiba</td>\n",
       "      <td>3e1ffbedca3569f9e8032d401e8cb4e6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "      <td>50</td>\n",
       "      <td>2000</td>\n",
       "      <td>1000</td>\n",
       "      <td>2011-07-05 12:00:00</td>\n",
       "      <td>2011-07-06 12:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>2011-12-30</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>kanto</td>\n",
       "      <td>chiba</td>\n",
       "      <td>chiba</td>\n",
       "      <td>782934b6c815b4030ea204eef7d4a734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  capsule_text genre_name  price_rate  catalog_price  discount_price  \\\n",
       "0         Food       Food          50           3000            1500   \n",
       "1         Food       Food          51           2080            1000   \n",
       "2         Food       Food          50           7000            3500   \n",
       "3         Food       Food          50           3000            1500   \n",
       "4         Food       Food          50           2000            1000   \n",
       "\n",
       "             dispfrom             dispend  dispperiod  validfrom   validend  \\\n",
       "0 2011-07-08 12:00:00 2011-07-09 12:00:00           1 2011-07-10 2011-12-08   \n",
       "1 2011-07-01 12:00:00 2011-07-02 12:00:00           1 2011-07-03 2011-12-04   \n",
       "2 2011-07-12 12:00:00 2011-07-15 12:00:00           3 2011-07-16 2012-01-11   \n",
       "3 2011-07-09 12:00:00 2011-07-11 12:00:00           2 2011-07-12 2011-12-01   \n",
       "4 2011-07-05 12:00:00 2011-07-06 12:00:00           1 2011-07-07 2011-12-30   \n",
       "\n",
       "   validperiod  usable_date_mon  usable_date_tue  usable_date_wed  \\\n",
       "0        151.0              1.0              1.0              1.0   \n",
       "1        154.0              1.0              1.0              1.0   \n",
       "2        179.0              0.0              1.0              1.0   \n",
       "3        142.0              1.0              1.0              1.0   \n",
       "4        176.0              1.0              1.0              0.0   \n",
       "\n",
       "   usable_date_thu  usable_date_fri  usable_date_sat  usable_date_sun  \\\n",
       "0              1.0              0.0              0.0              1.0   \n",
       "1              1.0              1.0              1.0              1.0   \n",
       "2              1.0              1.0              1.0              1.0   \n",
       "3              1.0              0.0              0.0              1.0   \n",
       "4              1.0              0.0              0.0              1.0   \n",
       "\n",
       "   usable_date_holiday  usable_date_before_holiday large_area_name ken_name  \\\n",
       "0                  1.0                         0.0           kanto  saitama   \n",
       "1                  1.0                         1.0           kanto    chiba   \n",
       "2                  1.0                         1.0           kanto    chiba   \n",
       "3                  1.0                         1.0           kanto    chiba   \n",
       "4                  1.0                         0.0           kanto    chiba   \n",
       "\n",
       "  small_area_name                    coupon_id_hash  \n",
       "0         saitama  6b263844241eea98c5a97f1335ea82af  \n",
       "1           chiba  cc031f250e8bad1e24060263b9fc0ddd  \n",
       "2           chiba  ba5e9b7453ca52ff711635a5d2e8102d  \n",
       "3           chiba  3e1ffbedca3569f9e8032d401e8cb4e6  \n",
       "4           chiba  782934b6c815b4030ea204eef7d4a734  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# master list of coupons which are considered part of the training set\n",
    "df_coupons = pd.read_csv(os.path.join(inp_dir,\"coupon_list_train.csv\"))\n",
    "df_coupons['dispfrom'] = pd.to_datetime(df_coupons.dispfrom, infer_datetime_format=True)\n",
    "df_coupons['dispend'] = pd.to_datetime(df_coupons.dispend, infer_datetime_format=True)\n",
    "df_coupons['validfrom'] = pd.to_datetime(df_coupons.validfrom, infer_datetime_format=True)\n",
    "df_coupons['validend'] = pd.to_datetime(df_coupons.validend, infer_datetime_format=True)\n",
    "\n",
    "df_coupons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_flg</th>\n",
       "      <th>i_date</th>\n",
       "      <th>page_serial</th>\n",
       "      <th>referrer_hash</th>\n",
       "      <th>view_coupon_id_hash</th>\n",
       "      <th>user_id_hash</th>\n",
       "      <th>session_id_hash</th>\n",
       "      <th>purchaseid_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-03-28 14:15:00</td>\n",
       "      <td>7</td>\n",
       "      <td>7d3892e54acb559ae36c459978489330</td>\n",
       "      <td>34c48f84026e08355dc3bd19b427f09a</td>\n",
       "      <td>d9dca3cb44bab12ba313eaa681f663eb</td>\n",
       "      <td>673af822615593249e7c6a9a1a6bbb1a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-03-28 14:17:28</td>\n",
       "      <td>9</td>\n",
       "      <td>7d3892e54acb559ae36c459978489330</td>\n",
       "      <td>34c48f84026e08355dc3bd19b427f09a</td>\n",
       "      <td>d9dca3cb44bab12ba313eaa681f663eb</td>\n",
       "      <td>673af822615593249e7c6a9a1a6bbb1a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-03-28 14:20:05</td>\n",
       "      <td>16</td>\n",
       "      <td>7d3892e54acb559ae36c459978489330</td>\n",
       "      <td>17c450c3b470c045d35ec22b02daa690</td>\n",
       "      <td>d9dca3cb44bab12ba313eaa681f663eb</td>\n",
       "      <td>673af822615593249e7c6a9a1a6bbb1a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-03-28 14:23:16</td>\n",
       "      <td>18</td>\n",
       "      <td>7d3892e54acb559ae36c459978489330</td>\n",
       "      <td>91a15e6a95d09e5e01b50747833b317d</td>\n",
       "      <td>d9dca3cb44bab12ba313eaa681f663eb</td>\n",
       "      <td>673af822615593249e7c6a9a1a6bbb1a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-03-28 14:26:25</td>\n",
       "      <td>20</td>\n",
       "      <td>7d3892e54acb559ae36c459978489330</td>\n",
       "      <td>96fcbc8f6e45d5a2de1661eb140c6e82</td>\n",
       "      <td>d9dca3cb44bab12ba313eaa681f663eb</td>\n",
       "      <td>673af822615593249e7c6a9a1a6bbb1a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   purchase_flg              i_date  page_serial  \\\n",
       "0             0 2012-03-28 14:15:00            7   \n",
       "1             0 2012-03-28 14:17:28            9   \n",
       "2             0 2012-03-28 14:20:05           16   \n",
       "3             0 2012-03-28 14:23:16           18   \n",
       "4             0 2012-03-28 14:26:25           20   \n",
       "\n",
       "                      referrer_hash               view_coupon_id_hash  \\\n",
       "0  7d3892e54acb559ae36c459978489330  34c48f84026e08355dc3bd19b427f09a   \n",
       "1  7d3892e54acb559ae36c459978489330  34c48f84026e08355dc3bd19b427f09a   \n",
       "2  7d3892e54acb559ae36c459978489330  17c450c3b470c045d35ec22b02daa690   \n",
       "3  7d3892e54acb559ae36c459978489330  91a15e6a95d09e5e01b50747833b317d   \n",
       "4  7d3892e54acb559ae36c459978489330  96fcbc8f6e45d5a2de1661eb140c6e82   \n",
       "\n",
       "                       user_id_hash                   session_id_hash  \\\n",
       "0  d9dca3cb44bab12ba313eaa681f663eb  673af822615593249e7c6a9a1a6bbb1a   \n",
       "1  d9dca3cb44bab12ba313eaa681f663eb  673af822615593249e7c6a9a1a6bbb1a   \n",
       "2  d9dca3cb44bab12ba313eaa681f663eb  673af822615593249e7c6a9a1a6bbb1a   \n",
       "3  d9dca3cb44bab12ba313eaa681f663eb  673af822615593249e7c6a9a1a6bbb1a   \n",
       "4  d9dca3cb44bab12ba313eaa681f663eb  673af822615593249e7c6a9a1a6bbb1a   \n",
       "\n",
       "  purchaseid_hash  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the viewing log of users browsing coupons during training\n",
    "df_visits = pd.read_csv(os.path.join(inp_dir,\"coupon_visit_train.csv\"))\n",
    "df_visits['i_date'] = pd.to_datetime(df_visits.i_date, infer_datetime_format=True)\n",
    "\n",
    "df_visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_count</th>\n",
       "      <th>i_date</th>\n",
       "      <th>small_area_name</th>\n",
       "      <th>purchaseid_hash</th>\n",
       "      <th>user_id_hash</th>\n",
       "      <th>coupon_id_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-03-28 15:06:06</td>\n",
       "      <td>hyogo</td>\n",
       "      <td>c820a8882374a4e472f0984a8825893f</td>\n",
       "      <td>d9dca3cb44bab12ba313eaa681f663eb</td>\n",
       "      <td>34c48f84026e08355dc3bd19b427f09a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-04 23:52:54</td>\n",
       "      <td>tokyo_ginza</td>\n",
       "      <td>1b4eb2435421ede98c8931c42e8220ec</td>\n",
       "      <td>560574a339f1b25e57b0221e486907ed</td>\n",
       "      <td>767673b7a777854a92b73b0934ddfae7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-16 00:52:49</td>\n",
       "      <td>tokyo_ebisu</td>\n",
       "      <td>36b5f9ba46c44b65587d0b16f2e4c77f</td>\n",
       "      <td>560574a339f1b25e57b0221e486907ed</td>\n",
       "      <td>4f3b5b91d9831192557c056022fdc1f2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-16 00:54:53</td>\n",
       "      <td>tokyo_ebisu</td>\n",
       "      <td>2f30f46937cc9004774e576914b2aa1a</td>\n",
       "      <td>560574a339f1b25e57b0221e486907ed</td>\n",
       "      <td>4f3b5b91d9831192557c056022fdc1f2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-16 00:55:52</td>\n",
       "      <td>tokyo_ebisu</td>\n",
       "      <td>4d000c64a55ac573d0ae1a8f03677f50</td>\n",
       "      <td>560574a339f1b25e57b0221e486907ed</td>\n",
       "      <td>4f3b5b91d9831192557c056022fdc1f2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_count              i_date small_area_name  \\\n",
       "0           1 2012-03-28 15:06:06           hyogo   \n",
       "1           1 2011-07-04 23:52:54     tokyo_ginza   \n",
       "2           1 2011-07-16 00:52:49     tokyo_ebisu   \n",
       "3           1 2011-07-16 00:54:53     tokyo_ebisu   \n",
       "4           1 2011-07-16 00:55:52     tokyo_ebisu   \n",
       "\n",
       "                    purchaseid_hash                      user_id_hash  \\\n",
       "0  c820a8882374a4e472f0984a8825893f  d9dca3cb44bab12ba313eaa681f663eb   \n",
       "1  1b4eb2435421ede98c8931c42e8220ec  560574a339f1b25e57b0221e486907ed   \n",
       "2  36b5f9ba46c44b65587d0b16f2e4c77f  560574a339f1b25e57b0221e486907ed   \n",
       "3  2f30f46937cc9004774e576914b2aa1a  560574a339f1b25e57b0221e486907ed   \n",
       "4  4d000c64a55ac573d0ae1a8f03677f50  560574a339f1b25e57b0221e486907ed   \n",
       "\n",
       "                     coupon_id_hash  \n",
       "0  34c48f84026e08355dc3bd19b427f09a  \n",
       "1  767673b7a777854a92b73b0934ddfae7  \n",
       "2  4f3b5b91d9831192557c056022fdc1f2  \n",
       "3  4f3b5b91d9831192557c056022fdc1f2  \n",
       "4  4f3b5b91d9831192557c056022fdc1f2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the purchase log of users buying coupons during training\n",
    "df_purchases = pd.read_csv(os.path.join(inp_dir,\"coupon_detail_train.csv\"))\n",
    "df_purchases['i_date'] = pd.to_datetime(df_purchases.i_date, infer_datetime_format=True)\n",
    "\n",
    "df_purchases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the most recent date during training using interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-06-23 23:59:48\n"
     ]
    }
   ],
   "source": [
    "df_interactions_l = [df_visits, df_purchases]\n",
    "most_recent = []\n",
    "for df in df_interactions_l:\n",
    "    for col in df.columns:\n",
    "        if col == 'i_date':\n",
    "            most_recent.append(df[col].max())\n",
    "present = np.max(most_recent)\n",
    "\n",
    "print(present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute a \"days to present\" column that will be used to split the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df_visits = pd.DataFrame({'present': [present]*df_visits.shape[0]})\n",
    "df_visits['days_to_present'] = (tmp_df_visits['present'] - df_visits['i_date'])\n",
    "df_visits['days_to_present'] = df_visits.days_to_present.dt.days\n",
    "\n",
    "tmp_df_detail = pd.DataFrame({'present': [present]*df_purchases.shape[0]})\n",
    "df_purchases['days_to_present'] = (tmp_df_detail['present'] - df_purchases['i_date'])\n",
    "df_purchases['days_to_present'] = df_purchases.days_to_present.dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, for the excercise here we will only use customers that were seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df_users = pd.DataFrame({'present': [present]*df_users.shape[0]})\n",
    "df_users['days_to_present'] = (tmp_df_users['present'] - df_users['reg_date'])\n",
    "df_users['days_to_present'] = df_users.days_to_present.dt.days\n",
    "\n",
    "tmp_df_coupons = pd.DataFrame({'present': [present]*df_coupons.shape[0]})\n",
    "df_coupons['days_to_present'] = (tmp_df_detail['present'] - df_coupons['dispfrom'])\n",
    "df_coupons['days_to_present'] = df_coupons.days_to_present.dt.days\n",
    "\n",
    "# clean a bit\n",
    "del(tmp_df_visits,tmp_df_detail,tmp_df_users,tmp_df_coupons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore a series of scenarios that I will discuss when we get there. For now and for convenience, let's split all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding flags whether train/test/valid\n",
    "df_visits['days_to_present_flag'] = df_visits.days_to_present.apply(\n",
    "    lambda x: 0 if x<=tp-1 else 1 if ((x>tp-1) and (x<=(tp*2)-1)) else 2)\n",
    "df_purchases['days_to_present_flag'] = df_purchases.days_to_present.apply(\n",
    "    lambda x: 0 if x<=tp-1 else 1 if ((x>tp-1) and (x<=(tp*2)-1)) else 2)\n",
    "df_users['days_to_present_flag'] = df_users.days_to_present.apply(\n",
    "    lambda x: 0 if x<=tp-1 else 1 if ((x>tp-1) and (x<=(tp*2)-1)) else 2)\n",
    "df_coupons['days_to_present_flag'] = df_coupons.days_to_present.apply(\n",
    "    lambda x: 0 if x<=tp-1 else 1 if ((x>tp-1) and (x<=(tp*2)-1)) else 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: splitting visits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/core/frame.py:3694: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: splitting purchases\n",
      "INFO: splitting users\n",
      "INFO: splitting coupons\n"
     ]
    }
   ],
   "source": [
    "df_l = ['df_visits', 'df_purchases', 'df_users', 'df_coupons']\n",
    "for df in df_l:\n",
    "    print('INFO: splitting {}'.format(df.split('_')[1]))\n",
    "    tmp_train = eval(df)[eval(df)['days_to_present_flag'] == 2]\n",
    "    tmp_valid = eval(df)[eval(df)['days_to_present_flag'] == 1]\n",
    "    tmp_test  = eval(df)[eval(df)['days_to_present_flag'] == 0]\n",
    "\n",
    "    tmp_train.drop('days_to_present_flag', axis=1, inplace=True)\n",
    "    tmp_valid.drop('days_to_present_flag', axis=1, inplace=True)\n",
    "    tmp_test.drop('days_to_present_flag', axis=1, inplace=True)\n",
    "\n",
    "    tmp_train.reset_index(drop=True, inplace=True)\n",
    "    tmp_valid.reset_index(drop=True, inplace=True)\n",
    "    tmp_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    tmp_train.to_pickle(open(os.path.join(out_dir,'train',df+'_train.p'), 'wb'))\n",
    "    tmp_valid.to_pickle(open(os.path.join(out_dir,'valid',df+'_valid.p'), 'wb'))\n",
    "    tmp_test.to_pickle(open(os.path.join(out_dir,'test',df+'_test.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the following files\n",
    "\n",
    "```\n",
    "df_coupons_train.p       \n",
    "df_purchases_train.p \n",
    "df_visits_train.p   \n",
    "df_users_train.p         \n",
    "```\n",
    "\n",
    "Should be in `../datasets/Ponpare/data_processed/train/`. The same applies to the `test` and `valid` directories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
