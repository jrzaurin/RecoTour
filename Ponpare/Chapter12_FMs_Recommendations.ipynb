{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 12\n",
    "\n",
    "### 12.1 Factorization Machines\n",
    "\n",
    "The text in this notebook is based on this [paper](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf). [Chao Ma](https://github.com/aksnzhy) is the author of the [xlearn](http://xlearn-doc.readthedocs.io/en/latest/start.html) package that we will use in this and the next chapter. It is important to mention that `xlearn` is still not production ready, but it looks really promising. \n",
    "\n",
    "The differentiating method in `xlearn` is Field Aware Factorization Machines (FFMs), which has been the winning method in a couple of click through rate (CTR) prediction competitions. However, the library also includes [Factorization Machines](https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf) (FMs) and linear methods for large datasets. Therefore, we will first explore FMs and then we will move onto FFMs in the next Chapter/notebook.\n",
    "\n",
    "There are a number of packages for Factorization Machines in python: \n",
    "\n",
    "\n",
    "1. [pyFM](https://github.com/coreylynch/pyFM)\n",
    "2. [pywFM](https://github.com/jfloff/pywFM)\n",
    "3. [fastFM](https://github.com/jfloff/pywFM)\n",
    "4. [ligtFM](https://github.com/lyst/lightfm)\n",
    "\n",
    "While I am familiar with `pyFM` and `lightFM`, I have never used the `pywFM` and only \"played a bit\" with `fastFM`. To be honest, with the exception of the `lightFM`, I do not think any of them are production ready. Let me clarify that `lighFM` is not strictly speaking FMs, but a hybrid matrix factorisation model. However, given the resemble between methods (see Section 3 of Maciej Kula's [paper](https://arxiv.org/pdf/1507.08439.pdf)), the author decided to call it `lightFM`, and I think it must be included in the list above.\n",
    "\n",
    "So, what are factorization machines? Let's see if I can answer this question with some math and plane English.\n",
    "\n",
    "Let's assume we have $m$ items that are displayed in a site. For each item we have $(y_i, \\boldsymbol{x}_i)$ where $i=1,...,n$, $\\boldsymbol {x}_i$ is an n-dimensional feature vector and $y_i$ is our *\"target\"*, for example whether a user will click on a specific link to an item. In this scenario (logistic regression, click or not) the model can be obtained by solving the following optimization problem, i.e. minimizing the log-loss with regularization:\n",
    "\n",
    "$$ \\min\\limits_{w} \\frac{\\lambda}{2} ||w||^{2}_{2} +  \\sum_{i=1}^{m} log(1+exp(-y_i\\phi_{LM}(\\boldsymbol{w},\\boldsymbol{x}_i)))$$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter and $\\phi_{LM}$ is linear:\n",
    "\n",
    "$$\\phi_{LM}(\\boldsymbol{w},\\boldsymbol{x}) = \\boldsymbol{w} \\cdot \\boldsymbol{x}$$\n",
    "\n",
    "Let's use the example in the [Yuchin Juan, et al 2017](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf) paper. \n",
    "\n",
    "| | |Publisher (P)| Advertiser (A)| \n",
    "|-----|-----|--------|--------|\n",
    "| +80 | −20 |  ESPN  | Nike   |\n",
    "| +10 | −90 |  ESPN  | Gucci  |\n",
    "| +0  | −1  |  ESPN  | Adidas |\n",
    "| +15 | −85 |  Vogue | Nike   |\n",
    "| +90 | −10 |  Vogue | Gucci  |\n",
    "| +10 | −90 |  Vogue | Adidas |\n",
    "| +85 | −15 |  NBC   | Nike   |\n",
    "| +0  | −0  |  NBC   | Gucci  |\n",
    "| +90 | −10 |  NBC   | Adidas |\n",
    "\n",
    "where + (−) represents the number of clicked (unclicked) impressions. And a single instance:\n",
    "\n",
    "|  |Publisher (P)| Advertiser (A)| Gender (G)| \n",
    "|----|-------|--------|--|\n",
    "| YES| ESPN  | Nike   |M |\n",
    "\n",
    "With a linear model, the click probability for this observations would be calculated as:\n",
    "\n",
    "$$\\phi_{LM} = \\boldsymbol{w}_0 + \\boldsymbol{w}_\\text{ESPN} x_\\text{ESPN} + \\boldsymbol{w}_\\text{Nike} x_\\text{Nike} + \\boldsymbol{w}_\\text{M} x_\\text{M} = \\boldsymbol{w}_0 + \\boldsymbol{w}_\\text{ESPN} + \\boldsymbol{w}_\\text{Nike} + \\boldsymbol{w}_\\text{M}$$ \n",
    "\n",
    "Note that `Publisher`, `Advertiser` and `Gender` are categorical features. Once one-hot encoded, $x_\\text{ESPN}$ will be 1 if the value for the feature Publisher is ESPN and 0 otherwise, and hence the equality in the expression above.\n",
    "\n",
    "Of course, a limitation of that model is that it does not capture feature interactions. A way to address that limitation is using polynomial models, for example, of degree 2: \n",
    "\n",
    "$$\\phi_\\text{Poli2} = \\boldsymbol{w}_0 + \\boldsymbol{w}_\\text{ESPN} x_\\text{ESPN} + \\boldsymbol{w}_\\text{Nike} x_\\text{Nike} + \\boldsymbol{w}_\\text{M} x_\\text{M} + \\boldsymbol{w}_\\text{ESPN, Nike} x_\\text{ESPN} x_\\text{Nike} + \\boldsymbol{w}_\\text{Nike, M} x_\\text{ESPN} x_\\text{M} + \\boldsymbol{w}_\\text{ESPN, M} x_\\text{ESPN} x_\\text{M}$$ \n",
    "\n",
    "or in a more compact notation:\n",
    "\n",
    "$$\\phi_\\text{Poli2}(\\boldsymbol{w}, \\boldsymbol{x}) = \\boldsymbol{w}_{0} + \\sum_{i=1}^{n} w_i x_i +  \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} \\boldsymbol{w}_{h(i,j)}x_{i} x_{j}$$\n",
    "\n",
    "where $h(i, j)$ is a function encoding $i$ and $j$ into a natural number. The complexity of computing that expression is $O(\\overline{n}^2)$, where $\\overline{n}$ is the average non-negative values per instance. One drawback of the Poly2 solution is the \"limited learning\". For example, in our first table, there is only one example of the pair (ESPN, Adidas), and the user did not click. For Poly2, it is likely that a very negative weight $w_\\text{ESPN,Adidas}$ is learned. Also, there are no examples for the pair (NBC, Gucci) and in consequence, no weight will be learned. Such limiation is overcome by Factorization Machines.\n",
    "\n",
    "Factorization Machines (FMs), proposed by Steffen Rendle in his [paper](https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf), implicitly learn a latent vector for each feature. Each latent vector contains $k$ latent factors, where $k$ is a user-specified parameter. Then, the effect of feature conjunction is modelled by the inner product of two latent vectors: \n",
    "\n",
    "$$\\phi_\\text{FM}(\\boldsymbol{w}, \\boldsymbol{x}) = \\boldsymbol{w}_{0} + \\sum_{i=1}^{n} w_i x_i + \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} (\\boldsymbol{w}_i \\cdot \\boldsymbol{w}_j) x_i x_j$$\n",
    "\n",
    "One can prove that the complexity of computing that expression is $O(\\overline{n}k)$. Since one would expect the number of latent factors to be smaller that the average number of non zero elements per instance, FMs are normally faster than Poly2 approaches.\n",
    "\n",
    "In addition, for FMs the prediction of (ESPN, Adidas) is determined by $w_\\text{ESPN} \\cdot w_\\text{Adidas}$, and $w_\\text{ESPN}$ and $w_\\text{Adidas}$ are also learned from other pairs (e.g. (ESPN, Nike), (NBC, Adidas)). Therefore, it is likely that the corresponding prediction will be more accurate. Furthermore, even though there is no training data for the pair (NBC, Gucci), because $w_\\text{ESPN}$ and $w_\\text{Adidas}$ can be learned from other pairs, it is still possible to do meaningful predictions with those weights.\n",
    "\n",
    "Although the formulation of the problem is different, the idea is similar to the Matrix Factorization (MF) technique described in the previous chapter. There, the ratings for an item where obtained as the inner product of two latent vectors (item and user latent vectors) with $k$ latent factors. \n",
    "\n",
    "One difference is that in the example here, the latent vectors are associated to features (Publisher, Advertiser, etc). However, user and item vectors can also be learned, along with feature latent vectors, when using FMs. You just need to encode them as part of the the sparse matrix of features (see Rendle 2010 his Figure 1). Therefore, if features are important when predicting ratings or CTR, FMs are likely to perform better than MF.\n",
    "\n",
    "Note that I say: *\"if features are important\"*. This is because I sometimes find that using only user behaviour (e.g. interactions) yields better results that using user behaviour plus user and item features. Of course, this is something that needs to be carefully explored when building your algorithm. \n",
    "\n",
    "I hope at this stage we are clear on why FMs are powerful when building prediction algorithms. Let's use them for the example here with the Ponpare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import xlearn as xl\n",
    "import pickle\n",
    "\n",
    "from time import time\n",
    "from sklearn.datasets import dump_svmlight_file, load_svmlight_file\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from recutils.average_precision import mapk\n",
    "\n",
    "inp_dir = \"../datasets/Ponpare/data_processed/\"\n",
    "train_dir = \"train\"\n",
    "valid_dir = \"valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COUPONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['price_rate',\n",
       " 'catalog_price',\n",
       " 'discount_price',\n",
       " 'dispperiod',\n",
       " 'validperiod',\n",
       " 'validperiod_method2_cat',\n",
       " 'validfrom_method2_cat',\n",
       " 'validend_method2_cat']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train coupon features (with coupons we will focus on categorical features only)\n",
    "df_coupons_train_feat = pd.read_pickle(os.path.join(inp_dir, train_dir, 'df_coupons_train_feat.p'))\n",
    "drop_cols = [c for c in df_coupons_train_feat.columns\n",
    "    if (('_cat' not in c) or ('method2' in c)) and (c!='coupon_id_hash')]\n",
    "\n",
    "df_coupons_train_cat_feat = df_coupons_train_feat.drop(drop_cols, axis=1)\n",
    "coupons_cols_to_oh = [c for c in df_coupons_train_cat_feat.columns if (c!='coupon_id_hash')]\n",
    "\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use FMs (and linear models) with xlearn. Here there is no \"automatic\" \n",
    "# treatment of categorical features. Therefore, we need to one-hot encode them. \n",
    "# To one hot encode we need to do it all at once, validation and training coupons\n",
    "\n",
    "# Read the validation coupon features\n",
    "df_coupons_valid_feat = pd.read_pickle(os.path.join(inp_dir, 'valid', 'df_coupons_valid_feat.p'))\n",
    "df_coupons_valid_cat_feat = df_coupons_valid_feat.drop(drop_cols, axis=1)\n",
    "\n",
    "df_coupons_train_cat_feat['is_valid'] = 0\n",
    "df_coupons_valid_cat_feat['is_valid'] = 1\n",
    "\n",
    "df_all_coupons = (df_coupons_train_cat_feat\n",
    "    .append(df_coupons_valid_cat_feat, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18622, 233)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_coupons_oh_feat = pd.get_dummies(df_all_coupons, columns=coupons_cols_to_oh)\n",
    "df_coupons_train_oh_feat = (df_all_coupons_oh_feat[df_all_coupons_oh_feat.is_valid==0]\n",
    "    .drop('is_valid', axis=1))\n",
    "df_coupons_valid_oh_feat = (df_all_coupons_oh_feat[df_all_coupons_oh_feat.is_valid==1]\n",
    "    .drop('is_valid', axis=1))\n",
    "df_coupons_train_oh_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train user-features: there are a lot of features for users, both, numerical\n",
    "# and categorical. We keep them all\n",
    "df_users_train_feat = pd.read_pickle(os.path.join(inp_dir, train_dir, 'df_user_train_feat.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22624, 456)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing the numerical columns\n",
    "user_categorical_cols = [c for c in df_users_train_feat.columns if c.endswith('_cat')]\n",
    "user_numerical_cols = [c for c in df_users_train_feat.columns\n",
    "    if ((c not in user_categorical_cols) and (c!='user_id_hash'))]\n",
    "user_numerical_df = df_users_train_feat[user_numerical_cols]\n",
    "\n",
    "# I know I could use MinMaxScaler(), but it returns a np array. I would have to transform the\n",
    "# object into a pandas df and add column names. Is really easier, but the line below is easier\n",
    "user_numerical_df_norm = (user_numerical_df-user_numerical_df.min())/(user_numerical_df.max()-user_numerical_df.min())\n",
    "df_users_train_feat.drop(user_numerical_cols, axis=1, inplace=True)\n",
    "df_users_train_feat = pd.concat([user_numerical_df_norm, df_users_train_feat], axis=1)\n",
    "df_users_train_oh_feat = pd.get_dummies(df_users_train_feat, columns=user_categorical_cols)\n",
    "df_users_train_oh_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTEREST DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1560464, 687)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load interest dataframe\n",
    "df_interest = pd.read_pickle(os.path.join(inp_dir, train_dir, 'df_interest.p'))\n",
    "df_train = pd.merge(df_interest, df_users_train_oh_feat, on='user_id_hash')\n",
    "df_train = pd.merge(df_train, df_coupons_train_oh_feat, on = 'coupon_id_hash')\n",
    "\n",
    "# drop unneccesary columns\n",
    "df_train.drop(['user_id_hash','coupon_id_hash','recency_factor'], axis=1, inplace=True)\n",
    "y_train = df_train.interest.values\n",
    "df_train.drop('interest', axis=1, inplace=True)\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 The joys of xlearn \n",
    "\n",
    "(If you want to jump to the final solution simply go to the next section 12.3 below)\n",
    "\n",
    "As I mentioned at the beginning of this notebook, I will be using `xlearn`. While this package is really promissing, is still \"rough on the edges\", and I will illustrate why in the following lines. Nonetheless is always fun to check new packages as they are created and I do hope they bring it up to production standards. \n",
    "\n",
    "I normally prefer to use native methods of the packages I use. However, this time, after reading the documentation I decided to start using the sklearn-like wrap-up. However, this is what happened. Let's start with a small sample so tests happen quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample of 10000/1000 train/test instances\n",
    "rnd_indx = random.sample(range(df_train.shape[0]), 11000)\n",
    "rnd_indx_tr = rnd_indx[:10000]\n",
    "rnd_indx_te = rnd_indx[10000:]\n",
    "\n",
    "# temporal matrices\n",
    "tmp_X_train = df_train.iloc[rnd_indx_tr,:].values\n",
    "tmp_y_train = y_train[rnd_indx_tr]\n",
    "tmp_X_test = df_train.iloc[rnd_indx_te,:].values\n",
    "tmp_y_test = y_train[rnd_indx_te]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try the linear method in xlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following the tutorial on their site:\n",
    "lr_model = xl.LRModel(task='reg', epoch=10, lr=0.1)\n",
    "lr_model.fit(tmp_X_train, tmp_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs the following on the terminal (not here (?) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "----------------------------------------------------------------------------------------------\n",
    "           _\n",
    "          | |\n",
    "     __  _| |     ___  __ _ _ __ _ __\n",
    "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\\n",
    "      >  <| |___|  __/ (_| | |  | | | |\n",
    "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
    "\n",
    "        xLearn   -- 0.31 Version --\n",
    "----------------------------------------------------------------------------------------------\n",
    "\n",
    "[ WARNING    ] Validation file not found, xLearn has already disable early-stopping.\n",
    "[ WARNING    ] Validation file not found, xLearn has already disable (-x auc) option.\n",
    "[ ACTION     ] Read Problem ...\n",
    "[------------] First check if the text file has been already converted to binary format.\n",
    "[------------] Binary file (/tmp/tmpcqz8a9rk.bin) NOT found. Convert text file to binary file.\n",
    "[------------] Number of Feature: 687\n",
    "[------------] Time cost for reading problem: 0.16 (sec)\n",
    "[ ACTION     ] Initialize model ...\n",
    "[------------] Model size: 2.69 KB\n",
    "[------------] Time cost for model initial: 0.00 (sec)\n",
    "[ ACTION     ] Start to train ...\n",
    "[------------] Epoch      Train mse_loss     Time cost (sec)\n",
    "[   10%      ]     1                -nan                0.00\n",
    "[   20%      ]     2                -nan                0.00\n",
    "[   30%      ]     3                -nan                0.00\n",
    "[   40%      ]     4                -nan                0.00\n",
    "[   50%      ]     5                -nan                0.00\n",
    "[   60%      ]     6                -nan                0.01\n",
    "[   70%      ]     7                -nan                0.01\n",
    "[   80%      ]     8                -nan                0.01\n",
    "[   90%      ]     9                -nan                0.01\n",
    "[  100%      ]    10                -nan                0.01\n",
    "[ ACTION     ] Start to save model ...\n",
    "[------------] Model file: /tmp/tmpjkhnbd2f\n",
    "[------------] Time cost for saving model: 0.00 (sec)\n",
    "[ ACTION     ] Start to save txt model ...\n",
    "[------------] TXT Model file: /tmp/tmpkcmsbtwl\n",
    "[------------] Time cost for saving txt model: 0.00 (sec)\n",
    "[ ACTION     ] Finish training\n",
    "[ ACTION     ] Clear the xLearn environment ...\n",
    "[------------] Total time cost: 0.22 (sec)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, all `NaNs`. I have tried a number of set ups, parameters and data sizes (yes, desperate attemps) and nothing changed data. So I decided to move to native methods. Here the input has to be passed as files read from disk in . svmlight format. Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.24 s, sys: 12 ms, total: 2.25 s\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "# dump to svmlight\n",
    "%time dump_svmlight_file(tmp_X_train, tmp_y_train, \"trainfm.txt\")\n",
    "\n",
    "lr_model = xl.create_linear()\n",
    "lr_model.setTrain(\"trainfm.txt\")\n",
    "param = {'task':'reg', 'lr':0.1, 'epoch': 10}\n",
    "lr_model.fit(param, \"model.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, output now looks better\n",
    "\n",
    "```\n",
    "----------------------------------------------------------------------------------------------\n",
    "           _\n",
    "          | |\n",
    "     __  _| |     ___  __ _ _ __ _ __\n",
    "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\\n",
    "      >  <| |___|  __/ (_| | |  | | | |\n",
    "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
    "\n",
    "        xLearn   -- 0.31 Version --\n",
    "----------------------------------------------------------------------------------------------\n",
    "\n",
    "[ WARNING    ] Validation file not found, xLearn has already disable early-stopping.\n",
    "[ ACTION     ] Read Problem ...\n",
    "[------------] First check if the text file has been already converted to binary format.\n",
    "[------------] Binary file (trainfm.txt.bin) NOT found. Convert text file to binary file.\n",
    "[------------] Number of Feature: 687\n",
    "[------------] Time cost for reading problem: 0.15 (sec)\n",
    "[ ACTION     ] Initialize model ...\n",
    "[------------] Model size: 5.38 KB\n",
    "[------------] Time cost for model initial: 0.00 (sec)\n",
    "[ ACTION     ] Start to train ...\n",
    "[------------] Epoch      Train mse_loss     Time cost (sec)\n",
    "[   10%      ]     1            0.058563                0.01\n",
    "[   20%      ]     2            0.039636                0.01\n",
    "[   30%      ]     3            0.038048                0.01\n",
    "[   40%      ]     4            0.036483                0.01\n",
    "[   50%      ]     5            0.035845                0.01\n",
    "[   60%      ]     6            0.035200                0.01\n",
    "[   70%      ]     7            0.035077                0.01\n",
    "[   80%      ]     8            0.034705                0.01\n",
    "[   90%      ]     9            0.034303                0.01\n",
    "[  100%      ]    10            0.033958                0.01\n",
    "[ ACTION     ] Start to save model ...\n",
    "[------------] Model file: model.out\n",
    "[------------] Time cost for saving model: 0.00 (sec)\n",
    "[ ACTION     ] Finish training\n",
    "[ ACTION     ] Clear the xLearn environment ...\n",
    "[------------] Total time cost: 0.23 (sec)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I thought I will use these methods and optimize + cross validate, given that the library comes with a convenient `.cv` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = xl.create_linear()\n",
    "lr_model.setTrain(\"trainfm.txt\")\n",
    "param = {'task':'reg', 'lr':0.1, 'epoch': 10}\n",
    "lr_model.cv(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[ ACTION     ] Cross-validation: 1/5:\n",
    "[------------] Epoch      Train mse_loss       Test mse_loss     Time cost (sec)\n",
    "[   10%      ]     1            0.059444            0.044607                0.01\n",
    "[   20%      ]     2            0.040383            0.040577                0.00\n",
    "[   30%      ]     3            0.038289            0.040104                0.00\n",
    "[   40%      ]     4            0.036922            0.039543                0.00\n",
    "[   50%      ]     5            0.036106            0.037690                0.00\n",
    "[   60%      ]     6            0.035162            0.036985                0.00\n",
    "[   70%      ]     7            0.035021            0.036967                0.00\n",
    "[   80%      ]     8            0.034697            0.036919                0.00\n",
    "[   90%      ]     9            0.034426            0.037782                0.00\n",
    "[  100%      ]    10            0.034217            0.037063                0.01\n",
    "[ ACTION     ] Cross-validation: 2/5:\n",
    "[------------] Epoch      Train mse_loss       Test mse_loss     Time cost (sec)\n",
    "[   10%      ]     1            0.090296            0.039171                0.00\n",
    "[   20%      ]     2            0.039904            0.038145                0.00\n",
    "[   30%      ]     3            0.037674            0.037457                0.00\n",
    "[   40%      ]     4            0.036615            0.036940                0.00\n",
    "[   50%      ]     5            0.036017            0.036663                0.00\n",
    "[   60%      ]     6            0.035474            0.036272                0.00\n",
    "[   70%      ]     7            0.034949            0.036434                0.01\n",
    "[   80%      ]     8            0.034661            0.035400                0.00\n",
    "[   90%      ]     9            0.034408            0.035600                0.00\n",
    "[  100%      ]    10            0.034342            0.036243                0.00\n",
    "[ ACTION     ] Cross-validation: 3/5:\n",
    "[------------] Epoch      Train mse_loss       Test mse_loss     Time cost (sec)\n",
    "[   10%      ]     1            0.085794            0.043569                0.00\n",
    "[   20%      ]     2            0.039700            0.040191                0.00\n",
    "[   30%      ]     3            0.037755            0.037411                0.00\n",
    "[   40%      ]     4            0.036678            0.037501                0.00\n",
    "[   50%      ]     5            0.036058            0.038654                0.00\n",
    "[   60%      ]     6            0.035338            0.036062                0.00\n",
    "[   70%      ]     7            0.034920            0.035899                0.00\n",
    "[   80%      ]     8            0.034780            0.038572                0.00\n",
    "[   90%      ]     9            0.034402            0.035061                0.01\n",
    "[  100%      ]    10            0.034162            0.036144                0.00\n",
    "[ ACTION     ] Cross-validation: 4/5:\n",
    "[------------] Epoch      Train mse_loss       Test mse_loss     Time cost (sec)\n",
    "[   10%      ]     1            0.079564            0.041987                0.00\n",
    "[   20%      ]     2            0.040053            0.036263                0.00\n",
    "[   30%      ]     3            0.038571            0.035752                0.00\n",
    "[   40%      ]     4            0.037176            0.036989                0.00\n",
    "[   50%      ]     5            0.036446            0.034759                0.00\n",
    "[   60%      ]     6            0.036075            0.034664                0.00\n",
    "[   70%      ]     7            0.035224            0.034854                0.00\n",
    "[   80%      ]     8            0.035005            0.037606                0.00\n",
    "[   90%      ]     9            0.034773            0.036742                0.00\n",
    "[  100%      ]    10            0.034599            0.034972                0.00\n",
    "[ ACTION     ] Cross-validation: 5/5:\n",
    "[------------] Epoch      Train mse_loss       Test mse_loss     Time cost (sec)\n",
    "[   10%      ]     1            0.072777            0.041085                0.00\n",
    "[   20%      ]     2            0.039750            0.039631                0.01\n",
    "[   30%      ]     3            0.038511            0.040434                0.00\n",
    "[   40%      ]     4            0.037066            0.039133                0.00\n",
    "[   50%      ]     5            0.036170            0.036739                0.00\n",
    "[   60%      ]     6            0.035572            0.036710                0.00\n",
    "[   70%      ]     7            0.035060            0.035377                0.00\n",
    "[   80%      ]     8            0.034890            0.037823                0.00\n",
    "[   90%      ]     9            0.034619            0.035106                0.00\n",
    "[  100%      ]    10            0.034623            0.035278                0.00\n",
    "[------------] Average mse_loss: 0.035940\n",
    "[ ACTION     ] Finish Cross-Validation\n",
    "[ ACTION     ] Clear the xLearn environment ...\n",
    "[------------] Total time cost: 0.47 (sec)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok...that worked...however, I have no way of accessing the score per fold (`xlearn` default is 5-folds). \n",
    "\n",
    "All methods an attributes of our linear model are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_set_Param',\n",
       " 'cv',\n",
       " 'disableEarlyStop',\n",
       " 'disableLockFree',\n",
       " 'disableNorm',\n",
       " 'fit',\n",
       " 'predict',\n",
       " 'setOnDisk',\n",
       " 'setQuiet',\n",
       " 'setSigmoid',\n",
       " 'setSign',\n",
       " 'setTXTModel',\n",
       " 'setTest',\n",
       " 'setTrain',\n",
       " 'setValidate',\n",
       " 'setValidateDMatrix',\n",
       " 'show']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_methods = [method for method in dir(lr_model) if callable(getattr(lr_model, method))]\n",
    "model_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no (or I can't see) a `score` or `result` attribute, or any method that, given some argument, will retrieve the information I want. At this point I decided to code my own cross validation and move on. This is how I went about it (spoiler alert, was not the final solution):\n",
    "\n",
    "1. I will take X% of the training data and split it in 3 folds. This is because with all data it takes ages\n",
    "2. Save them to disk in respective files\n",
    "3. perform cv manually within an hyperopt function\n",
    "\n",
    "before we move on let me clean some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(tmp_X_test, tmp_X_train, tmp_y_test, tmp_y_train, lr_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: iteration 1 of 3\n",
      "INFO: saving svmlight training file to ../datasets/Ponpare/data_processed/xlearn_data/train_part_0.txt\n",
      "INFO: saving svmlight validatio file to ../datasets/Ponpare/data_processed/xlearn_data/valid_part_0.txt\n",
      "INFO: saving y_valid to ../datasets/Ponpare/data_processed/xlearn_data/target_part_0.txt\n",
      "INFO: iteration 2 of 3\n",
      "INFO: saving svmlight training file to ../datasets/Ponpare/data_processed/xlearn_data/train_part_1.txt\n",
      "INFO: saving svmlight validatio file to ../datasets/Ponpare/data_processed/xlearn_data/valid_part_1.txt\n",
      "INFO: saving y_valid to ../datasets/Ponpare/data_processed/xlearn_data/target_part_1.txt\n",
      "INFO: iteration 3 of 3\n",
      "INFO: saving svmlight training file to ../datasets/Ponpare/data_processed/xlearn_data/train_part_2.txt\n",
      "INFO: saving svmlight validatio file to ../datasets/Ponpare/data_processed/xlearn_data/valid_part_2.txt\n",
      "INFO: saving y_valid to ../datasets/Ponpare/data_processed/xlearn_data/target_part_2.txt\n"
     ]
    }
   ],
   "source": [
    "XLEARN_DIR = inp_dir+\"xlearn_data\"\n",
    "\n",
    "rnd_indx_cv = random.sample(range(df_train.shape[0]), round(df_train.shape[0]*0.25))\n",
    "X_train_cv = csr_matrix(df_train.iloc[rnd_indx_cv,:].values)\n",
    "y_train_cv =  y_train[rnd_indx_cv]\n",
    "seed = 37\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "train_fpaths, valid_fpaths, valid_target_fpaths = [],[],[]\n",
    "\n",
    "# Here we go...\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X_train_cv)):\n",
    "\n",
    "    print(\"INFO: iteration {} of {}\".format(i+1,kf.n_splits))\n",
    "\n",
    "    x_tr, y_tr = X_train_cv[train_index], y_train_cv[train_index]\n",
    "    x_va, y_va = X_train_cv[valid_index], y_train_cv[valid_index]\n",
    "\n",
    "    train_fpath = os.path.join(XLEARN_DIR,'train_part_'+str(i)+\".txt\")\n",
    "    valid_fpath = os.path.join(XLEARN_DIR,'valid_part_'+str(i)+\".txt\")\n",
    "    valid_target_fpath = os.path.join(XLEARN_DIR,'target_part_'+str(i)+\".txt\")\n",
    "\n",
    "    print(\"INFO: saving svmlight training file to {}\".format(train_fpath))\n",
    "    dump_svmlight_file(x_tr, y_tr, train_fpath)\n",
    "\n",
    "    print(\"INFO: saving svmlight validatio file to {}\".format(valid_fpath))\n",
    "    dump_svmlight_file(x_va, y_va, valid_fpath)\n",
    "\n",
    "    print(\"INFO: saving y_valid to {}\".format(valid_target_fpath))\n",
    "    np.savetxt(valid_target_fpath, y_va)\n",
    "\n",
    "    train_fpaths.append(train_fpath)\n",
    "    valid_fpaths.append(valid_fpath)\n",
    "    valid_target_fpaths.append(valid_target_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took some time. Ideally one would like to wrap-up the content of that loop into a function, and use joblib's Parallel to paralelise the process. At least, you would be using 3 cores instead of one. I will leave that to you, the reader.\n",
    "\n",
    "The required files are now created. Let's define our parameter space and hyperopt objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_parameter_space = {\n",
    "    'lr': hp.uniform('lr', 0.01, 0.5),           \n",
    "    'lambda': hp.uniform('lambda', 0.001,0.01),  # regularization\n",
    "    'init': hp.uniform('init', 0.2,0.8),         # model (w) initialization\n",
    "    'epoch': hp.quniform('epoch', 10, 200, 10),\n",
    "    'k': hp.quniform('k', 2, 10, 1),             # latent factors\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xl_objective(params, method=\"fm\"):\n",
    "\n",
    "    xl_objective.i+=1\n",
    "\n",
    "    params['task'] = 'reg'\n",
    "    params['metric'] = 'rmse'\n",
    "\n",
    "    # remember hyperopt casts as floats\n",
    "    params['epoch'] = int(params['epoch'])\n",
    "    params['k'] = int(params['k'])\n",
    "\n",
    "    if method is \"linear\":\n",
    "        xl_model = xl.create_linear()\n",
    "    elif method is \"fm\":\n",
    "        xl_model = xl.create_fm()\n",
    "\n",
    "    results = []\n",
    "    for train, valid, target in zip(train_fpaths, valid_fpaths, valid_target_fpaths):\n",
    "\n",
    "        preds_fname = os.path.join(XLEARN_DIR, 'tmp_output.txt')\n",
    "        model_fname = os.path.join(XLEARN_DIR, \"tmp_model.out\")\n",
    "\n",
    "        xl_model.setTrain(train)\n",
    "        xl_model.setTest(valid)\n",
    "        # whether quiet of not, it'll output a lot of stuff...\n",
    "        xl_model.setQuiet()\n",
    "        xl_model.fit(params, model_fname)\n",
    "        xl_model.predict(model_fname, preds_fname)\n",
    "\n",
    "        y_valid = np.loadtxt(target)\n",
    "        predictions = np.loadtxt(preds_fname)\n",
    "        loss = np.sqrt(mean_squared_error(y_valid, predictions))\n",
    "\n",
    "        results.append(loss)\n",
    "\n",
    "    error = np.mean(results)\n",
    "    print(\"INFO: iteration {} error {:.3f}\".format(xl_objective.i, error))\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn the objective function into a partial function of params and run 3 iterations to see why I end up optimizing without `cv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: iteration 1 error 0.263\n",
      "INFO: iteration 2 error 0.264\n",
      "INFO: iteration 3 error 0.264\n",
      "507.526216506958\n"
     ]
    }
   ],
   "source": [
    "partial_objective = lambda params: xl_objective(\n",
    "    params,\n",
    "    method=\"fm\")\n",
    "\n",
    "start = time()\n",
    "xl_objective.i = 0\n",
    "best_fm = fmin(\n",
    "    fn=partial_objective,\n",
    "    space=xl_parameter_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=3\n",
    "    )\n",
    "end = time()-start\n",
    "print(\"{} min\".format(round(end/60,3)))\n",
    "\n",
    "pickle.dump(best_fm, open(os.path.join(XLEARN_DIR,'best_fm.p'), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 iterations on a c5.4xlarge instance (30GB or RAM and 16 cores) took over 8 min, which is a lot for this excercise moreover bearing in mind I am only using 25% of the data. Therefore I decided to **NOT use cross validation** and optimize with a single train/test split. Not ideal, but better than manually trying parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 Final Solution\n",
    "\n",
    "Let's define a series of paths that we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "XLEARN_DIR = inp_dir+\"xlearn_data\"\n",
    "\n",
    "# train and validation paths\n",
    "train_fpath = os.path.join(XLEARN_DIR,\"train_xl.txt\")\n",
    "train_target_path = os.path.join(XLEARN_DIR,\"train_target_xl.txt\")\n",
    "valid_fpath = os.path.join(XLEARN_DIR,\"valid_xl.txt\")\n",
    "\n",
    "# temporal filenames for the optimization process\n",
    "xlmodel_fname_tmp = os.path.join(XLEARN_DIR,\"xlfm_model_tmp.out\")\n",
    "xlpreds_fname_tmp = os.path.join(XLEARN_DIR,\"xlfm_preds_tmp.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the training dataset into train and evaluation (I run out of names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780232, 687)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For memory issues, I will still not use the whole dataset here, but 50%.\n",
    "rnd_indx = random.sample(range(df_train.shape[0]), round(df_train.shape[0]*0.50))\n",
    "X_train_rn = csr_matrix(df_train.iloc[rnd_indx,:].values)\n",
    "y_train_rn =  y_train[rnd_indx]\n",
    "X_train_rn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save them to a svmlight format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: saving svmlight training file to ../datasets/Ponpare/data_processed/xlearn_data/train_xl.txt\n",
      "INFO: saving target to ../datasets/Ponpare/data_processed/xlearn_data/train_target_xl.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"INFO: saving svmlight training file to {}\".format(train_fpath))\n",
    "dump_svmlight_file(X_train_rn, y_train_rn, train_fpath)\n",
    "\n",
    "print(\"INFO: saving target to {}\".format(train_target_path))\n",
    "np.savetxt(train_target_path, y_train_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(X_train_rn, y_train_rn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/jrz/lib/python3.6/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2173418, 689)\n",
      "(2173418, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read the interactions during validation\n",
    "interactions_valid_dict = pickle.load(\n",
    "    open(\"../datasets/Ponpare/data_processed/valid/interactions_valid_dict.p\", \"rb\"))\n",
    "\n",
    "left = pd.DataFrame({'user_id_hash':list(interactions_valid_dict.keys())})\n",
    "left['key'] = 0\n",
    "right = df_coupons_valid_feat[['coupon_id_hash']]\n",
    "right['key'] = 0\n",
    "df_valid = (pd.merge(left, right, on='key', how='outer')\n",
    "    .drop('key', axis=1))\n",
    "df_valid = pd.merge(df_valid, df_users_train_oh_feat, on='user_id_hash')\n",
    "df_valid = pd.merge(df_valid, df_coupons_valid_oh_feat, on = 'coupon_id_hash')\n",
    "df_preds = df_valid[['user_id_hash','coupon_id_hash']]\n",
    "print(df_valid.shape)\n",
    "print(df_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 3s, sys: 2.86 s, total: 8min 6s\n",
      "Wall time: 8min 5s\n"
     ]
    }
   ],
   "source": [
    "X_valid = csr_matrix(df_valid\n",
    "    .drop(['user_id_hash','coupon_id_hash'], axis=1)\n",
    "    .values)\n",
    "# svmlight needs a target column\n",
    "y_valid = np.array([0.1]*X_valid.shape[0])\n",
    "%time dump_svmlight_file(X_valid,y_valid,valid_fpath)\n",
    "del(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xl_objective(params, method=\"fm\"):\n",
    "\n",
    "    start = time()\n",
    "    xl_objective.i+=1\n",
    "\n",
    "    params['task'] = 'reg'\n",
    "    params['metric'] = 'rmse'\n",
    "\n",
    "    # remember hyperopt casts as floats\n",
    "    params['epoch'] = int(params['epoch'])\n",
    "    params['k'] = int(params['k'])\n",
    "\n",
    "    # I added an option in case you want to use linear\n",
    "    if method is \"linear\":\n",
    "        xl_model = xl.create_linear()\n",
    "    elif method is \"fm\":\n",
    "        xl_model = xl.create_fm()\n",
    "\n",
    "    # if you just want fm or linear the firs 3 lines here can go outside the function    \n",
    "    xl_model.setTrain(train_fpath)\n",
    "    xl_model.setTest(valid_fpath)\n",
    "    xl_model.disableNorm()\n",
    "    #xl_model.setQuiet()    \n",
    "    xl_model.fit(params, xlmodel_fname_tmp)\n",
    "    xl_model.predict(xlmodel_fname_tmp, xlpreds_fname_tmp)\n",
    "\n",
    "    # add predicitions and rank\n",
    "    preds = np.loadtxt(xlpreds_fname_tmp)\n",
    "    df_preds['interest'] = preds\n",
    "\n",
    "    df_ranked = df_preds.sort_values(['user_id_hash', 'interest'],\n",
    "        ascending=[False, False])\n",
    "    df_ranked = (df_ranked\n",
    "        .groupby('user_id_hash')['coupon_id_hash']\n",
    "        .apply(list)\n",
    "        .reset_index())\n",
    "    recomendations_dict = pd.Series(df_ranked.coupon_id_hash.values,\n",
    "        index=df_ranked.user_id_hash).to_dict()\n",
    "\n",
    "    actual = []\n",
    "    pred = []\n",
    "    for k,_ in recomendations_dict.items():\n",
    "        actual.append(list(interactions_valid_dict[k]))\n",
    "        pred.append(list(recomendations_dict[k]))\n",
    "\n",
    "    score = mapk(actual,pred)\n",
    "    end = round((time() - start)/60.,2)\n",
    "\n",
    "    print(\"INFO: iteration {} was completed in {} min. Score {:.3f}\".format(xl_objective.i, end, score))\n",
    "\n",
    "    return 1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_parameter_space = {\n",
    "    'lr': hp.uniform('lr', 0.01, 0.4),                # learning rate default ?          \n",
    "    'lambda': hp.uniform('lambda', 0.,0.02),          # regularization default 0.00002\n",
    "    'init': hp.uniform('init', 0.4,0.8),              # model (w) initialization default 0.66\n",
    "    'epoch': hp.quniform('epoch', 10, 50, 5),         # epoch default 10\n",
    "    'k': hp.quniform('k', 4, 20, 2),                  # latent factors default 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run the same experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/jrz/lib/python3.6/site-packages/ipykernel/__main__.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: iteration 1 was completed in 1.47 min. Score 0.013\n",
      "INFO: iteration 2 was completed in 0.76 min. Score 0.013\n",
      "INFO: iteration 3 was completed in 1.97 min. Score 0.018\n",
      "INFO: iteration 4 was completed in 1.01 min. Score 0.013\n",
      "INFO: iteration 5 was completed in 0.85 min. Score 0.009\n",
      "INFO: iteration 6 was completed in 0.66 min. Score 0.013\n",
      "INFO: iteration 7 was completed in 2.11 min. Score 0.011\n",
      "INFO: iteration 8 was completed in 1.81 min. Score 0.011\n"
     ]
    }
   ],
   "source": [
    "partial_objective = lambda params: xl_objective(\n",
    "    params,\n",
    "    method=\"fm\")\n",
    "\n",
    "start = time()\n",
    "xl_objective.i = 0\n",
    "best_fm = fmin(\n",
    "    fn=partial_objective,\n",
    "    space=xl_parameter_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=10\n",
    "    )\n",
    "end = time()-start\n",
    "print(\"{} min\".format(round(end/60,3)))\n",
    "\n",
    "pickle.dump(best_fm, open(os.path.join(XLEARN_DIR,'best_fm.p'), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`Memory error!`**  \n",
    "\n",
    "You will see the `Killed` message in the terminal\n",
    "\n",
    "The datasets are relatively small and I am on a c5.4xlarge instance. Therefore, I am not sure whether there is a memory leak in the package or some other memory related issue because every iteration accumulates a lot of memory (up to 3GB). \n",
    "\n",
    "Nonetheless, even after completing only 8 iterations, I would say that these are dissapointing results. Of course, there are a number of ways to improve the methods here, starting with the obvious: using the whole dataset instead of 50% (but optimizing will take longer and memory blows quite quickly). Also, one could increase the number of factors, but the impact is less notable.\n",
    "\n",
    "For example, when using the whole training dataset and these parameters:\n",
    "\n",
    "```\n",
    "param_fm = {'epoch': 20,\n",
    " 'init': 0.4,\n",
    " 'k': 10,\n",
    " 'lambda': 0.2,\n",
    " 'lr': 0.01,\n",
    " 'task': 'reg',\n",
    " 'metric': 'rmse'}\n",
    "```\n",
    "\n",
    "you get MAP@10=0.02. It looks like is \"all about the data\". The more the better, obviously. Anyway, we will take all the lessons learned in this chapter, and move into the next, more powerful technique: Field-Aware Factorization Machines  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_jrz)",
   "language": "python",
   "name": "conda_jrz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
