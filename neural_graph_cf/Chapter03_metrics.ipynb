{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Metrics\n",
    "\n",
    "Before we move onto the model, and how is trained and tested, let's quickly go through the metrics that we will use here. The first part of the code below is either a direct copy/paste from the original [repo](https://github.com/xiangwang1223/neural_graph_collaborative_filtering) or a minor adaptation. When this is not the case I will explain the corresponding details. Therefore, **all credit to the authors** (Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng and Tat-Seng Chua)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.choice(2, 20, p=[0.7, 0.3])\n",
    "k = 10\n",
    "n_inter = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(r, k, n_inter):\n",
    "    \"\"\"recall @ k\n",
    "    Parameters:\n",
    "    ----------\n",
    "    r: Iterable\n",
    "        binary iterable (nonzero is relevant).\n",
    "    k: Int\n",
    "        number of recommendations to consider\n",
    "    n_inter: Int\n",
    "        number of interactions\n",
    "    Returns:\n",
    "    ----------\n",
    "    recall @ k\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r) / n_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_at_k(r, k, n_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(r, k):\n",
    "    \"\"\"precision @ k\n",
    "    Parameters:\n",
    "    ----------\n",
    "    r: Iterable\n",
    "        binary iterable (nonzero is relevant).\n",
    "    k: Int\n",
    "        number of recommendations to consider\n",
    "    Returns:\n",
    "    ----------\n",
    "    Precision @ k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_at_k(r, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k, method=1):\n",
    "    \"\"\" discounted cumulative gain (dcg) @ k\n",
    "    Parameters:\n",
    "    ----------\n",
    "    r: Iterable\n",
    "        Relevance is positive real values. If binary, nonzero is relevant.\n",
    "    k: Int\n",
    "        number of recommendations to consider\n",
    "    method: Int\n",
    "        one of 0 or 1. Simply, different dcg implementations\n",
    "    Returns:\n",
    "    ----------\n",
    "    dcg @ k\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1737365524159569"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcg_at_k(r, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3175293653079347"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcg_at_k(r, k, method=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(r, k, method=1):\n",
    "    \"\"\" Normalized discounted cumulative gain @ k\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35517551357284516"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_at_k(r, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_at_k(r, k):\n",
    "    \"\"\"hit ratio @ k\n",
    "    Parameters:\n",
    "    ----------\n",
    "    r: Iterable\n",
    "        binary iterable (nonzero is relevant).\n",
    "    k: Int\n",
    "        number of recommendations to consider\n",
    "    Returns:\n",
    "    ----------\n",
    "    hit ratio @ k\n",
    "    \"\"\"\n",
    "    r = np.array(r)[:k]\n",
    "    if np.sum(r) > 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_at_k(r,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(item_score, user_pos_test):\n",
    "    \"\"\"Wrap up around sklearn's roc_auc_score\n",
    "    Parameters:\n",
    "    ----------\n",
    "    item_score: Dict\n",
    "        Dict. keys are item_ids, values are predictions\n",
    "    user_pos_test: List\n",
    "        List with the items that the user actually interacted with\n",
    "    Returns:\n",
    "    ----------\n",
    "    res: Float\n",
    "        roc_auc_score\n",
    "    \"\"\"\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_id = [x[0] for x in item_score]\n",
    "    score = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_id:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "\n",
    "    try:\n",
    "        res = roc_auc_score(r, score)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the inputs of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example...let's assume 100 items in total\n",
    "item_score = {k:v for k,v in zip(np.arange(100), np.random.rand(100))}\n",
    "user_pos_test = np.random.choice(100, 20, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020429522439006087"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id = 1\n",
    "item_score[item_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90 26 58 86 93 70 91  8 96 50 38 33 66 99 41 20 74 28 22 49]\n"
     ]
    }
   ],
   "source": [
    "print(user_pos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51125"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_auc(item_score, user_pos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(true, pred):\n",
    "    \"\"\"Simple wrap up around sklearn's roc_auc_score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = roc_auc_score(true, pred)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res\n",
    "\n",
    "\n",
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    \"\"\"\n",
    "    Retursn a binary list, where relevance is nonzero, based on a ranked list\n",
    "    with the n largest scores. Also returns the AUC\n",
    "    Parameters:\n",
    "    ----------\n",
    "    user_pos_test: List\n",
    "        List with the items that the user actually interacted with\n",
    "    test_items: List\n",
    "        List with the all items in the test dataset\n",
    "    rating: List\n",
    "        List with the ratings corresponding to test_items\n",
    "    Ks: Int or List\n",
    "        the k in @k\n",
    "    Returns:\n",
    "    ----------\n",
    "    r: binary list where nonzero in relevant\n",
    "    auc: testing roc_auc_score\n",
    "    \"\"\"\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items, rating = list(item_score.keys()), list(item_score.values())\n",
    "Ks = [5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0, 1, 0, 0, 1, 0], 0.51125)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranklist_by_sorted(user_pos_test, test_items, rating, Ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    \"\"\"\n",
    "    Retursn a binary list, where relevance is nonzero, based on a ranked list\n",
    "    with the n largest scores. For consistency with ranklist_by_sorted, also\n",
    "    returns auc=0 (since auc does not make sense within a mini batch)\n",
    "    Parameters:\n",
    "    ----------\n",
    "    user_pos_test: List\n",
    "        List with the items that the user actually interacted with\n",
    "    test_items: List\n",
    "        List with the all items in the test dataset\n",
    "    rating: List\n",
    "        List with the ratings corresponding to test_items\n",
    "    Ks: Int or List\n",
    "        the k in @k\n",
    "    Returns:\n",
    "    ----------\n",
    "    r: binary list where nonzero in relevant\n",
    "    \"\"\"\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0, 1, 0, 0, 1, 0], 0.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranklist_by_heapq(user_pos_test, test_items, rating, Ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, getting altogether:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    \"\"\"wrap up around all other previous functions\n",
    "    ----------\n",
    "    user_pos_test: List\n",
    "        List with the items that the user actually interacted with\n",
    "    r: List\n",
    "        binary list where nonzero in relevant\n",
    "    auc: Float\n",
    "        sklearn's roc_auc_score\n",
    "    Ks: List\n",
    "        the k in @k\n",
    "    Returns:\n",
    "    ----------\n",
    "    dictionary of metrics\n",
    "    \"\"\"\n",
    "\n",
    "    precision, recall, ndcg, hit_ratio = [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(precision_at_k(r, K))\n",
    "        recall.append(recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(ndcg_at_k(r, K))\n",
    "        hit_ratio.append(hit_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': array([0. , 0.1]),\n",
       " 'precision': array([0. , 0.2]),\n",
       " 'ndcg': array([0.        , 0.40298313]),\n",
       " 'hit_ratio': array([0., 1.]),\n",
       " 'auc': 0.51125}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_performance(user_pos_test, r, auc, Ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU test\n",
    "\n",
    "Before we leave the metrics behind, let's pause for one second and have a look to the functions above. They all provide scores/metrics for one user. This means that this will have run in a loop or distributed over the cores of the machine where we run the algorithm. Given the fact that the algorithm will run on a GPU, maybe we could take advantage of it and write some evaluation function that uses tensors and therefore can run on the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "n_users = 100\n",
    "n_items = 200\n",
    "n_embed = 12\n",
    "Ks=[5,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create some small, fake dataset to illustrate the use of this testing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user and item embeddings\n",
    "user_emb = torch.from_numpy(np.random.rand(n_users, n_embed))\n",
    "item_emb = torch.from_numpy(np.random.rand(n_items, n_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ratings Matrix\n",
    "def randbin(r,c,p):\n",
    "    return np.random.choice([0, 1], size=(r,c), p=[p, 1-p])\n",
    "R_tr = randbin(n_users, n_items, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Rating Matrix\n",
    "# removing all items in training\n",
    "temp_mtx = 1 - R_tr\n",
    "# finding the corresponding indexes\n",
    "temp_idx = np.where(temp_mtx)\n",
    "# setting the testing size as, for example, training//5\n",
    "test_fr = np.where(R_tr)[0].size//5\n",
    "# chosing indexes at random\n",
    "R_te_idx = np.random.choice(temp_idx[0].size, test_fr, replace=False)\n",
    "i,j = temp_idx[0][R_te_idx], temp_idx[1][R_te_idx]\n",
    "# setting them to 1\n",
    "R_te = np.zeros((n_users, n_items))\n",
    "R_te[i,j] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the \"real thing\" `R_tr` and `R_te` will be sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_tr = sp.csr_matrix(R_tr, dtype='float64')\n",
    "R_te = sp.csr_matrix(R_te, dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They will be large, so we need to split them in folds and we will then run a loop over the `n_folds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mtx(X, n_folds=10):\n",
    "    \"\"\"\n",
    "    Split a matrix/Tensor into n_folds    \n",
    "    \"\"\"\n",
    "    X_folds = []\n",
    "    fold_len = X.shape[0]//n_folds\n",
    "    for i in range(n_folds):\n",
    "        start = i * fold_len\n",
    "        if i == n_folds -1:\n",
    "            end = X.shape[0]\n",
    "        else:\n",
    "            end = (i + 1) * fold_len\n",
    "        X_folds.append(X[start:end])\n",
    "    return X_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need another helper to make the code more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k_gpu(pred_items, test_items, test_indices, k):\n",
    "    \"\"\"\n",
    "    pred_items: Tensor dim(fold_size, n_items)\n",
    "        binary tensor with 1s in those locations corresponding to the predicted item interactions\n",
    "    test_items: Tensor dim(fold_size, n_items)\n",
    "        binary tensor with 1s in locations corresponding to the real test interactions\n",
    "    test_indices: Tensor dim(fold_size, max(Ks))\n",
    "        tensor with the location of the topk predicted items \n",
    "    k: int\n",
    "    \"\"\"    \n",
    "    r = (test_items * pred_items).gather(1, test_indices)\n",
    "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().cuda()\n",
    "    dcg = (r[:, :k]/f).sum(1)\n",
    "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)\n",
    "    ndcg = dcg/dcg_max\n",
    "    ndcg[torch.isnan(ndcg)] = 0\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go in detail on what's going on inside that function just below, so keep reading...\n",
    "\n",
    "And this is the testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_GPU(u_emb, i_emb, Rtr, Rte, Ks):\n",
    "\n",
    "    ue_folds = split_mtx(u_emb)\n",
    "    tr_folds = split_mtx(Rtr)\n",
    "    te_folds = split_mtx(Rte)\n",
    "\n",
    "    fold_prec, fold_rec, fold_ndcg, fold_hr = \\\n",
    "        defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    for ue_f, tr_f, te_f in zip(ue_folds, tr_folds, te_folds):\n",
    "\n",
    "        scores = torch.mm(ue_f, i_emb.t())\n",
    "        test_items = torch.from_numpy(te_f.todense()).float().cuda()\n",
    "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().cuda()\n",
    "        scores = scores * non_train_items\n",
    "        _, test_indices = torch.topk(scores, dim=1, k=max(Ks))\n",
    "        pred_items = torch.zeros_like(scores).float()\n",
    "        pred_items.scatter_(dim=1,index=test_indices,src=torch.tensor(1.0).cuda())\n",
    "\n",
    "        for k in Ks:\n",
    "            topk_preds = torch.zeros_like(scores).float()\n",
    "            topk_preds.scatter_(dim=1,index=test_indices[:, :k],src=torch.tensor(1.0))\n",
    "\n",
    "            TP = (test_items * topk_preds).sum(1)\n",
    "            prec = TP/k\n",
    "            rec = TP/test_items.sum(1)\n",
    "            hit_r = (TP > 0).float()\n",
    "            ndcg = ndcg_at_k_gpu(pred_items, test_items, test_indices, k)\n",
    "\n",
    "            fold_prec[k].append(prec)\n",
    "            fold_rec[k].append(rec)\n",
    "            fold_ndcg[k].append(ndcg)\n",
    "            fold_hr[k].append(hit_r)\n",
    "\n",
    "    result = {'precision': [], 'recall': [], 'ndcg': [], 'hit_ratio': []}\n",
    "    for k in Ks:\n",
    "        result['precision'].append(torch.cat(fold_prec[k]).mean())\n",
    "        result['recall'].append(torch.cat(fold_rec[k]).mean())\n",
    "        result['ndcg'].append(torch.cat(fold_ndcg[k]).mean())\n",
    "        result['hit_ratio'].append(torch.cat(fold_hr[k]).mean())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what we are doing there. \n",
    "\n",
    "First, when running the real \"thing\", the rating matrices will be large, so we split them into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_folds = split_mtx(user_emb)\n",
    "tr_folds = split_mtx(R_tr)\n",
    "te_folds = split_mtx(R_te)\n",
    "ue_f, tr_f, te_f = ue_folds[0], tr_folds[0], te_folds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per fold, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores are simply the matrix multiplication between user and item embeddings\n",
    "scores = torch.mm(ue_f, item_emb.t())\n",
    "# test_items is a binary tensor containing REAL interactions\n",
    "test_items = torch.from_numpy(te_f.todense()).float().cuda()\n",
    "# non_train_items is a binary tensor containing ALL interactions that are not in train\n",
    "non_train_items = torch.from_numpy(1-(tr_f.todense())).float().cuda()\n",
    "# We only need to rate non train items\n",
    "scores = scores.float().cuda() * non_train_items\n",
    "# test_indices is a tensor containing the topk indices, per row, in score\n",
    "_, test_indices = torch.topk(scores, dim=1, k=max(Ks))\n",
    "# pred_items is a binary tensor of dim (fold_size, n_items) with 1s in those locations where \n",
    "# we have predicted an interaction\n",
    "pred_items = torch.zeros_like(scores).float()\n",
    "pred_items.scatter_(dim=1,index=test_indices,src=torch.tensor(1.0).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, per each k value we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topk_preds is a binary tensor of dim (fold_size, n_items) with 1s in those locations where\n",
    "# we have predicted the topk interactions\n",
    "topk_preds = torch.zeros_like(scores).float()\n",
    "topk_preds.scatter_(dim=1,index=test_indices[:, :k],src=torch.tensor(1.0))\n",
    "\n",
    "# True positives \n",
    "TP = (test_items * topk_preds).sum(1)\n",
    "# precision as defined by Xiang Wang et al: np.mean(np.asarray(r)[:k])\n",
    "prec = TP/k\n",
    "# recall as defined by Xiang Wang et al: np.sum(np.asfarray(r)[:k]) / all_pos_num\n",
    "rec = TP/test_items.sum(1)\n",
    "# hit ratio = 1 if np.sum(np.array(r)[:k]) > 0 else 0\n",
    "hit_r = (TP > 0).float()\n",
    "# ndcg as defined by Xiang Wang et al\n",
    "ndcg = ndcg_at_k_gpu(pred_items, test_items, test_indices, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just finish by having a look into `ndcg_at_k_gpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r is a binary tensor of dim (fold_size, n_items) with 1s in ALL locations where \n",
    "# we have predicted an interaction\n",
    "r = (test_items * pred_items).gather(1, test_indices)\n",
    "# simply the denominator in their expression: np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().cuda()\n",
    "# the tensor equivalent to  np.sum(r / np.log2(np.arange(2, r.size + 2))) \n",
    "dcg = (r[:, :k]/f).sum(1)\n",
    "# From here on is pretty straightforward\n",
    "dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)\n",
    "ndcg = dcg/dcg_max\n",
    "ndcg[torch.isnan(ndcg)] = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
