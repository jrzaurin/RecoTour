{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data for the model\n",
    "\n",
    "In this notebook my intention is going through the [original](https://github.com/xiangwang1223/neural_graph_collaborative_filtering) implementation keeping only what I consider necessary to eventually understand the model. There are some elements in the authors' original code that are related to the different experiments carried out in the [paper](https://arxiv.org/pdf/1905.08108.pdf). These will not include here. As always, please, go and read the original paper and **all credit to the authors**.\n",
    "\n",
    "As I mentioned in Chapter01, my intention is that the notebooks can run in any machine. The real goal of the notebooks is to understand the process more than executing the real model. With that in mind, I have included a script called `generate_toy_data.py`, which generates a dataset with a small number of users and items that has the exact same format as that of the datasets used by the authors in the [original repo](https://github.com/xiangwang1223/neural_graph_collaborative_filtering).\n",
    "\n",
    "To generate such small dataset with random interactions (just to play with) simply run, for example:\n",
    "    \n",
    "    python generate_toy_data.py --n_users 1000 --n_items 2000 --min_interaction 11 --max_interactions 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "import pdb\n",
    "\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/home/ubuntu/projects/RecoTour/datasets/toy_data/\")\n",
    "batch_size = 32\n",
    "\n",
    "train_file = path/'train.txt'\n",
    "test_file = path/'test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users and items are numbered from 0 to (n_users-1) and (n_items-1), so let's count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of users and items. \n",
    "n_users, n_items = 0, 0\n",
    "n_train, n_test = 0, 0\n",
    "\n",
    "exist_users = []\n",
    "with open(train_file) as f:\n",
    "    for l in f.readlines():\n",
    "        if len(l) > 0:\n",
    "            l = l.strip('\\n').split(' ')\n",
    "            # first element is the user_id, then items\n",
    "            uid = int(l[0])\n",
    "            items = [int(i) for i in l[1:]]\n",
    "            exist_users.append(uid)\n",
    "            n_items = max(n_items, max(items))\n",
    "            n_users = max(n_users, uid)\n",
    "            n_train += len(items)\n",
    "\n",
    "# same as before but for testing\n",
    "with open(test_file) as f:\n",
    "    for l in f.readlines():\n",
    "        if len(l) > 0:\n",
    "            l = l.strip('\\n')\n",
    "            try:\n",
    "                items = [int(i) for i in l.split(' ')[1:]]\n",
    "            except Exception:\n",
    "                continue\n",
    "            n_items = max(n_items, max(items))\n",
    "            n_test += len(items)\n",
    "n_items += 1\n",
    "n_users += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 1000\n"
     ]
    }
   ],
   "source": [
    "print(n_items, n_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All OK. Let's build the interactions/ratings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating matrix for the training dataset\n",
    "Rtr = sp.dok_matrix((n_users, n_items), dtype=np.float32)\n",
    "# rating matrix for the testing dataset \n",
    "Rte = sp.dok_matrix((n_users, n_items), dtype=np.float32)\n",
    "\n",
    "train_set, test_set = {}, {}\n",
    "with open(train_file) as f_train, open(test_file) as f_test:\n",
    "    for l in f_train.readlines():\n",
    "        if len(l) == 0: break\n",
    "        l = l.strip('\\n')\n",
    "        items = [int(i) for i in l.split(' ')]\n",
    "        uid, train_items = items[0], items[1:]\n",
    "        # simply 1 if user interacted with item, otherwise, 0.\n",
    "        for i in train_items:\n",
    "            Rtr[uid, i] = 1.\n",
    "        train_set[uid] = train_items\n",
    "\n",
    "    for l in f_test.readlines():\n",
    "        if len(l) == 0: break\n",
    "        l = l.strip('\\n')\n",
    "        try:\n",
    "            items = [int(i) for i in l.split(' ')]\n",
    "        except Exception:\n",
    "            continue\n",
    "        uid, test_items = items[0], items[1:]\n",
    "        for i in test_items:\n",
    "            Rte[uid, i] = 1.\n",
    "        test_set[uid] = test_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x2000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 24228 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x2000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 6552 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1365, 1073, 664, 292, 1248, 1897, 1370, 1625, 672, 729]\n",
      "[194, 258, 386, 525, 674, 1265, 1347, 1683, 1763, 1930]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0][:10])\n",
    "print(test_set[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They use a number of difference adjacency matrices, see [here](https://github.com/xiangwang1223/neural_graph_collaborative_filtering). \n",
    "\n",
    "Below is their implementation of a function to normalise the adjacency matrix. Here, each decay factor between two connected nodes is set as `(1/out-degree of the node)` (a few cells below there are more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_adj_single(adj):\n",
    "    # rowsum = out-degree of the node    \n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    # inverted and set to 0 if no connections\n",
    "    d_inv = np.power(rowsum, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    # sparse diagonal matrix with the normalizing factors in the diagonal\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "    # dot product resulting in a row-normalised version of the input matrix\n",
    "    norm_adj = d_mat_inv.dot(adj)\n",
    "    return norm_adj.tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function in the next cell is used to check the expression 8 in their paper, where the Laplacian Matrix is formulated as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\text{D}^{\\frac{-1}{2}}\\text{A}\\text{D}^{\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "where D is the diagonal degree matrix and A:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} \n",
    "0 & \\text{R} \\\\\n",
    "\\text{R}^{\\text{T}} & 0 \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_adj_if_equal(adj):\n",
    "    dense_A = np.array(adj.todense())\n",
    "    degree = np.sum(dense_A, axis=1, keepdims=False)\n",
    "    temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build A, the adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17641139030456543\n"
     ]
    }
   ],
   "source": [
    "adj_mat = sp.dok_matrix((n_users + n_items, n_users + n_items), dtype=np.float32)\n",
    "adj_mat = adj_mat.tolil()\n",
    "\n",
    "# A:\n",
    "s = time()\n",
    "adj_mat[:n_users, n_users:] = Rtr.tolil()\n",
    "adj_mat[n_users:, :n_users] = Rtr.tolil().T\n",
    "print(time()-s)\n",
    "adj_mat = adj_mat.todok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3000x3000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 48456 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "along with the \"normal\" adjancecy matrix, we generate two additional ones\n",
    "\n",
    "`norm_adj_mat`: each decay factor bewteen two connected nodes is set as `1/(out degree of the node + self-conncetion)`\n",
    "\n",
    "`mean_adj_mat`: each decay factor bewteen two connected nodes is set as `1/(out degree of the node)`\n",
    "\n",
    "eventually a forth one will also be used (in fact is the default one) which will be\n",
    "\n",
    "`ngcf_adj_mat`: each decay factor bewteen two connected nodes is set as `1/(out degree of the node)` and each node is also assigned with 1 for self-connections. This is: `norm_adj_mat + sp.eye(mean_adj.shape[0])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "mean_adj_mat = normalized_adj_single(adj_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to the 1st row and search for non-zero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid0_nonzero = np.where(adj_mat[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([1056, 1108, 1169, 1177, 1270, 1282, 1292, 1367, 1421, 1486, 1579,\n",
       "        1589, 1631, 1634, 1664, 1672, 1724, 1729, 1745, 2065, 2073, 2117,\n",
       "        2248, 2365, 2370, 2407, 2441, 2562, 2622, 2625, 2672, 2713, 2751,\n",
       "        2758, 2803, 2897, 2913, 2943]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid0_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the training data for the 1st user (id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 108, 169, 177, 270, 282, 292, 367, 421, 486, 579, 589, 631, 634, 664, 672, 724, 729, 745, 1065, 1073, 1117, 1248, 1365, 1370, 1407, 1441, 1562, 1622, 1625, 1672, 1713, 1751, 1758, 1803, 1897, 1913, 1943]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(train_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uid0_nonzero[0]) == len(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the 1st element different than 0 is `n_users + train_set[0][0]`. Note that if we included self-connections, the elements in the diagonal would also be diff than 0. \n",
    "\n",
    "Let's now create \"negative pools\", simply collections of N items that users never interacted with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pools = {}\n",
    "for u in train_set.keys():\n",
    "    neg_items = list(set(range(n_items)) - set(train_set[u]))\n",
    "    pools = np.random.choice(neg_items, 10)\n",
    "    neg_pools[u] = pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1733,  537, 1806,  655, 1140, 1846, 1995,  701, 1082,  677])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_pools[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions sample positive and negative (never seen or interacted with) items either directly from the dataset, or from the previously generated \"negative pools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pos_items_for_u(u, num):\n",
    "    pos_items = train_set[u]\n",
    "    n_pos_items = len(pos_items)\n",
    "    pos_batch = []\n",
    "    while True:\n",
    "        # Once we have sample num positive items, stop\n",
    "        if len(pos_batch) == num: break\n",
    "        pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "        pos_i_id = pos_items[pos_id]\n",
    "        if pos_i_id not in pos_batch: pos_batch.append(pos_i_id)\n",
    "    return pos_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_neg_items_for_u(u, num):\n",
    "    neg_items = []\n",
    "    while True:\n",
    "        # Once we have sample num negative items, stop\n",
    "        if len(neg_items) == num: break\n",
    "        neg_id = np.random.randint(low=0, high=n_items,size=1)[0]\n",
    "        if neg_id not in train_set[u] and neg_id not in neg_items:\n",
    "            neg_items.append(neg_id)\n",
    "    return neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_neg_items_for_u_from_pools(u, num):\n",
    "    # this line must be a bug because no train_items[u] will ever be in neg_pools[u], \n",
    "    # neg_items = list(set(range(n_items)) - set(train_set[u]))\n",
    "    # pools = np.random.choice(neg_items, 100)\n",
    "    # neg_pools[u] = pools\n",
    "    neg_items = list(set(neg_pools[u]) - set(train_set[u]))\n",
    "    return rd.sample(neg_items, num)\n",
    "\n",
    "# To me this should be\n",
    "def sample_neg_items_for_u_from_pools(u, num):\n",
    "    return rd.sample(neg_pools[u], num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, pos_items, neg_items = [], [], []\n",
    "for u in np.random.choice(exist_users, 5):\n",
    "    users.append(u)\n",
    "    pos_items += sample_pos_items_for_u(u, 1)\n",
    "    neg_items += sample_neg_items_for_u(u, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 290, 941, 149, 403]\n",
      "[1284, 1843, 1741, 880, 1965]\n",
      "[1052, 1695, 574, 1246, 1983]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(users), print(pos_items), print(neg_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see if item `pos_items[2]` and `neg_items[2]` are positive and negative respectively for user `users[2]` (for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(pos_items[2] in train_set[users[2]])\n",
    "print(neg_items[2] not in train_set[users[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is about it for us, because the functions below will not be used in this repo. \n",
    "\n",
    "These functions correspond to their study of the effect of sparsity. Have a look to their section 4.3.2 Performance Comparison w.r.t. Interaction Sparsity Levels: \"*.... In particular, based on interaction number per user, we divide the test set into four groups, each of which has the same total interactions...*\"\n",
    "\n",
    "Nonetheless, here is the code and an explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparsity_split():\n",
    "    all_users_to_test = list(test_set.keys())\n",
    "    user_n_iid = dict()\n",
    "\n",
    "    # generate a dictionary to store (key=n_iids, value=a list of uid).\n",
    "    for uid in all_users_to_test:\n",
    "        # train and test items for user_id\n",
    "        train_iids = train_set[uid]\n",
    "        test_iids = test_set[uid]\n",
    "\n",
    "        # number of \"interactions\"\n",
    "        n_iids = len(train_iids) + len(test_iids)\n",
    "\n",
    "        if n_iids not in user_n_iid.keys():\n",
    "            # dictionary where the keys are the number of interactions \n",
    "            # and the values are the users that have that number of interactions\n",
    "            user_n_iid[n_iids] = [uid]\n",
    "        else:\n",
    "            user_n_iid[n_iids].append(uid)\n",
    "    split_uids = list()\n",
    "\n",
    "    # split the whole user set into four subset.\n",
    "    temp = []\n",
    "    count = 1\n",
    "    fold = 4\n",
    "    # total number of interactions in the dataset\n",
    "    n_count = (n_train + n_test) \n",
    "    n_rates = 0\n",
    "\n",
    "    split_state = []\n",
    "    for idx, n_iids in enumerate(sorted(user_n_iid)):\n",
    "        temp += user_n_iid[n_iids]\n",
    "        # n_rates -> number of ratings\n",
    "        # n_iids  -> key corresponding to a certain number of interactions (e.g. 10 ratins)\n",
    "        # len(user_n_iid[n_iids]) -> number of users that interacted with 10 items\n",
    "        n_rates += n_iids * len(user_n_iid[n_iids])\n",
    "        n_count -= n_iids * len(user_n_iid[n_iids])\n",
    "        # when number of rates/interaction has reached 25% of the total number of interactions, \n",
    "        # append the corresponding users to split_uids (remember we loop over sorted(user_n_iid))\n",
    "        if n_rates >= count * 0.25 * (n_train + n_test):\n",
    "            split_uids.append(temp)\n",
    "\n",
    "            state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' %(n_iids, len(temp), n_rates)\n",
    "            split_state.append(state)\n",
    "            print(state)\n",
    "\n",
    "            temp = []\n",
    "            n_rates = 0\n",
    "            fold -= 1 # don't think we need this if we manually state 0.25\n",
    "        \n",
    "        if idx == len(user_n_iid.keys()) - 1 or n_count == 0:\n",
    "            split_uids.append(temp)\n",
    "\n",
    "            state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' % (n_iids, len(temp), n_rates)\n",
    "            split_state.append(state)\n",
    "            print(state)\n",
    "    return split_uids, split_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity_split():\n",
    "    # here, once the previous function is understood, there is not much to explain\n",
    "    try:\n",
    "        split_uids, split_state = [], []\n",
    "        lines = open(path + '/sparsity.split', 'r').readlines()\n",
    "\n",
    "        for idx, line in enumerate(lines):\n",
    "            if idx % 2 == 0:\n",
    "                split_state.append(line.strip())\n",
    "                print(line.strip())\n",
    "            else:\n",
    "                split_uids.append([int(uid) for uid in line.strip().split(' ')])\n",
    "        print('get sparsity split.')\n",
    "\n",
    "    except Exception:\n",
    "        split_uids, split_state = create_sparsity_split()\n",
    "        f = open(path + '/sparsity.split', 'w')\n",
    "        for idx in range(len(split_state)):\n",
    "            f.write(split_state[idx] + '\\n')\n",
    "            f.write(' '.join([str(uid) for uid in split_uids[idx]]) + '\\n')\n",
    "        print('create sparsity split.')\n",
    "\n",
    "    return split_uids, split_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
