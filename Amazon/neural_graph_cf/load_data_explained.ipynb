{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "import pdb\n",
    "\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/home/ubuntu/projects/neural_graph_cf/Data/gowalla\")\n",
    "batch_size = 32\n",
    "\n",
    "train_file = path/'train.txt'\n",
    "test_file = path/'test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users and items are numbered from 0 to (n_users-1) and (n_items-1), so let's count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of users and items. \n",
    "n_users, n_items = 0, 0\n",
    "n_train, n_test = 0, 0\n",
    "\n",
    "exist_users = []\n",
    "with open(train_file) as f:\n",
    "    for l in f.readlines():\n",
    "        if len(l) > 0:\n",
    "            l = l.strip('\\n').split(' ')\n",
    "            # first element is the user_id, then items\n",
    "            uid = int(l[0])\n",
    "            items = [int(i) for i in l[1:]]\n",
    "            exist_users.append(uid)\n",
    "            n_items = max(n_items, max(items))\n",
    "            n_users = max(n_users, uid)\n",
    "            n_train += len(items)\n",
    "\n",
    "# same as before but for testing\n",
    "with open(test_file) as f:\n",
    "    for l in f.readlines():\n",
    "        if len(l) > 0:\n",
    "            l = l.strip('\\n')\n",
    "            try:\n",
    "                items = [int(i) for i in l.split(' ')[1:]]\n",
    "            except Exception:\n",
    "                continue\n",
    "            n_items = max(n_items, max(items))\n",
    "            n_test += len(items)\n",
    "n_items += 1\n",
    "n_users += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40981 29858\n"
     ]
    }
   ],
   "source": [
    "print(n_items, n_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the interactions/ratings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = sp.dok_matrix((n_users, n_items), dtype=np.float32)\n",
    "train_set, test_set = {}, {}\n",
    "with open(train_file) as f_train, open(test_file) as f_test:\n",
    "    for l in f_train.readlines():\n",
    "        if len(l) == 0: break\n",
    "        l = l.strip('\\n')\n",
    "        items = [int(i) for i in l.split(' ')]\n",
    "        uid, train_items = items[0], items[1:]\n",
    "        # simply 1 if user interacted with item, otherwise, 0.\n",
    "        for i in train_items:\n",
    "            R[uid, i] = 1.\n",
    "        train_set[uid] = train_items\n",
    "\n",
    "    for l in f_test.readlines():\n",
    "        if len(l) == 0: break\n",
    "        l = l.strip('\\n')\n",
    "        try:\n",
    "            items = [int(i) for i in l.split(' ')]\n",
    "        except Exception:\n",
    "            continue\n",
    "        uid, test_items = items[0], items[1:]\n",
    "        test_set[uid] = test_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<29858x40981 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 810128 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[7580, 3730, 5983, 5990, 7608, 1213, 6017, 7510, 7513, 8343]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0][:10])\n",
    "print(test_set[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use a number of difference adjacency matrix, see [here](https://github.com/xiangwang1223/neural_graph_collaborative_filtering): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_adj_single(adj):\n",
    "    # rowsum = out-degree of the node    \n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    # inverted and set to 0 if no connections\n",
    "    d_inv = np.power(rowsum, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    # sparse diagonal matrix with the normalizing factors in the diagonal\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "    # dot product resulting in a row-normalised version of the input matrix\n",
    "    norm_adj = d_mat_inv.dot(adj)\n",
    "    return norm_adj.tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to check the expression 8 in their paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_adj_if_equal(adj):\n",
    "    dense_A = np.array(adj.todense())\n",
    "    degree = np.sum(dense_A, axis=1, keepdims=False)\n",
    "    temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat = sp.dok_matrix((n_users + n_items, n_users + n_items), dtype=np.float32)\n",
    "adj_mat = adj_mat.tolil()\n",
    "\n",
    "# This would be their A matrix in expression 8\n",
    "adj_mat[:n_users, n_users:] = R.tolil()\n",
    "adj_mat[n_users:, :n_users] = R.tolil().T\n",
    "adj_mat = adj_mat.todok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<70839x70839 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 1620256 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "along with the \"normal\" adjancecy matrix, we generate two additional ones\n",
    "\n",
    "`norm_adj_mat`: each decay factor bewteen two connected nodes is set as 1/(out degree of the node + self-conncetion)\n",
    "`mean_adj_mat`: each decay factor bewteen two connected nodes is set as 1/(out degree of the node)\n",
    "\n",
    "eventually a forth one will also be used which will be\n",
    "`norm_adj_mat + sp.eye(mean_adj.shape[0])`: each decay factor bewteen two connected nodes is set as 1/(out degree of the node) and each node is also assigned with 1 for self-connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "mean_adj_mat = normalized_adj_single(adj_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to the 1st row and search for non-zero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid0_nonzero = np.where(adj_mat[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([29858, 29859, 29860, 29861, 29862, 29863, 29864, 29865, 29866,\n",
       "        29867, 29868, 29869, 29870, 29871, 29872, 29873, 29874, 29875,\n",
       "        29876, 29877, 29878, 29879, 29880, 29881, 29882, 29883, 29884,\n",
       "        29885, 29886, 29887, 29888, 29889, 29890, 29891, 29892, 29893,\n",
       "        29894, 29895, 29896, 29897, 29898, 29899, 29900, 29901, 29902,\n",
       "        29903, 29904, 29905, 29906, 29907, 29908, 29909, 29910, 29911,\n",
       "        29912, 29913, 29914, 29915, 29916, 29917, 29918, 29919, 29920,\n",
       "        29921, 29922, 29923, 29924, 29925, 29926, 29927, 29928, 29929,\n",
       "        29930, 29931, 29932, 29933, 29934, 29935, 29936, 29937, 29938,\n",
       "        29939, 29940, 29941, 29942, 29943, 29944, 29945, 29946, 29947,\n",
       "        29948, 29949, 29950, 29951, 29952, 29953, 29954, 29955, 29956,\n",
       "        29957, 29958, 29959, 29960, 29961, 29962, 29963, 29964, 29965,\n",
       "        29966, 29967, 29968, 29969, 29970, 29971, 29972, 29973, 29974,\n",
       "        29975, 29976, 29977, 29978, 29979, 29980, 29981, 29982, 29983,\n",
       "        29984]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid0_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the training data for the 1st user (id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uid0_nonzero[0]) == len(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the 1st element different than 0 is 29858, which is equal to the number of users. From there in advance the non-zero elements are ordered consecutively through the next 126 numbers. Note that if we included self-connections, the elements in the diagonal woudl also be diff than 0. \n",
    "\n",
    "Let's now create \"negative pools\", simply collections of 100 items that users never interacted with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29858/29858 [02:40<00:00, 185.52it/s]\n"
     ]
    }
   ],
   "source": [
    "neg_pools = {}\n",
    "for u in tqdm(train_set.keys()):\n",
    "    neg_items = list(set(range(n_items)) - set(train_set[u]))\n",
    "    pools = np.random.choice(neg_items, 100)\n",
    "    neg_pools[u] = pools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions sample positive and negative items either directly from the dataset, or from the previously generated \"negative pools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pos_items_for_u(u, num):\n",
    "    pos_items = train_set[u]\n",
    "    n_pos_items = len(pos_items)\n",
    "    pos_batch = []\n",
    "    while True:\n",
    "        if len(pos_batch) == num: break\n",
    "        pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "        pos_i_id = pos_items[pos_id]\n",
    "\n",
    "        if pos_i_id not in pos_batch:\n",
    "            pos_batch.append(pos_i_id)\n",
    "    return pos_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_neg_items_for_u(u, num):\n",
    "    neg_items = []\n",
    "    while True:\n",
    "        if len(neg_items) == num: break\n",
    "        neg_id = np.random.randint(low=0, high=n_items,size=1)[0]\n",
    "        if neg_id not in train_set[u] and neg_id not in neg_items:\n",
    "            neg_items.append(neg_id)\n",
    "    return neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_neg_items_for_u_from_pools(u, num):\n",
    "    # this line must be a bug because no train_items[u] will ever be in neg_pools[u], \n",
    "    # neg_items = list(set(range(n_items)) - set(train_set[u]))\n",
    "    # pools = np.random.choice(neg_items, 100)\n",
    "    # neg_pools[u] = pools\n",
    "    neg_items = list(set(neg_pools[u]) - set(train_set[u]))\n",
    "    return rd.sample(neg_items, num)\n",
    "\n",
    "# To me this should be\n",
    "def sample_neg_items_for_u_from_pools(u, num):\n",
    "    return rd.sample(neg_pools[u], num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'd say something wild need to happen for this first condition to be true...\n",
    "if batch_size <= n_users:\n",
    "    users = rd.sample(exist_users, batch_size)\n",
    "else:\n",
    "    # I prefer:\n",
    "    # users = np.random.choice(exist_users, batch_size, replace=False)\n",
    "    # This allows for user repetition. Is still ok, since likely it will \n",
    "    # appear with different items after sample_pos_items_for_u(u, 1), but \n",
    "    # still...Maybe is intentional\n",
    "    users = [rd.choice(exist_users) for _ in range(batch_size)]\n",
    "\n",
    "pos_items, neg_items = [], []\n",
    "for u in users:\n",
    "    pos_items += sample_pos_items_for_u(u, 1)\n",
    "    neg_items += sample_neg_items_for_u(u, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32095, 13531, 38977, 6661, 26459, 12706, 23297, 40753, 15854, 372, 28526, 6562, 3211, 30712, 10119, 19004, 21719, 8052, 26495, 26569, 18559, 15043, 7666, 7383, 11197, 16875, 22241, 32487, 1619, 26453, 36401, 4924]\n",
      "[18323, 23035, 13776, 2253, 14696, 2403, 35035, 18789, 39980, 21736, 38653, 37400, 14521, 3717, 21929, 10842, 4253, 26933, 26023, 9167, 28712, 7036, 38847, 17432, 3442, 6357, 37294, 40265, 36642, 13861, 30338, 22653]\n",
      "[22996, 18198, 28724, 6748, 20207, 3565, 18808, 22092, 20675, 22479, 9179, 4620, 977, 29645, 8052, 4787, 12676, 23904, 12416, 23144, 8764, 4733, 6967, 17445, 4683, 12457, 13287, 26701, 11972, 21529, 24212, 17115]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pos_items), print(neg_items), print(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(pos_items)), print(len(neg_items)), print(len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(810128, 217242)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train, n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is about it for us, because the functions below will not be used in this repo. # this function is something that I will not use in this repo. These correspond to their study of the effect of sparsity. Have a look to their section 4.3.2 Performance Comparison w.r.t. Interaction Sparsity Levels: \".... In particular, based on interaction number per user, we divide the test set into four groups, each of which has the same total interactions...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparsity_split():\n",
    "    all_users_to_test = list(test_set.keys())\n",
    "    user_n_iid = dict()\n",
    "\n",
    "    # generate a dictionary to store (key=n_iids, value=a list of uid).\n",
    "    for uid in all_users_to_test:\n",
    "        # train and test items for user_id\n",
    "        train_iids = train_set[uid]\n",
    "        test_iids = test_set[uid]\n",
    "\n",
    "        # number of \"interactions\"\n",
    "        n_iids = len(train_iids) + len(test_iids)\n",
    "\n",
    "        if n_iids not in user_n_iid.keys():\n",
    "            # dictionary where the keys are the number of interactions \n",
    "            # and the values are the users that have that number of interactions\n",
    "            user_n_iid[n_iids] = [uid]\n",
    "        else:\n",
    "            user_n_iid[n_iids].append(uid)\n",
    "    split_uids = list()\n",
    "\n",
    "    # split the whole user set into four subset.\n",
    "    temp = []\n",
    "    count = 1\n",
    "    fold = 4\n",
    "    # total number of interactions in the dataset\n",
    "    n_count = (n_train + n_test) \n",
    "    n_rates = 0\n",
    "\n",
    "    split_state = []\n",
    "    for idx, n_iids in enumerate(sorted(user_n_iid)):\n",
    "        temp += user_n_iid[n_iids]\n",
    "        # n_rates -> number of ratings\n",
    "        # n_iids  -> key corresponding to a certain number of interactions (e.g. 10 ratins)\n",
    "        # len(user_n_iid[n_iids]) -> number of users that interacted with 10 items\n",
    "        n_rates += n_iids * len(user_n_iid[n_iids])\n",
    "        n_count -= n_iids * len(user_n_iid[n_iids])\n",
    "        # when number of rates/interaction has reached 25% of the total number of interactions, \n",
    "        # append the corresponding users to split_uids (remember we loop over sorted(user_n_iid))\n",
    "        if n_rates >= count * 0.25 * (n_train + n_test):\n",
    "            split_uids.append(temp)\n",
    "\n",
    "            state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' %(n_iids, len(temp), n_rates)\n",
    "            split_state.append(state)\n",
    "            print(state)\n",
    "\n",
    "            temp = []\n",
    "            n_rates = 0\n",
    "            fold -= 1 # don't think we need this if we manually state 0.25\n",
    "        \n",
    "        if idx == len(user_n_iid.keys()) - 1 or n_count == 0:\n",
    "            split_uids.append(temp)\n",
    "\n",
    "            state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' % (n_iids, len(temp), n_rates)\n",
    "            split_state.append(state)\n",
    "            print(state)\n",
    "    return split_uids, split_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity_split():\n",
    "    try:\n",
    "        split_uids, split_state = [], []\n",
    "        lines = open(path + '/sparsity.split', 'r').readlines()\n",
    "\n",
    "        for idx, line in enumerate(lines):\n",
    "            if idx % 2 == 0:\n",
    "                split_state.append(line.strip())\n",
    "                print(line.strip())\n",
    "            else:\n",
    "                split_uids.append([int(uid) for uid in line.strip().split(' ')])\n",
    "        print('get sparsity split.')\n",
    "\n",
    "    except Exception:\n",
    "        split_uids, split_state = create_sparsity_split()\n",
    "        f = open(path + '/sparsity.split', 'w')\n",
    "        for idx in range(len(split_state)):\n",
    "            f.write(split_state[idx] + '\\n')\n",
    "            f.write(' '.join([str(uid) for uid in split_uids[idx]]) + '\\n')\n",
    "        print('create sparsity split.')\n",
    "\n",
    "    return split_uids, split_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_ngcf)",
   "language": "python",
   "name": "conda_ngcf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
