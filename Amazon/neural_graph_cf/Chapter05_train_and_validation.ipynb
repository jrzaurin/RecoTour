{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "Let's start by manually defining some neccesary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import scipy.sparse as sp\n",
    "import multiprocessing\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from time import time\n",
    "from functools import partial\n",
    "from utils.load_data import Data\n",
    "from utils.metrics import ranklist_by_heapq, get_performance\n",
    "# from utils.parser import parse_args\n",
    "from ngcf import NGCF\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=1000, n_items=2000\n",
      "n_interactions=30780\n",
      "n_train=24228, n_test=6552, sparsity=0.01539\n",
      "already load adj matrix (3000, 3000) 0.012362957000732422\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "Ks = [10, 20]\n",
    "\n",
    "data_path = \"Data/toy_data/\"\n",
    "batch_size = 64\n",
    "data_generator = Data(data_path, batch_size, val=False)\n",
    "n_users = data_generator.n_users\n",
    "n_items = data_generator.n_items\n",
    "\n",
    "_, _, mean_adj = data_generator.get_adj_mat()\n",
    "adj_mtx = mean_adj + sp.eye(mean_adj.shape[0])\n",
    "\n",
    "emb_dim = 12\n",
    "layers = [12, 6]\n",
    "node_dropout = 0.1\n",
    "mess_dropout = [0.1]*len(layers)\n",
    "reg = 1e-5\n",
    "lr = 0.01\n",
    "n_fold = 10\n",
    "\n",
    "pretrain = 0\n",
    "\n",
    "print_every, eval_every, save_every = 1, 1, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGCF(data_generator.n_users, data_generator.n_items, emb_dim, layers, \n",
    "             reg,node_dropout, mess_dropout, adj_mtx, n_fold)\n",
    "if use_cuda: \n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_embeddings\n",
      "i_embeddings\n",
      "u_g_embeddings\n",
      "i_g_embeddings\n",
      "W1.0.weight\n",
      "W1.0.bias\n",
      "W1.1.weight\n",
      "W1.1.bias\n",
      "W2.0.weight\n",
      "W2.0.bias\n",
      "W2.1.weight\n",
      "W2.1.bias\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    if p.requires_grad: print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is it really regarding the parameters of the model. We only need the user/item embeddings and then a series of linear layers (that we could refer as graph layers). The embeddings will be concatenated over rows, multiplied by the Laplacian matrix, and then passed through a the graph/linear layers recursively. \n",
    "\n",
    "Let's now move to the training phase. The training phase is your typical pytorch training function, with the exception that the output of the forward pass is already the [BPR](https://arxiv.org/pdf/1205.2618.pdf) loss. I will leave out of this notebook schedulers, different optimizers and some other rings and bells.\n",
    "\n",
    "It goes this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_generator, optimizer):\n",
    "    model.train()\n",
    "    n_batch = data_generator.n_train // data_generator.batch_size + 1\n",
    "    running_loss=0\n",
    "    for _ in range(n_batch):\n",
    "        u, i, j = data_generator.sample()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(u,i,j)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not talked about the BPR loss yet, so let's have a look. The definition in the [paper](https://arxiv.org/pdf/1905.08108.pdf) is:\n",
    "\n",
    "$$\n",
    "Loss = \\sum_{(u,i,j) \\in \\mathcal{O}} -ln \\big(\\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})\\big) + \\lambda ||\\Theta||^{2}_{2}\n",
    "$$\n",
    "\n",
    "Where $\\mathcal{O} = \\{ (u,i,j)|(u,i) \\in  R^{+}, (u,j) \\in R^{-} \\}$ is the set of training tuples with $R^{+}$ and $R^{-}$ corresponding to observed and unobserved interactions (aka positive and negative) respectively. $\\sigma$ is the sigmoid function and $||\\Theta|| = \\{ \\text{E}, \\{ \\textbf{W}^{l}_{1},\\textbf{W}^{l}_{2} \\}^{L}_{l=1}  \\}$ are all training parameters. \n",
    "\n",
    "In pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(self, u, i, j):\n",
    "    # first term\n",
    "    y_ui = torch.mul(u, i).sum(dim=1)\n",
    "    y_uj = torch.mul(u, j).sum(dim=1)\n",
    "    log_prob = (torch.log(torch.sigmoid(y_ui-y_uj))).mean()\n",
    "\n",
    "    # regularization (to be honest this does not help much when using pytorch)\n",
    "    l2norm = (torch.sum(u**2)/2. + torch.sum(i**2)/2. + torch.sum(j**2)/2.).mean()\n",
    "    l2reg  = reg*l2norm\n",
    "\n",
    "    #Â Loss\n",
    "    return -log_prob + l2reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay, so now we now how the training happens, let's move to the validation/testing. Here, we will first use the authors `early_stopping` function. I am sure there are more \"pytorchian\" ways of doing it, but this function is simple and does the job, so let's use it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(log_value, best_value, stopping_step, expected_order='asc', patience=10):\n",
    "\n",
    "    # better is higher or lower\n",
    "    assert expected_order in ['asc', 'dec']\n",
    "\n",
    "    if (expected_order == 'asc' and log_value >= best_value) or (expected_order == 'dec' and log_value <= best_value):\n",
    "        stopping_step = 0\n",
    "        best_value = log_value\n",
    "    else:\n",
    "        stopping_step += 1\n",
    "\n",
    "    if stopping_step >= patience:\n",
    "        print(\"Early stopping is trigger at step: {} log:{}\".format(patience, log_value))\n",
    "        should_stop = True\n",
    "    else:\n",
    "        should_stop = False\n",
    "\n",
    "    return best_value, stopping_step, should_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we test on one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_user(x):\n",
    "    rating = x[0]\n",
    "    u = x[1]\n",
    "\n",
    "    try:\n",
    "        training_items = data_generator.train_items[u]\n",
    "    except Exception:\n",
    "        training_items = []\n",
    "\n",
    "    user_pos_test = data_generator.test_set[u]\n",
    "    all_items = set(range(data_generator.n_items))\n",
    "    test_items = list(all_items - set(training_items))\n",
    "    r = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
    "\n",
    "    return get_performance(user_pos_test, r, Ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that we know how to test in one user, let's do it for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_CPU(model, users_to_test):\n",
    "    model.eval()\n",
    "    result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "              'hit_ratio': np.zeros(len(Ks))}\n",
    "\n",
    "    pool = multiprocessing.Pool(cores)\n",
    "\n",
    "    u_batch_size = batch_size * 2\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for u_batch_id in range(n_user_batchs):\n",
    "\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "        user_batch = test_users[start: end]\n",
    "        item_batch = range(data_generator.n_items)\n",
    "\n",
    "        user_emb = model.u_g_embeddings[user_batch].detach()\n",
    "\n",
    "        rate_batch  = torch.mm(user_emb, model.i_g_embeddings.t().detach()).cpu().numpy()\n",
    "\n",
    "        user_batch_rating_uid = zip(rate_batch, user_batch)\n",
    "        batch_result = pool.map(test_one_user, user_batch_rating_uid)\n",
    "        count += len(batch_result)\n",
    "\n",
    "        for re in batch_result:\n",
    "            result['precision'] += re['precision']/n_test_users\n",
    "            result['recall'] += re['recall']/n_test_users\n",
    "            result['ndcg'] += re['ndcg']/n_test_users\n",
    "            result['hit_ratio'] += re['hit_ratio']/n_test_users\n",
    "\n",
    "    assert count == n_test_users\n",
    "    pool.close()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how all comes together! (Note that the process here is **extremely** inefficient since we are splitting a 3000x3000 matrix into 10 folds and using a 32 batch for only 1000 users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 94.24s, Loss = 183.0861\n",
      "VALIDATION: \n",
      " Epoch: 0, 0.65s \n",
      " Recall@10: 0.0056, Recall@20: 0.0109 \n",
      " Precision@10: 0.0036, Precision@20: 0.0035 \n",
      " Hit_ratio@10: 0.0360, Hit_ratio@20: 0.0660 \n",
      " NDCG@10: 0.0185, NDCG@20: 0.0265\n",
      "Epoch:1 94.42s, Loss = 169.0698\n",
      "VALIDATION: \n",
      " Epoch: 1, 0.64s \n",
      " Recall@10: 0.0063, Recall@20: 0.0108 \n",
      " Precision@10: 0.0042, Precision@20: 0.0034 \n",
      " Hit_ratio@10: 0.0420, Hit_ratio@20: 0.0650 \n",
      " NDCG@10: 0.0167, NDCG@20: 0.0229\n"
     ]
    }
   ],
   "source": [
    "stopping_step, should_stop = 0, False\n",
    "for epoch in range(2):\n",
    "\n",
    "    t1 = time()\n",
    "    loss = train(model, data_generator, optimizer)\n",
    "\n",
    "    if epoch % print_every  == (print_every - 1):\n",
    "        print(\"Epoch:{} {:.2f}s, Loss = {:.4f}\".\n",
    "            format(epoch, time()-t1, loss))\n",
    "\n",
    "    if epoch % eval_every  == (eval_every - 1):\n",
    "        with torch.no_grad():\n",
    "            t2 = time()\n",
    "            users_to_test = list(data_generator.test_set.keys())\n",
    "            res = test_CPU(model, users_to_test)\n",
    "        print(\n",
    "            \"VALIDATION:\",\"\\n\",\n",
    "            \"Epoch: {}, {:.2f}s\".format(epoch, time()-t2),\"\\n\",\n",
    "            \"Recall@{}: {:.4f}, Recall@{}: {:.4f}\".format(Ks[0], res['recall'][0],  Ks[-1], res['recall'][-1]), \"\\n\",\n",
    "            \"Precision@{}: {:.4f}, Precision@{}: {:.4f}\".format(Ks[0], res['precision'][0],  Ks[-1], res['precision'][-1]), \"\\n\",\n",
    "            \"Hit_ratio@{}: {:.4f}, Hit_ratio@{}: {:.4f}\".format(Ks[0], res['hit_ratio'][0],  Ks[-1], res['hit_ratio'][-1]), \"\\n\",\n",
    "            \"NDCG@{}: {:.4f}, NDCG@{}: {:.4f}\".format(Ks[0], res['ndcg'][0],  Ks[-1], res['ndcg'][-1])\n",
    "            )        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-enabled test\n",
    "\n",
    "We could also use the GPU-enabled test functions I described in Chapter03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def split_mtx(X, n_folds=10):\n",
    "    X_folds = []\n",
    "    fold_len = X.shape[0]//n_folds\n",
    "    for i in range(n_folds):\n",
    "        start = i * fold_len\n",
    "        if i == n_folds -1:\n",
    "            end = X.shape[0]\n",
    "        else:\n",
    "            end = (i + 1) * fold_len\n",
    "        X_folds.append(X[start:end])\n",
    "    return X_folds\n",
    "\n",
    "\n",
    "def ndcg_at_k_gpu(pred_items, test_items, test_indices, k):\n",
    "    r = (test_items * pred_items).gather(1, test_indices)\n",
    "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().cuda()\n",
    "    dcg = (r[:, :k]/f).sum(1)\n",
    "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)\n",
    "    ndcg = dcg/dcg_max\n",
    "    ndcg[torch.isnan(ndcg)] = 0\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def test_GPU(u_emb, i_emb, Rtr, Rte, Ks):\n",
    "\n",
    "    ue_folds = split_mtx(u_emb)\n",
    "    tr_folds = split_mtx(Rtr)\n",
    "    te_folds = split_mtx(Rte)\n",
    "\n",
    "    fold_prec, fold_rec, fold_ndcg, fold_hr = \\\n",
    "        defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    for ue_f, tr_f, te_f in zip(ue_folds, tr_folds, te_folds):\n",
    "\n",
    "        scores = torch.mm(ue_f, i_emb.t())\n",
    "        test_items = torch.from_numpy(te_f.todense()).float().cuda()\n",
    "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().cuda()\n",
    "        scores = scores * non_train_items\n",
    "        _, test_indices = torch.topk(scores, dim=1, k=max(Ks))\n",
    "        pred_items = torch.zeros_like(scores).float()\n",
    "        pred_items.scatter_(dim=1,index=test_indices,src=torch.tensor(1.0).cuda())\n",
    "\n",
    "        for k in Ks:\n",
    "            topk_preds = torch.zeros_like(scores).float()\n",
    "            topk_preds.scatter_(dim=1,index=test_indices[:, :k],src=torch.tensor(1.0))\n",
    "\n",
    "            TP = (test_items * topk_preds).sum(1)\n",
    "            prec = TP/k\n",
    "            rec = TP/test_items.sum(1)\n",
    "            hit_r = (TP > 0).float()\n",
    "            ndcg = ndcg_at_k_gpu(pred_items, test_items, test_indices, k)\n",
    "\n",
    "            fold_prec[k].append(prec)\n",
    "            fold_rec[k].append(rec)\n",
    "            fold_ndcg[k].append(ndcg)\n",
    "            fold_hr[k].append(hit_r)\n",
    "\n",
    "    result = {'precision': [], 'recall': [], 'ndcg': [], 'hit_ratio': []}\n",
    "    for k in Ks:\n",
    "        result['precision'].append(torch.cat(fold_prec[k]).mean())\n",
    "        result['recall'].append(torch.cat(fold_rec[k]).mean())\n",
    "        result['ndcg'].append(torch.cat(fold_ndcg[k]).mean())\n",
    "        result['hit_ratio'].append(torch.cat(fold_hr[k]).mean())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in identical fashion to test, we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 94.42s, Loss = 148.5109\n",
      "VALIDATION. \n",
      "Epoch: 0, 0.27s \n",
      " Recall@10: 0.0045, Recall@20: 0.0081 \n",
      "Precision@10: 0.0031, Precision@20: 0.0028 \n",
      "Hit_ratio@10: 0.0290, Hit_ratio@20: 0.0530 \n",
      "NDCG@10: 0.0147, NDCG@20: 0.0208\n",
      "Epoch:1 94.22s, Loss = 140.6485\n",
      "VALIDATION. \n",
      "Epoch: 1, 0.20s \n",
      " Recall@10: 0.0039, Recall@20: 0.0096 \n",
      "Precision@10: 0.0029, Precision@20: 0.0032 \n",
      "Hit_ratio@10: 0.0270, Hit_ratio@20: 0.0610 \n",
      "NDCG@10: 0.0143, NDCG@20: 0.0228\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "stopping_step, should_stop = 0, False\n",
    "for epoch in range(2):\n",
    "    t1 = time()\n",
    "    loss = train(model, data_generator, optimizer)\n",
    "    if epoch % print_every  == (print_every - 1):\n",
    "        print(\"Epoch:{} {:.2f}s, Loss = {:.4f}\".\n",
    "            format(epoch, time()-t1, loss))\n",
    "    if epoch % eval_every  == (eval_every - 1):\n",
    "        t2 = time()\n",
    "        res = test_GPU(\n",
    "            model.u_g_embeddings.detach(),\n",
    "            model.i_g_embeddings.detach(),\n",
    "            data_generator.Rtr,\n",
    "            data_generator.Rte,\n",
    "            Ks)\n",
    "        print(\"VALIDATION.\",\"\\n\"\n",
    "            \"Epoch: {}, {:.2f}s\".format(epoch, time()-t2),\"\\n\",\n",
    "            \"Recall@{}: {:.4f}, Recall@{}: {:.4f}\".format(Ks[0], res['recall'][0],  Ks[-1], res['recall'][-1]), \"\\n\"\n",
    "            \"Precision@{}: {:.4f}, Precision@{}: {:.4f}\".format(Ks[0], res['precision'][0],  Ks[-1], res['precision'][-1]), \"\\n\"\n",
    "            \"Hit_ratio@{}: {:.4f}, Hit_ratio@{}: {:.4f}\".format(Ks[0], res['hit_ratio'][0],  Ks[-1], res['hit_ratio'][-1]), \"\\n\"\n",
    "            \"NDCG@{}: {:.4f}, NDCG@{}: {:.4f}\".format(Ks[0], res['ndcg'][0],  Ks[-1], res['ndcg'][-1])\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_GPU` test will be a lot faster that `test_CPU` when running the real exercise. For example, for the `Gowalla` dataset used by the authors, using a batch size of 5096 the testing time is nearly 5 min when using `test_CPU` and is less than 15sec when using `test_GPU`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
