{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Graph Collaborative Filtering\n",
    "\n",
    "Here I intend to describe the algorithm presented in [Wang Xiang et al. Neural Graph Collaborative Filtering](https://arxiv.org/pdf/1905.08108.pdf) and my implementation using pytorch. \n",
    "\n",
    "I will go step by step (where *\"step\"* is mostly defined by my understanding) with snippets in tensorflow and their corresponding \"translation\" into pytorch.\n",
    "\n",
    "Here we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.dataset import Data\n",
    "from utils.metrics import *\n",
    "from utils.parser import parse_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be *\"imperatively\"* comparing `tf` and `pytorch`, and for that we need `tf` eager execution enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using our toy-data example so this notebook can initially run in any laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=1000, n_items=2000\n",
      "n_interactions=30780\n",
      "n_train=24228, n_test=6552, sparsity=0.01539\n",
      "already load adj matrix (3000, 3000) 0.021552085876464844\n"
     ]
    }
   ],
   "source": [
    "# 1000 users and 2000 items\n",
    "data_path = \"Data/toy_data/\"\n",
    "batch_size = 32\n",
    "# for the toy dataset I did not create a validation set\n",
    "data_generator = Data(data_path, batch_size, val=False)\n",
    "_, _, mean_adj = data_generator.get_adj_mat()\n",
    "adjacency_matrix = mean_adj + sp.eye(mean_adj.shape[0])\n",
    "n_users = data_generator.n_users\n",
    "n_items = data_generator.n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "emb_size = 12\n",
    "layers = [12, 6]\n",
    "n_layers = len(layers)\n",
    "node_dropout = 0.1\n",
    "mess_dropout = [0.1]*len(layers)\n",
    "regularization = 1e-5\n",
    "lr = 0.01\n",
    "n_fold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialise weights. They (Wang Xiang et al) do it internally in their `NGCF` class, considering already the use of pre-trained weights. In my case, I will simply initialise all weights as if there were not pretrained and I will deal with pretrained weights outside the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF\n",
    "\n",
    "Their code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data = None\n",
    "def _init_weights_tf():\n",
    "    all_weights = dict()\n",
    "\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "    if pretrain_data is None:\n",
    "        all_weights['user_embedding'] = tf.Variable(initializer([n_users, emb_size]), name='user_embedding')\n",
    "        all_weights['item_embedding'] = tf.Variable(initializer([n_items, emb_size]), name='item_embedding')\n",
    "        print('using xavier initialization')\n",
    "    else:\n",
    "        all_weights['user_embedding'] = tf.Variable(initial_value=pretrain_data['user_embed'], trainable=True,\n",
    "                                                    name='user_embedding', dtype=tf.float32)\n",
    "        all_weights['item_embedding'] = tf.Variable(initial_value=pretrain_data['item_embed'], trainable=True,\n",
    "                                                    name='item_embedding', dtype=tf.float32)\n",
    "        print('using pretrained initialization')\n",
    "\n",
    "    weight_size_list = [emb_size] + layers\n",
    "    for k in range(n_layers):\n",
    "        # k = 0 are the embeddings\n",
    "        all_weights['W_gc_%d' %k] = tf.Variable(\n",
    "            initializer([weight_size_list[k], weight_size_list[k+1]]), name='W_gc_%d' % k)\n",
    "        all_weights['b_gc_%d' %k] = tf.Variable(\n",
    "            initializer([1, weight_size_list[k+1]]), name='b_gc_%d' % k)\n",
    "\n",
    "        all_weights['W_bi_%d' % k] = tf.Variable(\n",
    "            initializer([weight_size_list[k], weight_size_list[k + 1]]), name='W_bi_%d' % k)\n",
    "        all_weights['b_bi_%d' % k] = tf.Variable(\n",
    "            initializer([1, weight_size_list[k+1]]), name='b_bi_%d' % k)\n",
    "    return all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "using xavier initialization\n"
     ]
    }
   ],
   "source": [
    "weights = _init_weights_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['user_embedding', 'item_embedding', 'W_gc_0', 'b_gc_0', 'W_bi_0', 'b_bi_0', 'W_gc_1', 'b_gc_1', 'W_bi_1', 'b_bi_1'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 12)\n",
      "(2000, 12)\n",
      "(12, 12)\n",
      "(12, 6)\n"
     ]
    }
   ],
   "source": [
    "print(weights['user_embedding'].shape)\n",
    "print(weights['item_embedding'].shape)\n",
    "print(weights['W_gc_0'].shape)\n",
    "print(weights['W_gc_1'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "\n",
    "As I mentioned, I will simply initialise the weights as with no pretrained and I will deal with pretrained weights outside the model class (see `run_bpr.py`)\n",
    "\n",
    "In pytorch I simply do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run. This will go inside the model class\n",
    "def _init_weights(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now focus on the 4 helpers that will be needed to create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert sparse matrix to sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_sp_mat_to_sp_tensor_tf(X):\n",
    "    coo = X.tocoo().astype(np.float32)\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node Dropout a for sparse tensor (see below, I'd call this edge dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dropout_sparse(X, keep_prob, n_nonzero_elems):\n",
    "    \"\"\"\n",
    "    Dropout for sparse tensors.\n",
    "    \"\"\"\n",
    "    noise_shape = [n_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(X, dropout_mask)\n",
    "    \n",
    "    return pre_out * tf.math.divide(1., keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the large sparse adjancecy matrix in n folds, with/without node dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_A_hat_tf(X):\n",
    "    \"split the Adjancency matrix so is tractable\"\n",
    "    A_fold_hat = []\n",
    "\n",
    "    fold_len = (n_users + n_items) // n_fold\n",
    "    for i_fold in range(n_fold):\n",
    "        start = i_fold * fold_len\n",
    "        if i_fold == n_fold -1:\n",
    "            end = n_users + n_items\n",
    "        else:\n",
    "            end = (i_fold + 1) * fold_len\n",
    "\n",
    "        A_fold_hat.append(_convert_sp_mat_to_sp_tensor_tf(X[start:end]))\n",
    "    return A_fold_hat\n",
    "\n",
    "def _split_A_hat_node_dropout_tf(X):\n",
    "    A_fold_hat = []\n",
    "\n",
    "    fold_len = (n_users + n_items) // n_fold\n",
    "    for i_fold in range(n_fold):\n",
    "        start = i_fold * fold_len\n",
    "        if i_fold == n_fold -1:\n",
    "            end = n_users + n_items\n",
    "        else:\n",
    "            end = (i_fold + 1) * fold_len\n",
    "\n",
    "        temp = _convert_sp_mat_to_sp_tensor_tf(X[start:end])\n",
    "        n_nonzero_temp = X[start:end].count_nonzero()\n",
    "        A_fold_hat.append(_dropout_sparse(temp, 1 - node_dropout, n_nonzero_temp))\n",
    "\n",
    "    return A_fold_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving into the code, I would like stop one second at their method `_dropout_sparse`. This method is intended to drop out nodes, and all their connections. However, I do not think the code in their original repo (the same as above) does that. I think their code drops edges. Let's see. After running: \n",
    "\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "   \n",
    "they have a boolean tensor indicating **the locations** to keep (see [here](https://www.tensorflow.org/api_docs/python/tf/sparse/retain)). Therefore, when applying `dropout_mask` through: \n",
    "\n",
    "    pre_out = tf.sparse_retain(X, dropout_mask)\n",
    "    \n",
    "they are removing specific locations in the adjancency matrix, i.e. edges, not a node. Dropping a node (with all its connections) would imply dropping an entire row of the adjancency matrix, not only locations.\n",
    "\n",
    "Anyway, let's see how those functions work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3000x3000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 51456 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember\n",
    "adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_tensor = _convert_sp_mat_to_sp_tensor_tf(adjacency_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3000)\n"
     ]
    }
   ],
   "source": [
    "print(adjacency_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_fold_hat = _split_A_hat_node_dropout_tf(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(300), Dimension(3000)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_fold_hat[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are the adaptation to `Pytorch` of the functions above. In addition, I have added one additional form of dropout. Let's go, first, convert a sparse matrix into a sparse tensor using Pytorch `sparse` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sp_mat_to_sp_tensor(X):\n",
    "    coo = X.tocoo().astype(np.float32)\n",
    "    i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
    "    v = torch.FloatTensor(coo.data)\n",
    "    return torch.sparse.FloatTensor(i, v, coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_tensor = convert_sp_mat_to_sp_tensor(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0, 1056, 1108,  ...,  969,  981, 2999],\n",
       "                       [   0,    0,    0,  ..., 2999, 2999, 2999]]),\n",
       "       values=tensor([1.0000, 0.0714, 0.0625,  ..., 0.0357, 0.0833, 1.0000]),\n",
       "       size=(3000, 3000), nnz=51456, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remember, before I said that I believe what the authors of the original paper (and code) refer as \"node dropout\" is in reality edge dropout. Dropping a node (i.e. all its connections) in a the graph involves dropping an entire row from the adjancency matrix. With that in mind, I included two methods, `_edge_dropout_sparse` designed to reproduce the dropout in the original code and `_node_dropout_sparse` which will drop a row (i.e. node) in the adjancency matrix. Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _edge_dropout_sparse(self, X, keep_prob):\n",
    "\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.FloatTensor(X._nnz()).uniform_()\n",
    "    dropout_mask = random_tensor.floor()\n",
    "    dropout_tensor = torch.sparse.FloatTensor(X.coalesce().indices(), dropout_mask, X.size())\n",
    "    X_w_dropout = X.mul(dropout_tensor)\n",
    "\n",
    "    return  X_w_dropout.mul(1./keep_prob)\n",
    "\n",
    "def _node_dropout_sparse(self, X, keep_prob):\n",
    "\n",
    "    random_array = keep_prob\n",
    "    random_array += np.random.rand(X.size()[0])\n",
    "    dropout_mask = np.floor(random_array)\n",
    "    dropout_mask = np.tile(dropout_mask.reshape(-1,1), X.size()[1])\n",
    "    dropout_tensor = self.convert_sp_mat_to_sp_tensor(sp.csr_matrix(dropout_mask))\n",
    "    X_w_dropout = X.mul(dropout_tensor)\n",
    "\n",
    "    return  X_w_dropout.mul(1./keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first one is fairly straightforward to read, since the process is nearly identical to the one in `_dropout_sparse`. The only difference is that `pytorch` does not have an equivalent to `tf.sparse_retain()`. However, that's ok, one can simply do an element-wise multiplication to set to zero `(1-keep_prob)%` of the  elements. Regarding to the second one, let's go line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 1 - node_dropout\n",
    "random_array = keep_prob\n",
    "random_array += np.random.rand(adjacency_tensor.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000,)\n",
      "[1.80294764 1.85788471 1.6895335  1.45745756 1.71669362 1.69276678\n",
      " 1.82779998 1.86452777 1.19759748 1.5501779 ]\n"
     ]
    }
   ],
   "source": [
    "print(random_array.shape)\n",
    "print(random_array[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an array of 3000 numbers with a 90% of them greater 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_mask = np.floor(random_array)\n",
    "dropout_mask = np.tile(dropout_mask.reshape(-1,1), adjacency_tensor.size()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3000)\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(dropout_mask.shape)\n",
    "print(dropout_mask[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, we now have a matrix where 90% of the rows are all ones and 10% are all 0s. When element-wise multiply by `adjacency_tensor` it will drop a node and all its connections.\n",
    "\n",
    "The next two methods are identical to those above for tensorflow with very minor adaptations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_A_hat(X):\n",
    "    A_fold_hat = []\n",
    "\n",
    "    fold_len = (n_users + n_items) // n_fold\n",
    "    for i_fold in range(n_fold):\n",
    "        start = i_fold * fold_len\n",
    "        if i_fold == n_fold -1:\n",
    "            end = n_users + n_items\n",
    "        else:\n",
    "            end = (i_fold + 1) * fold_len\n",
    "\n",
    "        A_fold_hat.append(convert_sp_mat_to_sp_tensor(X[start:end]))\n",
    "    return A_fold_hat\n",
    "\n",
    "def _split_A_hat_node_dropout(X):\n",
    "    A_fold_hat = []\n",
    "\n",
    "    fold_len = (n_users + n_items) // n_fold\n",
    "    for i_fold in range(n_fold):\n",
    "        start = i_fold * fold_len\n",
    "        if i_fold == n_fold -1:\n",
    "            end = n_users + n_items\n",
    "        else:\n",
    "            end = (i_fold + 1) * fold_len\n",
    "\n",
    "        temp = convert_sp_mat_to_sp_tensor(X[start:end])\n",
    "        A_fold_hat.append(_edge_dropout_sparse(temp, 1 - node_dropout))\n",
    "\n",
    "    return A_fold_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "We can now jump into the design of the model. Let's first include here the relevant figures from their paper as well as the relevant mathematical expressions. \n",
    "\n",
    "The figures below and the mathematical expression are taken directly from their paper. Thefore, as I have mentioned a number of times, **all credit to the authors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='figures/Figure1.png'></td><td><img src='figures/Figure2.png'></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<table><tr><td><img src='figures/Figure1.png'></td><td><img src='figures/Figure2.png'></td></tr></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Figure on the left is an illustration of the user-item interaction graph, and what the authors call the high-order connectivity. The node/user $u_{1}$ is the target user to provide recommendations for. We can see that at first order ($l$-hop with $l$=1), one would capture the information that the user interacted with items 1, 2 and 3. At 2nd order (2-hop), one would capture $u_{1} \\leftarrow i_{2} \\leftarrow u_{2}$ and $u_{1} \\leftarrow i_{3} \\leftarrow u_{3}$, and so on.\n",
    "\n",
    "To the right we have the model scheme. In the author's words: *\"we design a neural network method to propagate embeddings recursively on the graph. This is inspired by the recent developments of graph neural networks [...], which can be seen as constructing information flows in the embedding space. Specifically, we devise an embedding propagation layer, which refines a user’s (or an item’s) embedding by aggregating the embeddings of the interacted items (or users). By stacking multiple embedding propagation layers, we can enforce the embeddings to capture the collaborative signal in high-order connectivities.\"*\n",
    "\n",
    "To be honest this is one of these cases where the math (and then the code) will help to understand what is going on. Formally, the model consists of two pieces: *message construction* and *message aggregation*.  \n",
    "\n",
    "**Message Construction**: for a connected user, item, a message is defined as (their expression 3):\n",
    "\n",
    "\n",
    "$$\n",
    "\\textbf{m}_{u \\leftarrow i} = \\frac{1}{\\sqrt{|\\mathcal{N}_{u}||\\mathcal{N}_{i}}|}\\Big( \\textbf{W}_{1} e_i + \\textbf{W}_{2}(e_i \\odot e_u) \\Big)\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication. $1/{\\sqrt{|\\mathcal{N}_{u}||\\mathcal{N}_{i}}|}$ is the graph Laplacion norm, where $\\mathcal{N}_{u}$ and $\\mathcal{N}_{i}$ are the first-hop neighbors of user u and item i. Note that this factor (decay factor between two connected nodes) is already accounted by in our Laplacian matrix by construction. \n",
    "\n",
    "Also remember here $e_i$ or $e_u$ are *not* the initial embeddings, but the aggregated embeddings, i.e. for user 1, $e_{i}$ would be the aggregated embeddings of all the items that that user interacted with. This will be simply achieved by multiplying the initial embeddings by the Laplacian matrix. We'll see later. \n",
    "\n",
    "\n",
    "As simple as that, the \"messages\" are constructed, now we need to aggregate them. \n",
    "\n",
    "**Message Aggregation**: simply ((their expression 4): \n",
    "\n",
    "$$\n",
    "e^{(1)}_{u} = \\text{LeakyRelu} \\Big( \\textbf{m}_{u \\leftarrow u} + \\sum_{i \\in \\mathcal{N}_{u}} \\textbf{m}_{u \\leftarrow i} \\Big)\n",
    "$$\n",
    "\n",
    "And from here basically, one repeats the process through as many layers as you might want. If you want to know more about all the reasoning behind the formulation of the message construction and aggregation, please, go to the paper. Is easy to read and understand. For what I need for this notebook these two expressions would be enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assume no node dropout\n",
    "A_fold_hat = _split_A_hat_tf(adjacency_matrix)\n",
    "\n",
    "# they call this ego embeddings because they relate to ego-networks (i.e. 1st order connections)\n",
    "# Simply the concatenation over rows of user and item embeddings. Shape  (n_users+n_items, n_emb)\n",
    "ego_embeddings = tf.concat([weights['user_embedding'], weights['item_embedding']], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, per *\"graph layer\"*, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_embed = []\n",
    "for f in range(n_fold):\n",
    "    temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n",
    "side_embeddings = tf.concat(temp_embed, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`side_embeddings` contains, for a given user, the weighted sum of the item embeddings that a certain user interacted with (plus the embeddings of that user). Also, for a given item, contains the weighted sum of the user embeddings that an item \"interacted\" with (plus the embeddings of that item). The decay factors (or the weights in the weighted sum) are $1/{\\sqrt{|\\mathcal{N}_{u}||\\mathcal{N}_{i}}|}$. More formally, `side_embeddings` is: \n",
    "\n",
    "$$\n",
    "\\text{side_embeddings} =  \\frac{1}{\\sqrt{|\\mathcal{N}_{u}||\\mathcal{N}_{i}}|}e_i\n",
    "$$\n",
    "\n",
    "We now move to the second term of their expression (3). The authors refer to the element-wise multiplication $e_i \\odot e_u$ as `bi_embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally:\n",
    "\n",
    "$$\n",
    "\\text{bi_embeddings} = \\frac{1}{\\sqrt{|\\mathcal{N}_{u}||\\mathcal{N}_{i}}|} (e_i \\odot e_u)\n",
    "$$\n",
    "\n",
    "With all that, the only thing left to do to calculate:\n",
    "\n",
    "$$\n",
    "e^{(1)}_{u} = \\text{LeakyRelu} \\Big( \\textbf{m}_{u \\leftarrow u} + \\sum_{i \\in \\mathcal{N}_{u}} \\textbf{m}_{u \\leftarrow i} \\Big)\n",
    "$$\n",
    "\n",
    "with $l=0$ (first layer) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#let's assume we are in the first iteration of the loop through layers, so k=0\n",
    "k=0 \n",
    "sum_embeddings = tf.nn.leaky_relu(tf.matmul(side_embeddings, weights['W_gc_%d' % k]) + weights['b_gc_%d' % k])\n",
    "bi_embeddings = tf.nn.leaky_relu(tf.matmul(bi_embeddings, weights['W_bi_%d' % k]) + weights['b_bi_%d' % k])\n",
    "\n",
    "# redefine the ego embeddings so the information of the 1st order interactions is captured\n",
    "ego_embeddings = sum_embeddings + bi_embeddings\n",
    "ego_embeddings = tf.nn.dropout(ego_embeddings, keep_prob=1-mess_dropout[k])\n",
    "\n",
    "# some normalization\n",
    "norm_embeddings = tf.math.l2_normalize(ego_embeddings, axis=1)\n",
    "\n",
    "# back to user and item dim\n",
    "u_g_emb, i_g_emb = tf.split(norm_embeddings, [n_users, n_items], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(1000), Dimension(12)]),\n",
       " TensorShape([Dimension(2000), Dimension(12)]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_g_emb.shape, i_g_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have not defined the model components yet in pytorch :) \n",
    "# Note that the embeddings could also be defined as tensors:\n",
    "\n",
    "# embeddings_user = nn.Parameter(torch.rand(n_users, emb_dim))\n",
    "\n",
    "# but I prefer to use modules, so they print out when you call \n",
    "# the model and is easier to access to them internally within \n",
    "# the model class. In the end is a a matter of taste\n",
    "embeddings_user = nn.Embedding(n_users, emb_size)\n",
    "embeddings_item = nn.Embedding(n_items, emb_size)\n",
    "W1 = nn.ModuleList()\n",
    "W2 = nn.ModuleList()\n",
    "\n",
    "features = [emb_size] + layers\n",
    "for i in range(1,len(features)):\n",
    "        W1.append(nn.Linear(features[i-1],features[i]))\n",
    "        W2.append(nn.Linear(features[i-1],features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1000, 12)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assume no node dropout\n",
    "A_fold_hat = _split_A_hat(adjacency_matrix)\n",
    "\n",
    "# they call this ego embeddings because they relate to ego-networks (i.e. 1st order connections)\n",
    "# Simply the concatenation over rows of user and item embeddings. Shape  (n_users+n_items, n_emb)\n",
    "ego_embeddings = torch.cat([embeddings_user.weight, embeddings_item.weight], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 12])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ego_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_embed = []\n",
    "for f in range(n_fold):\n",
    "    temp_embed.append(torch.sparse.mm(A_fold_hat[f], ego_embeddings))\n",
    "weighted_sum_emb = torch.cat(temp_embed, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that tf.matmul(side_embeddings, weights['W_gc_%d' % k]) + weights['b_gc_%d' % k]\n",
    "# is just what a linear layer would do. So I implemented the first term of the summation (t1) as:\n",
    "t1 = W1[k](weighted_sum_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{t1} =  \\frac{1}{\\sqrt{|\\mathcal{N}_{u}||\\mathcal{N}_{i}}|}e_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in their paper they say that the element-wise multiplication makes the message dependent \n",
    "# on the affinity between ei and eu. So I call them affinity_emb and implemented the term 2 (t2) as:\n",
    "affinity_emb = ego_embeddings.mul(weighted_sum_emb)\n",
    "t2 = W2[k](affinity_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{t2} = \\frac{1}{\\sqrt{|\\mathcal{N}_{u}||\\mathcal{N}_{i}}|} (e_i \\odot e_u)\n",
    "$$\n",
    "\n",
    "so $e^{(l=0)}_u$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlike tf, nn.Dropout takes p, prob of element to be zeroed.\n",
    "ego_embeddings = nn.Dropout(mess_dropout[k])(F.leaky_relu(t1 + t2))\n",
    "\n",
    "# normalization\n",
    "norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "\n",
    "# back to user and item dim\n",
    "u_g_emb, i_g_emb= norm_embeddings.split([n_users, n_items], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 12]), torch.Size([2000, 12]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_g_emb.shape, i_g_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
