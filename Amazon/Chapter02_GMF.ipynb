{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generalized Matrix Factorization (GMF)\n",
    "\n",
    "For a mathematical formulation of the model, please go to the [Xiangnan He](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf)'s paper. I will describe here the details in a more \"programatic\" way. \n",
    "\n",
    "Let's start by importing the neccesary packages and define a number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from time import time\n",
    "from scipy.sparse import load_npz\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is mostly copied from my [repo](https://github.com/jrzaurin/neural_cf) and also wrapped up in a the funcion `parse_args()` at the script `gmf.py`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../datasets/Amazon\"\n",
    "dataname = \"neuralcf_split.npz\"\n",
    "train_matrix = \"neuralcf_train_sparse.npz\"\n",
    "modeldir = \"../datasets/Amazon/models\"\n",
    "n_emb = 8\n",
    "batch_size = 512\n",
    "epochs = 1\n",
    "learner = \"Adam\"\n",
    "lr = 0.03\n",
    "lr_scheduler = 1 #boolean\n",
    "validate_every = 1\n",
    "topk = 10\n",
    "n_neg = 4 # number of negative examples during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "The model is rather simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, n_emb=8):\n",
    "        super(GMF, self).__init__()\n",
    "\n",
    "        self.n_emb = n_emb\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "\n",
    "        self.embeddings_user = nn.Embedding(n_user, n_emb)\n",
    "        self.embeddings_item = nn.Embedding(n_item, n_emb)\n",
    "        self.out = nn.Linear(in_features=n_emb, out_features=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight)\n",
    "\n",
    "    def forward(self, users, items):\n",
    "\n",
    "        user_emb = self.embeddings_user(users)\n",
    "        item_emb = self.embeddings_item(items)\n",
    "        prod = user_emb*item_emb\n",
    "        preds = torch.sigmoid(self.out(prod))\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of visualising the model I found [this package](https://github.com/waleedka/hiddenlayer) quite handy. Is perhaps a bit too detailed about the operations that are happening in the forward pass, but still, very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hiddenlayer as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"270pt\" height=\"357pt\"\n",
       " viewBox=\"0.00 0.00 270.00 357.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(72 321)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-72,36 -72,-321 198,-321 198,36 -72,36\"/>\n",
       "<!-- GMF/Embedding[embeddings_user]/outputs/6 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>GMF/Embedding[embeddings_user]/outputs/6</title>\n",
       "<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"54,-285 0,-285 0,-249 54,-249 54,-285\"/>\n",
       "<text text-anchor=\"start\" x=\"13\" y=\"-264\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Gather</text>\n",
       "</g>\n",
       "<!-- GMF/outputs/8 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>GMF/outputs/8</title>\n",
       "<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"90,-202 36,-202 36,-166 90,-166 90,-202\"/>\n",
       "<text text-anchor=\"start\" x=\"54\" y=\"-181\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Mul</text>\n",
       "</g>\n",
       "<!-- GMF/Embedding[embeddings_user]/outputs/6&#45;&gt;GMF/outputs/8 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>GMF/Embedding[embeddings_user]/outputs/6&#45;&gt;GMF/outputs/8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M34.8115,-248.9902C39.5435,-238.0802 45.6763,-223.9407 51.0126,-211.6376\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.3112,-212.8282 55.0795,-202.2612 47.8893,-210.0427 54.3112,-212.8282\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.5\" y=\"-223\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">512x8</text>\n",
       "</g>\n",
       "<!-- GMF/Embedding[embeddings_item]/outputs/7 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>GMF/Embedding[embeddings_item]/outputs/7</title>\n",
       "<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"126,-285 72,-285 72,-249 126,-249 126,-285\"/>\n",
       "<text text-anchor=\"start\" x=\"85\" y=\"-264\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Gather</text>\n",
       "</g>\n",
       "<!-- GMF/Embedding[embeddings_item]/outputs/7&#45;&gt;GMF/outputs/8 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>GMF/Embedding[embeddings_item]/outputs/7&#45;&gt;GMF/outputs/8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M91.1885,-248.9902C86.4565,-238.0802 80.3237,-223.9407 74.9874,-211.6376\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"78.1107,-210.0427 70.9205,-202.2612 71.6888,-212.8282 78.1107,-210.0427\"/>\n",
       "<text text-anchor=\"middle\" x=\"96.5\" y=\"-223\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">512x8</text>\n",
       "</g>\n",
       "<!-- GMF/Linear[out]/outputs/9 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>GMF/Linear[out]/outputs/9</title>\n",
       "<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"90,-119 36,-119 36,-83 90,-83 90,-119\"/>\n",
       "<text text-anchor=\"start\" x=\"50\" y=\"-98\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Linear</text>\n",
       "</g>\n",
       "<!-- GMF/outputs/8&#45;&gt;GMF/Linear[out]/outputs/9 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>GMF/outputs/8&#45;&gt;GMF/Linear[out]/outputs/9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M63,-165.9902C63,-155.2963 63,-141.4994 63,-129.3706\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"66.5001,-129.2612 63,-119.2612 59.5001,-129.2613 66.5001,-129.2612\"/>\n",
       "<text text-anchor=\"middle\" x=\"76.5\" y=\"-140\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">512x8</text>\n",
       "</g>\n",
       "<!-- GMF/outputs/10 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>GMF/outputs/10</title>\n",
       "<polygon fill=\"#e8e8e8\" stroke=\"#000000\" points=\"90,-36 36,-36 36,0 90,0 90,-36\"/>\n",
       "<text text-anchor=\"start\" x=\"46\" y=\"-15\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">Sigmoid</text>\n",
       "</g>\n",
       "<!-- GMF/Linear[out]/outputs/9&#45;&gt;GMF/outputs/10 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>GMF/Linear[out]/outputs/9&#45;&gt;GMF/outputs/10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M63,-82.9902C63,-72.2963 63,-58.4994 63,-46.3706\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"66.5001,-46.2612 63,-36.2612 59.5001,-46.2613 66.5001,-46.2612\"/>\n",
       "<text text-anchor=\"middle\" x=\"76.5\" y=\"-57\" font-family=\"Times\" font-size=\"10.00\" fill=\"#000000\">512x1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<hiddenlayer.graph.Graph at 0x7fba9a00da90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_model = GMF(n_user=10, n_item=10)\n",
    "hl.build_graph(toy_model, (torch.zeros([batch_size]).long(),  torch.zeros([batch_size]).long()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Following the paper's implementation, here is how we will train the model. Remember that initially we are ignoring the rating itself. Our target will be a 1/0 depending on whether the user interacted saw the movie or not. To implement the implicit negative feedback we will pick at random `n_neg` movies that user never saw, per movie that the user has seen.\n",
    "\n",
    "Progamatically we do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, negatives, n_items, n_neg):\n",
    "    \"\"\"\n",
    "    Select n_neg never seen movies per movie seen\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    train: scipy.sparse.dok.dok_matrix\n",
    "        sparse key-based matrix \n",
    "    negatives: np.ndarray\n",
    "        array of (n_user, 99) movies the user never rated that are used for testing\n",
    "    n_neg: int\n",
    "        number of negative (i.e. non-rated) movies per positive (i.e. rated)\n",
    "    n_items: int\n",
    "        number of items in the dataset\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    train_w_negative: np.ndarray\n",
    "        array with (1+n_neg) entries per user\n",
    "    \"\"\"\n",
    "    user, item, labels = [],[],[]\n",
    "    for (u, i), r in train.items():\n",
    "        # positive instance\n",
    "        user.append(u)\n",
    "        item.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances: we also need to make sure they are not in the\n",
    "        # negative examples used for testing\n",
    "        for _ in range(n_neg):\n",
    "            j = np.random.randint(n_items)\n",
    "            while ((u, j) in train.keys()) or (j in negatives[u]):\n",
    "                j = np.random.randint(n_items)\n",
    "            user.append(u)\n",
    "            item.append(j)\n",
    "            labels.append(0)\n",
    "    train_w_negative = np.vstack([user,item,labels]).T\n",
    "    assert train_w_negative.shape[0] == (len(train) + len(train)*n_neg)\n",
    "    return train_w_negative.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, the training phase is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, epoch, batch_size,\n",
    "          train_ratings, negatives, n_items, n_neg):\n",
    "    model.train()\n",
    "    # Build a training dataset with n_neg instances per positive instance\n",
    "    train_dataset = get_train_instances(train_ratings,\n",
    "        negatives,\n",
    "        n_items,\n",
    "        n_neg)\n",
    "    # Build the corresponding loader\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        shuffle=True)\n",
    "    # From here in advance is a pretty standard training phase if you are familiar with pytorch\n",
    "    train_steps = (len(train_loader.dataset) // train_loader.batch_size) + 1\n",
    "    running_loss=0\n",
    "    for data in train_loader:\n",
    "        users = data[:,0]\n",
    "        items = data[:,1]\n",
    "        labels = data[:,2].float()\n",
    "        if use_cuda:\n",
    "            users, items, labels = users.cuda(), items.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        preds =  model(users, items)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        loss = criterion(preds.squeeze(1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss/train_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "As I mentioned before the evaluation will be done in terms of the Hit Ratio and the Normalized Discounted Cumulative Gain at k, k being 10 in this excercise. These metrics are defined here as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hitratio(ranklist, gtitem):\n",
    "    if gtitem in ranklist: return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_ndcg(ranklist, gtitem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtitem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_scores(items, preds, topk):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    items: List\n",
    "        list of 100 item ids where the 1st one is the rated one\n",
    "    preds: List\n",
    "        list of 100 predictions for those item ratings\n",
    "    topk: int\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    hr, ndcg: hit ratio and normalised discounted cumulative gain\n",
    "    \"\"\"\n",
    "    gtitem = items[0]\n",
    "    map_item_score = dict( zip(items, preds) )\n",
    "    ranklist = heapq.nlargest(topk, map_item_score, key=map_item_score.get)\n",
    "    hr = get_hitratio(ranklist, gtitem)\n",
    "    ndcg = get_ndcg(ranklist, gtitem)\n",
    "    return hr, ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play a bit with the functions to understand the values that they will output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26,  6, 51, 61, 37,  7, 81, 74, 54, 70, 45, 84, 66, 29, 80, 10, 27,\n",
       "       12, 28, 16, 82, 94, 21, 32, 68, 15, 88,  8,  0, 31, 79, 60, 89, 76,\n",
       "       17, 34, 56,  5, 25, 24, 41, 52, 46, 43, 55, 58, 48, 47, 98, 63, 20,\n",
       "       13, 73, 62, 53, 11, 86, 99, 90, 72, 19, 92, 65, 44, 85, 18, 71, 30,\n",
       "       93, 78, 38, 42,  1, 95, 87, 67, 91, 23, 97, 77, 33, 83, 64, 35, 50,\n",
       "       39,  2, 57, 36, 96,  4, 49, 40,  9, 69, 59, 14, 22,  3, 75])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranklist = np.arange(100)\n",
    "np.random.shuffle(ranklist)\n",
    "ranklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `gtitem` is not among the top 10 (remember, we will use k=10), then both HR@10 and NDCG@10 will be 0. HR@10 does not provide much sense of ranking other than the fact that the target movie (the movie that the user did see) is among the top 10 ranked movies. In other words, whether the movie is ranked the 10th of the 1st the HR@10 will be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "print(get_hitratio(ranklist[:10], ranklist[9]), get_hitratio(ranklist[:10], ranklist[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not the case for the NDCG@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.2890648263178878\n"
     ]
    }
   ],
   "source": [
    "print(get_ndcg(ranklist[:10], ranklist[0]), get_ndcg(ranklist[:10], ranklist[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of the two metrics will give us an idea if 1) we are managing to rank the target item among the top 10 and 2) how good (i.e. top 1) we actually did rank the target item. \n",
    "\n",
    "before we move on let's just go with some detail through the `get_scores` function. Remember that we ordered the test observations so that the 1st item is *always* the rated item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = np.arange(100)\n",
    "np.random.shuffle(items)\n",
    "preds = np.random.rand(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 [85, 4, 13, 95, 20, 58, 6, 29, 54, 45]\n"
     ]
    }
   ],
   "source": [
    "gtitem = items[0] # first item always the ranked item\n",
    "map_item_score = dict( zip(items, preds) )\n",
    "ranklist = heapq.nlargest(topk, map_item_score, key=map_item_score.get)\n",
    "print(gtitem, ranklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, both HR@10 and NDCG@10 will be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all this, the evaluation phase code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, use_cuda, topk):\n",
    "    model.eval()\n",
    "    scores=[]\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            users = data[:,0]\n",
    "            items = data[:,1]\n",
    "            labels = data[:,2].float()\n",
    "            if use_cuda:\n",
    "                users, items, labels = users.cuda(), items.cuda(), labels.cuda()\n",
    "            preds = model(users, items)\n",
    "            items_cpu = items.cpu().numpy()\n",
    "            preds_cpu = preds.squeeze(1).detach().cpu().numpy()\n",
    "            litems=np.split(items_cpu, test_loader.batch_size//100)\n",
    "            lpreds=np.split(preds_cpu, test_loader.batch_size//100)\n",
    "            scores += [get_scores(it,pr,topk) for it,pr in zip(litems,lpreds)]\n",
    "    hits = [s[0] for s in scores]\n",
    "    ndcgs = [s[1] for s in scores]\n",
    "    return (np.array(hits).mean(),np.array(ndcgs).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Model\n",
    "\n",
    "let's load the neccesary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(os.path.join(datadir, dataname))\n",
    "train_ratings = load_npz(os.path.join(datadir, train_matrix)).todok()\n",
    "test_ratings, negatives = dataset['test_negative'], dataset['negatives']\n",
    "n_users, n_items = dataset['n_users'].item(), dataset['n_items'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<123960x50051 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 1573573 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0    522      5]\n",
      " [     0   6661      0]\n",
      " [     0  47523      0]\n",
      " ...\n",
      " [123959  35172      0]\n",
      " [123959   1416      0]\n",
      " [123959  48343      0]] (12396000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_ratings, test_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  719 39241 43868 ... 23786 32682 45978]\n",
      " [38567 28920  2498 ... 31813  7297 45095]\n",
      " [35693 10527 49508 ... 21817 14640 34373]\n",
      " ...\n",
      " [30533 16720 11683 ... 25014 30921 34250]\n",
      " [16440  6679 21470 ... 41975 16216 21194]\n",
      " [25528 16193 25895 ... 36923 45989 48343]] (123960, 99)\n"
     ]
    }
   ],
   "source": [
    "print(negatives, negatives.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123960, 50052)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* comment: you can see that we miss one item (50051 vs 50052) that only appears in the testing dataset. This is ok. It only means that the embeddings for that item during training will never be learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_ratings,\n",
    "    # for speed purposes we use large test batch sizes. These will be broken in chunks of 100 during evaluation\n",
    "    batch_size=1000,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GMF(n_users, n_items, n_emb=n_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a reasoning on why I chose MSE over the usually preferred BCE have a look to the repo I mentioned before\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I run the excercise, I include the option of using [Cyclic Learning Rates](https://arxiv.org/abs/1506.01186) [1]. For this particular excercise here, it does not seem to help much. Also, let me add the following. There is a bug in the Pytorch version of CyclicLR related to optimizers that have no momentum parameter. To fix that bug, simply go to your `site-packages/torch/optim/lr_scheduler.py` file and indent the lines 593 and 594 (Pytorch version 1.1.0). More precisely, these lines:\n",
    "\n",
    "        self.cycle_momentum = cycle_momentum\n",
    "        if cycle_momentum:\n",
    "            if 'momentum' not in optimizer.defaults:\n",
    "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
    "\n",
    "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
    "            if last_epoch == -1:\n",
    "                for momentum, group in zip(base_momentums, optimizer.param_groups):\n",
    "                    group['momentum'] = momentum\n",
    "        self.base_momentums = list(map(lambda group: group['momentum'], optimizer.param_groups))\n",
    "        self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
    "\n",
    "\n",
    "Should look like this:\n",
    "\n",
    "        self.cycle_momentum = cycle_momentum\n",
    "        if cycle_momentum:\n",
    "            if 'momentum' not in optimizer.defaults:\n",
    "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
    "\n",
    "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
    "            if last_epoch == -1:\n",
    "                for momentum, group in zip(base_momentums, optimizer.param_groups):\n",
    "                    group['momentum'] = momentum\n",
    "            self.base_momentums = list(map(lambda group: group['momentum'], optimizer.param_groups))\n",
    "            self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
    "\n",
    "\n",
    "[1] Leslie N. Smith. Cyclical Learning Rates for Training Neural Networks. arXiv:1506.01186v6 [cs.CV] 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = ((len(train_ratings)+len(train_ratings)*n_neg)//batch_size)+1\n",
    "step_size = training_steps*3 # one cycle every 6 epochs\n",
    "cycle_momentum=True\n",
    "if learner.lower() == \"adagrad\" or learner.lower()==\"adam\":\n",
    "    cycle_momentum=False\n",
    "if lr_scheduler:\n",
    "    scheduler = CyclicLR(optimizer, step_size_up=step_size, base_lr=lr/10., max_lr=lr,\n",
    "        cycle_momentum=cycle_momentum)\n",
    "else:\n",
    "    scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are good to go! I will just run one epoch to show that all runs well. The companion python script (`gmf.py`) as a few more rings and bells here and there, but overall is the same code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 95.83s, LOSS = 0.1619, HR = 0.1189, NDCG = 0.0575, validated in 89.55s\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "best_hr, best_ndcgm, best_iter=0,0,0\n",
    "for epoch in range(1,epochs+1):\n",
    "    t1 = time()\n",
    "    loss = train(model, criterion, optimizer, scheduler, epoch, batch_size, \n",
    "                 train_ratings, negatives, n_items, n_neg)\n",
    "    t2 = time()\n",
    "    if epoch % validate_every == 0:\n",
    "        (hr, ndcg) = evaluate(model, test_loader, use_cuda, topk)\n",
    "        print(\"Epoch: {} {:.2f}s, LOSS = {:.4f}, HR = {:.4f}, NDCG = {:.4f}, validated in {:.2f}s\".\n",
    "            format(epoch, t2-t1, loss, hr, ndcg, time()-t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final notebook I will summarise the results and comment on the evaluation technique. For not, let's move to the next model: MLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_jrz)",
   "language": "python",
   "name": "conda_jrz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
