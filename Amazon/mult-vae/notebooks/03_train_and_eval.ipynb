{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "In Notebooks 01 and 02 I described how to prepare the data and build the model. Here I will show how to train, validate and test it. \n",
    "\n",
    "Once again, I will focus on the `Mxnet` implementation.\n",
    "\n",
    "Let's start by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import autograd, gluon, nd\n",
    "from tqdm import trange\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from models.mxnet_models import MultiDAE, MultiVAE\n",
    "from utils.data_loader import DataLoader\n",
    "from utils.metrics import NDCG_binary_at_k_batch, Recall_at_k_batch\n",
    "from utils.parser import parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "data_path = DATA_DIR / \"movielens_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data_path)\n",
    "n_items = data_loader.n_items\n",
    "train_data = data_loader.load_data(\"train\")\n",
    "valid_data_tr, valid_data_te = data_loader.load_data(\"validation\")\n",
    "test_data_tr, test_data_te = data_loader.load_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<116677x20108 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 8538846 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the training data (same applies to validation and test) is the binary sparse matrix of interactions. Have a look to the class `DataLoader` if you want a few more details on how it is built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in Notebook 02, [Liang et al, 2018](https://arxiv.org/pdf/1802.05814.pdf) interpret the Kullback-Leiber divergence as a regularization term. With that in mind they add a regularization parameter $\\beta$ and, in a procedure inspired by [Samuel R. Bowman et al, 2016](https://arxiv.org/abs/1511.06349), they linearly anneal the KL term slowly over a large number of training steps. Here is perhaps the part where it can get a bit (just a bit) confusing, moreover if you do not look at their code and focus in the paper alone. \n",
    "\n",
    "In their paper,  Liang et al write the following referring to their Figure 1 and their annealing approach: \"*[...] we plot the validation ranking metric [...] with KL annealing all the way to $\\beta$ = 1 [...] $\\beta$ reaches 1 at around 80 epochs) [...] Having identified the best $\\beta$ based on the peak validation metric, we can retrain the model with the same annealing schedule, but stop increasing $\\beta$ after reaching that value*\"\n",
    "\n",
    "When I read these lines, together with their Figure 1, I initially interpreted the following: $\\beta$ reaches 1 at around 80 epochs and the best validation metrics is occurs at epoch 20 approximately. By then, $\\beta$ must have a value of $\\sim$0.25. Therefore, my understanding is that they will then retrain the model with the same annealing schedule, but stop at epoch 20 when $\\beta$ reaches $\\sim$0.25. \n",
    "\n",
    "However, when I went to [their implementation](https://github.com/dawenl/vae_cf/blob/master/VAE_ML20M_WWW2018.ipynb), the authors do the following: using a batch size of 500 they set the total number of annealing steps to 200000. Given that the training dataset has a size of 116677, every epoch has 234 training steps. Their `anneal_cap` value, i.e. the maximum annealing reached during training, is set to 0.2, and during training they use the following approach: \n",
    "\n",
    "```python\n",
    "            if total_anneal_steps > 0:\n",
    "                anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
    "            else:\n",
    "                anneal = anneal_cap\n",
    "```\n",
    "\n",
    "where `update_count` will increase by 1 every training step/batch. They use 200 epochs, therefore, if we do the math, the `anneal_cap` value will stop increasing when `update_count / total_anneal_steps` = 0.2, i.e. after 40000 training steps, or in other words, after around 170 epochs, i.e. $\\sim$80% of the total number of epochs. Therefore, what they really meant is that once you select the best performing $\\beta$, you applied the same schedule as the ones used when annealing all the way to $\\beta$ = 1, reaching the annealing max value (e.g. 0.2) at $\\sim$80% of the total number of epochs.\n",
    "\n",
    "Whit that in mind my implementation looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "anneal_epochs = None\n",
    "anneal_cap = 0.2\n",
    "constant_anneal = False\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = len(range(0, train_data.shape[0], batch_size))\n",
    "try:\n",
    "    total_anneal_steps = (\n",
    "        training_steps * (n_epochs - int(n_epochs * 0.2))\n",
    "    ) / anneal_cap\n",
    "except ZeroDivisionError:\n",
    "    assert (\n",
    "        constant_anneal\n",
    "    ), \"if 'anneal_cap' is set to 0.0 'constant_anneal' must be set to 'True\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions will look very familiar if you are used to `Pytorch`\n",
    "\n",
    "### Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, data, epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    global update_count\n",
    "    N = data.shape[0]\n",
    "    idxlist = list(range(N))\n",
    "    np.random.shuffle(idxlist)\n",
    "    training_steps = len(range(0, N, batch_size))\n",
    "\n",
    "    with trange(training_steps) as t:\n",
    "        for batch_idx, start_idx in zip(t, range(0, N, batch_size)):\n",
    "            t.set_description(\"epoch: {}\".format(epoch + 1))\n",
    "\n",
    "            end_idx = min(start_idx + batch_size, N)\n",
    "            X_inp = data[idxlist[start_idx:end_idx]]\n",
    "            X_inp = nd.array(X_inp.toarray()).as_in_context(ctx)\n",
    "\n",
    "            if constant_anneal:\n",
    "                anneal = anneal_cap\n",
    "            else:\n",
    "                anneal = min(anneal_cap, update_count / total_anneal_steps)\n",
    "            update_count += 1\n",
    "\n",
    "            with autograd.record():\n",
    "                if model.__class__.__name__ == \"MultiVAE\":\n",
    "                    X_out, mu, logvar = model(X_inp)\n",
    "                    loss = vae_loss_fn(X_inp, X_out, mu, logvar, anneal)\n",
    "                    train_step.anneal = anneal\n",
    "                elif model.__class__.__name__ == \"MultiDAE\":\n",
    "                    X_out = model(X_inp)\n",
    "                    loss = -nd.mean(nd.sum(nd.log_softmax(X_out) * X_inp, -1))\n",
    "            loss.backward()\n",
    "            trainer.step(X_inp.shape[0])\n",
    "            running_loss += loss.asscalar()\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "\n",
    "            t.set_postfix(loss=avg_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(data_tr, data_te, data_type=\"valid\"):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    eval_idxlist = list(range(data_tr.shape[0]))\n",
    "    eval_N = data_tr.shape[0]\n",
    "    eval_steps = len(range(0, eval_N, batch_size))\n",
    "\n",
    "    n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "    with trange(eval_steps) as t:\n",
    "        for batch_idx, start_idx in zip(t, range(0, eval_N, batch_size)):\n",
    "            t.set_description(data_type)\n",
    "\n",
    "            end_idx = min(start_idx + batch_size, eval_N)\n",
    "            X_tr = data_tr[eval_idxlist[start_idx:end_idx]]\n",
    "            X_te = data_te[eval_idxlist[start_idx:end_idx]]\n",
    "            X_tr_inp = nd.array(X_tr.toarray()).as_in_context(ctx)\n",
    "\n",
    "            with autograd.predict_mode():\n",
    "                if model.__class__.__name__ == \"MultiVAE\":\n",
    "                    X_out, mu, logvar = model(X_tr_inp)\n",
    "                    loss = vae_loss_fn(X_tr_inp, X_out, mu, logvar, train_step.anneal)\n",
    "                elif model.__class__.__name__ == \"MultiDAE\":\n",
    "                    X_out = model(X_tr_inp)\n",
    "                    loss = -nd.mean(nd.sum(nd.log_softmax(X_out) * X_tr_inp, -1))\n",
    "\n",
    "            running_loss += loss.asscalar()\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "\n",
    "            # Exclude examples from training set\n",
    "            X_out = X_out.asnumpy()\n",
    "            X_out[X_tr.nonzero()] = -np.inf\n",
    "\n",
    "            n100 = NDCG_binary_at_k_batch(X_out, X_te, k=100)\n",
    "            r20 = Recall_at_k_batch(X_out, X_te, k=20)\n",
    "            r50 = Recall_at_k_batch(X_out, X_te, k=50)\n",
    "            n100_list.append(n100)\n",
    "            r20_list.append(r20)\n",
    "            r50_list.append(r50)\n",
    "\n",
    "            t.set_postfix(loss=avg_loss)\n",
    "\n",
    "        n100_list = np.concatenate(n100_list)\n",
    "        r20_list = np.concatenate(r20_list)\n",
    "        r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return avg_loss, np.mean(n100_list), np.mean(r20_list), np.mean(r50_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have widely discussed the evaluation metrics (NDCG@k and Recall@k) in a number of notebooks in this repo (and corresponding posts). Therefore, with that in mind and with the aim of not making another infinite notebook, I will not describe the corresponding implementation here. If you want details on those evaluation metrics, please go the `metrics.py` module in `utils`. The code there is a very small adaptation to the one in the [original implementation](https://github.com/dawenl/vae_cf/blob/master/VAE_ML20M_WWW2018.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the process\n",
    "\n",
    "Let's first instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiVAE(\n",
    "    p_dims=[200, 600, n_items],\n",
    "    q_dims=[n_items, 600, 200],\n",
    "    dropout_enc=[0.5, 0.0],\n",
    "    dropout_dec=[0.0, 0.0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiVAE(\n",
       "  (encode): VAEEncoder(\n",
       "    (q_layers): HybridSequential(\n",
       "      (0): Dropout(p = 0.5, axes=())\n",
       "      (1): Dense(20108 -> 600, linear)\n",
       "      (2): Dropout(p = 0.0, axes=())\n",
       "      (3): Dense(600 -> 400, linear)\n",
       "    )\n",
       "  )\n",
       "  (decode): Decoder(\n",
       "    (p_layers): HybridSequential(\n",
       "      (0): Dropout(p = 0.0, axes=())\n",
       "      (1): Dense(200 -> 600, linear)\n",
       "      (2): Dropout(p = 0.0, axes=())\n",
       "      (3): Dense(600 -> 20108, linear)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The the usual, use GPU if available, make it static/imperative if possible (see Notebook 02 and [here](https://gluon.mxnet.io/chapter07_distributed-learning/hybridize.html), etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/mxnet/gluon/parameter.py:887: UserWarning: Parameter 'vaeencoder2_dense0_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "/usr/local/lib/python3.6/site-packages/mxnet/gluon/parameter.py:887: UserWarning: Parameter 'vaeencoder2_dense0_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "/usr/local/lib/python3.6/site-packages/mxnet/gluon/parameter.py:887: UserWarning: Parameter 'vaeencoder2_dense1_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "/usr/local/lib/python3.6/site-packages/mxnet/gluon/parameter.py:887: UserWarning: Parameter 'vaeencoder2_dense1_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "/usr/local/lib/python3.6/site-packages/mxnet/gluon/parameter.py:887: UserWarning: Parameter 'decoder2_dense0_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "/usr/local/lib/python3.6/site-packages/mxnet/gluon/parameter.py:887: UserWarning: Parameter 'decoder2_dense0_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "/usr/local/lib/python3.6/site-packages/mxnet/gluon/parameter.py:887: UserWarning: Parameter 'decoder2_dense1_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "/usr/local/lib/python3.6/site-packages/mxnet/gluon/parameter.py:887: UserWarning: Parameter 'decoder2_dense1_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n"
     ]
    }
   ],
   "source": [
    "ctx = mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "model.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "model.hybridize()\n",
    "optimizer = mx.optimizer.Adam(learning_rate=0.001, wd=0.)\n",
    "trainer = gluon.Trainer(model.collect_params(), optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we need our custom loss (Eq 10 in Notebook 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_fn(inp, out, mu, logvar, anneal):\n",
    "    neg_ll = -nd.mean(nd.sum(nd.log_softmax(out) * inp, -1))\n",
    "    KLD = -0.5 * nd.mean(nd.sum(1 + logvar - nd.power(mu, 2) - nd.exp(logvar), axis=1))\n",
    "    return neg_ll + anneal * KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready, let's run one epoch (on a p2 instance in AWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1:   9%|▉         | 21/234 [00:27<04:37,  1.30s/it, loss=661]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-09b6afe0b96e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meval_every\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b12ec46ff4bc>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, data, epoch)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2569\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2570\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2551\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2552\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   2553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stop_step = 0\n",
    "update_count = 0\n",
    "stop = False\n",
    "for epoch in range(1):\n",
    "    train_step(model, optimizer, train_data, epoch)\n",
    "    if epoch % eval_every == (eval_every - 1):\n",
    "        val_loss, n100, r20, r50 = eval_step(valid_data_tr, valid_data_te)\n",
    "        print(\"=\" * 80)\n",
    "        print(\n",
    "            \"| valid loss {:4.3f} | n100 {:4.3f} | r20 {:4.3f} | \"\n",
    "            \"r50 {:4.3f}\".format(val_loss, n100, r20, r50)\n",
    "        )\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with a few more rings and bells (e.g. optional learning rate scheduler, early stopping, etc...) this is exactly the code that you will find in `main_mxnet.py`. \n",
    "\n",
    "Before I move to Notebook 04, just a quick comment about something I normally find in these scientific publications. Normally, once they have found the best hyperparameters on the validation set, they test the model on the test set. In \"real-life\" scenarios, there would be one additional step, the one merging the train and validation sets, re-training the model with the best hyperparameters and then testing on the test set. In any case, since here my goal is not to build a real-life system, I will follow the same procedure to that found in the original [implementation](https://github.com/dawenl/vae_cf/blob/master/VAE_ML20M_WWW2018.ipynb).\n",
    "\n",
    "Time now to have a look to the results obtained with both `Pytorch` and `Mxnet`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
